{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80187912-b988-416b-8fb7-970ab0c89be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c23e0e-0d63-459e-bed6-b292fcfc0681",
   "metadata": {},
   "source": [
    "## 3.10.1 定义模型\n",
    "\n",
    "和softmax回归唯一的不同在于，我们多加了一个全连接层作为隐藏层。它的隐藏单元个数为256，并使用ReLU函数作为激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70bd79bf-186e-4fe4-bcaa-8c6c1106e915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 隐藏层 第一层\n",
    "num_inputs  = 784\n",
    "num_hiddens = 512\n",
    "\n",
    "\n",
    "# 隐藏层 第二层\n",
    "num_hiddens_v1 = 512\n",
    "num_outputs_v1 = 256\n",
    "\n",
    "# 输出层\n",
    "num_outputs_v2 = 10\n",
    "num_hiddens_v2 = 256\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "        d2l.FlattenLayer(),\n",
    "        nn.Linear(num_inputs, num_hiddens),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_hiddens_v1, num_outputs_v1), \n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_hiddens_v2, num_outputs_v2), \n",
    "        )\n",
    "\n",
    "for params in net.parameters():\n",
    "    init.normal_(params, mean=0, std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb1d0c-6d82-4f4e-bb7a-ebc228c47f3a",
   "metadata": {},
   "source": [
    "## 3.10.2 读取数据并训练模型\n",
    "\n",
    "我们使用与3.7节中训练softmax回归几乎相同的步骤来读取数据并训练模型。\n",
    "\n",
    "> 注：由于这里使用的是PyTorch的SGD而不是d2lzh_pytorch里面的sgd，所以就不存在3.9节那样学习率看起来很大的问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25fda44-d64b-484c-9b4d-d0e8d937eb80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0044, train acc 0.574, test acc 0.766\n",
      "epoch 2, loss 0.0021, train acc 0.805, test acc 0.814\n",
      "epoch 3, loss 0.0018, train acc 0.833, test acc 0.794\n",
      "epoch 4, loss 0.0016, train acc 0.849, test acc 0.831\n",
      "epoch 5, loss 0.0014, train acc 0.864, test acc 0.791\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "# 损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# 参数更新\n",
    "optimizer = torch.optim.SGD(net.parameters(), \n",
    "                            lr=0.5\n",
    "                           )\n",
    "# 批次\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "# 开始训练\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss,\n",
    "              num_epochs, batch_size, \n",
    "              None, None, optimizer\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bf800-ff21-41df-9106-fa2c746d2dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
