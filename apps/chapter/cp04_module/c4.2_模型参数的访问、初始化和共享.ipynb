{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ecdfc6-7650-4eac-8183-12f87bd40a7e",
   "metadata": {},
   "source": [
    "# 4.2 模型参数的访问、初始化和共享parameters\n",
    "\n",
    "在3.3节（线性回归的简洁实现）中，我们通过`init`模块来初始化模型的参数。我们也介绍了访问模型参数的简单方法。本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。\n",
    "\n",
    "我们先定义一个与上一节中相同的含单隐藏层的多层感知机。我们依然使用默认方式初始化它的参数，并做一次前向计算。与之前不同的是，在这里我们从`nn`中导入了`init`模块，它包含了多种模型初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f93503b-7004-4242-84c1-1ac0b9a9aac1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "# pytorch已进行默认初始化\n",
    "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))  \n",
    "\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b1b424-4f1f-4f93-9841-66707841707e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = torch.rand(2, 4)\n",
    "Y = net(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5934c-e19a-4dd0-b765-bf9b595f3789",
   "metadata": {},
   "source": [
    "## 4.2.1 访问模型参数\n",
    "\n",
    "回忆一下上一节中提到的`Sequential`类与`Module`类的继承关系。对于`Sequential`实例中含模型参数的层，我们可以通过`Module`类的`parameters()`或者`named_parameters`方法来访问所有参数（以迭代器的形式返回），后者除了返回参数`Tensor`外还会返回其名字。下面，访问多层感知机`net`的所有参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc5033e-8975-45be-b469-6490c2cee769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "\n",
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(type(net.named_parameters()))\n",
    "\n",
    "print(\"\")\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66767769-2a73-4475-b77d-373f349433c2",
   "metadata": {},
   "source": [
    "可见返回的名字自动加上了层数的索引作为前缀。\n",
    "我们再来访问`net`中单层的参数。对于使用`Sequential`类构造的神经网络，我们可以通过方括号`[]`来访问网络的任一层。索引0表示隐藏层为`Sequential`实例最先添加的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e277eda-baf9-42a4-9fbf-663ffefb271d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for name, param in net[0].named_parameters():\n",
    "    print(name, param.size(), type(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627855ec-292d-4c8d-8c60-6bed66697da4",
   "metadata": {},
   "source": [
    "因为这里是单层的所以没有了层数索引的前缀。另外返回的`param`的类型为`torch.nn.parameter.Parameter`，其实这是`Tensor`的子类，和`Tensor`不同的是如果一个`Tensor`是`Parameter`，那么它会自动被添加到模型的参数列表里，来看下面这个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8722259b-4fcd-4d21-99e2-df6823511b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight1\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        self.weight1 = nn.Parameter(torch.rand(20, 20))\n",
    "        self.weight2 = torch.rand(20, 20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "n = MyModel()\n",
    "for name, param in n.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af30730-6185-457b-aa73-a5c422cc54d1",
   "metadata": {},
   "source": [
    "上面的代码中`weight1`在参数列表中但是`weight2`却没在参数列表中。\n",
    "\n",
    "因为`Parameter`是`Tensor`，即`Tensor`拥有的属性它都有，比如可以根据`data`来访问参数数值，用`grad`来访问参数梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccff5cd-ce78-4c0c-b6ef-9c05c9293add",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0034,  0.2987, -0.1725,  0.3656],\n",
      "        [ 0.1107,  0.2357, -0.2666, -0.3095],\n",
      "        [ 0.1930,  0.2819, -0.3784, -0.3048]])\n",
      "None\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1110, 0.0971, 0.3158, 0.1361],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "weight_0 = list(net[0].parameters())[0]\n",
    "\n",
    "print(weight_0.data)\n",
    "print(weight_0.grad) # 反向传播前梯度为None\n",
    "\n",
    "Y.backward()\n",
    "print(weight_0.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb44f6b0-35cf-405b-a91a-715c0d940787",
   "metadata": {},
   "source": [
    "## 4.2.2 初始化模型参数\n",
    "\n",
    "我们在3.15节（数值稳定性和模型初始化）中提到了PyTorch中`nn.Module`的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考[源代码](https://github.com/pytorch/pytorch/tree/master/torch/nn/modules)）。但我们经常需要使用其他方法来初始化权重。PyTorch的`init`模块里提供了多种预设的初始化方法。在下面的例子中，我们将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并依然将偏差参数清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636b27fd-77ab-4fd5-ac04-e411549ad4de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[ 1.1827e-03, -2.7215e-02,  2.4744e-03, -1.5965e-02],\n",
      "        [-7.4075e-03, -1.4228e-02, -5.1989e-05,  9.7374e-03],\n",
      "        [ 6.1147e-03,  1.0445e-02,  6.7197e-03,  7.6162e-03]])\n",
      "2.weight tensor([[ 0.0102, -0.0048, -0.0062]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    \n",
    "    if 'weight' in name:\n",
    "        init.normal_(param, mean=0, std=0.01)\n",
    "        print(name, param.data)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee120864-6c99-434e-bc67-9be00b403b4b",
   "metadata": {},
   "source": [
    "下面使用常数来初始化权重参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c9dd0e4-2685-4bcc-a8b0-f97fd9bfac6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([0., 0., 0.])\n",
      "2.bias tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        init.constant_(param, val=0)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a1032-775a-4967-9073-66dc34f0b447",
   "metadata": {},
   "source": [
    "## 4.2.3 自定义初始化方法\n",
    "\n",
    "有时候我们需要的初始化方法并没有在`init`模块中提供。这时，可以实现一个初始化方法，从而能够像使用其他初始化方法那样使用它。在这之前我们先来看看PyTorch是怎么实现这些初始化方法的，例如`torch.nn.init.normal_`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb9c8a9-895c-491a-8ca0-cb04ba9d4f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normal_(tensor, mean=0, std=1):\n",
    "    with torch.no_grad():\n",
    "        return tensor.normal_(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9568c180-61b2-4628-ab8c-1448b00a826b",
   "metadata": {},
   "source": [
    "可以看到这就是一个inplace改变`Tensor`值的函数，而且这个过程是不记录梯度的。\n",
    "类似的我们来实现一个自定义的初始化方法。在下面的例子里，我们令权重有一半概率初始化为0，有另一半概率初始化为$[-10,-5]$和$[5,10]$两个区间里均匀分布的随机数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e544f2a7-c3ca-4228-8a6b-60ba2098aabe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-7.4413, -8.1213,  0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  7.3145, -0.0000]])\n",
      "2.weight tensor([[7.3026, 5.2755, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def init_weight_(tensor):\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10, 10)\n",
    "        tensor *= (tensor.abs() >= 5).float()\n",
    "    pass\n",
    "\n",
    "        \n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init_weight_(param)\n",
    "        print(name, param.data)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05804ff1-6264-4465-9f2e-908de6cd6c2a",
   "metadata": {},
   "source": [
    "此外，参考2.3.2节，我们还可以通过改变这些参数的`data`来改写模型参数值同时不会影响梯度:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a683a9-b8a6-44ea-bb24-6c0184e40794",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([1., 1., 1.])\n",
      "2.bias tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        param.data += 1\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb6a5f-b23d-4731-b722-4da4797192ec",
   "metadata": {},
   "source": [
    "## 4.2.4 共享模型参数\n",
    "\n",
    "在有些情况下，我们希望在多个层之间共享模型参数。4.1.3节提到了如何共享模型参数: `Module`类的`forward`函数里多次调用同一个层。此外，如果我们传入`Sequential`的模块是同一个`Module`实例的话参数也是共享的，下面来看一个例子: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19bec8b6-5a5a-411d-a477-b95649d41f61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(1, 1, bias=False)\n",
    "net = nn.Sequential(linear, linear) \n",
    "\n",
    "print(net)\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    init.constant_(param, val=3)\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32403f82-e7ea-4316-ae11-73382147b932",
   "metadata": {},
   "source": [
    "在内存中，这两个线性层其实一个对象:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32f17271-65ff-4bc8-887f-95d80e0b670c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(id(net[0]) == id(net[1]))\n",
    "print(id(net[0].weight) == id(net[1].weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a3f63-7a16-4407-9980-4f0a803ef99e",
   "metadata": {},
   "source": [
    "因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fc1be62-c79a-4322-bdf7-f34eae1a0237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<SumBackward0>)\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, 1)\n",
    "y = net(x).sum()\n",
    "\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(net[0].weight.grad) # 单次梯度是3，两次所以就是6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6549acc-9b1a-42c8-b199-7d771d8eb8af",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 有多种方法来访问、初始化和共享模型参数。\n",
    "* 可以自定义初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399b808d-db68-4290-8a6e-de00c4525578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
