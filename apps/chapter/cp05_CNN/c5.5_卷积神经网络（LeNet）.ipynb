{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26d908c-b1f7-4886-b409-c13bc85c7a17",
   "metadata": {},
   "source": [
    "# 5.5 卷积神经网络（LeNet）\n",
    "\n",
    "在3.9节（多层感知机的从零开始实现）里我们构造了一个含单隐藏层的多层感知机模型来对Fashion-MNIST数据集中的图像进行分类。每张图像高和宽均是28像素。我们将图像中的像素逐行展开，得到长度为784的向量，并输入进全连接层中。然而，这种分类方法有一定的局限性。\n",
    "\n",
    "1. 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。\n",
    "2. 对于大尺寸的输入图像，使用全连接层容易造成模型过大。假设输入是高和宽均为1000像素的彩色照片（含3个通道）。即使全连接层输出个数仍是256，该层权重参数的形状是$3,000,000\\times 256$：它占用了大约3 GB的内存或显存。这带来过复杂的模型和过高的存储开销。\n",
    "\n",
    "卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。\n",
    "\n",
    "卷积神经网络就是含卷积层的网络。本节里我们将介绍一个早期用来识别手写数字图像的卷积神经网络：LeNet [1]。这个名字来源于LeNet论文的第一作者Yann LeCun。LeNet展示了通过梯度下降训练卷积神经网络可以达到手写数字识别在当时最先进的结果。这个奠基性的工作第一次将卷积神经网络推上舞台，为世人所知。LeNet的网络结构如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420571a-7e09-48d9-9c4b-6fad583f46f1",
   "metadata": {},
   "source": [
    "## 5.5.1 LeNet模型\n",
    "\n",
    "LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。\n",
    "\n",
    "卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5\\times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为$2\\times 2$，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。\n",
    "\n",
    "卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。\n",
    "\n",
    "下面我们通过`Sequential`类来实现LeNet模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3dcaf6-2ef3-44c1-ab21-6b872caf8dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57353cc4-5db0-4677-8a02-0ce5ab157544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc7d05-57de-472d-91a4-07663502b017",
   "metadata": {},
   "source": [
    "接下来查看每个层的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f99c81-5e74-4855-9af7-e0d23b8e377b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Sigmoid()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4173da-2733-426f-899d-049707239ca4",
   "metadata": {},
   "source": [
    "可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b50cf-f172-431f-8f32-b5a7bd8ad866",
   "metadata": {},
   "source": [
    "## 5.5.2 获取数据和训练模型\n",
    "\n",
    "下面我们来实验LeNet模型。实验中，我们仍然使用Fashion-MNIST作为训练数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d21707-f7a9-403b-a019-50ea655d8468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508ba4b-02d0-451f-83c5-f895818c402c",
   "metadata": {},
   "source": [
    "因为卷积神经网络计算比多层感知机要复杂，建议使用GPU来加速计算。因此，我们对3.6节（softmax回归的从零开始实现）中描述的`evaluate_accuracy`函数略作修改，使其支持GPU计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5f37c0-beeb-40f4-990d-998855a78ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进。\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    \n",
    "    # 如果没指定device就使用net的device\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device\n",
    "\n",
    "    acc_sum = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                # 评估模式, 这会关闭dropout\n",
    "                net.eval() \n",
    "                \n",
    "                y_hat = net(X.to(device)).argmax(dim=1) == y.to(device)\n",
    "                acc_sum += (y_hat).float().sum().cpu().item()\n",
    "                \n",
    "                # 改回训练模式\n",
    "                net.train() \n",
    "\n",
    "            # 自定义的模型, 3.13节之后不会用到, 不考虑GPU    \n",
    "            else: \n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    \n",
    "                    res_1 = net(X, is_training=False).argmax(dim=1) == y\n",
    "                    acc_sum += (res_1).float().sum().item() \n",
    "                else:\n",
    "                    res_1 = net(X).argmax(dim=1) == y\n",
    "                    acc_sum += (res_1).float().sum().item() \n",
    "            \n",
    "            n += y.shape[0]\n",
    "            pass\n",
    "        pass\n",
    "\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11503a-beee-4e1f-aa73-ac0085e08664",
   "metadata": {},
   "source": [
    "我们同样对3.6节中定义的`train_ch3`函数略作修改，确保计算使用的数据和模型同在内存或显存上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1861b1-adcc-4f93-9ee4-2f8d3f192696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def train_ch5(net, train_iter, test_iter, \n",
    "              batch_size, \n",
    "              optimizer, \n",
    "              device, num_epochs):\n",
    "    \n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    \n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_l_sum = 0.0\n",
    "        train_acc_sum = 0.0\n",
    "        n = 0\n",
    "        batch_count = 0 \n",
    "        start = time.time()\n",
    "        \n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            \n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_l_sum += l.cpu().item()\n",
    "            \n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "            pass\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        \n",
    "        epoch_count = epoch + 1             # 训练批次\n",
    "        loss_cp = train_l_sum / batch_count # 损失计算\n",
    "        train_acc = train_acc_sum / n       # 正确率\n",
    "        time_consume = time.time() - start  # 耗时\n",
    "        \n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time_ %.1f sec'\n",
    "              % (epoch_count, loss_cp, train_acc, test_acc, time_consume)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202bc6a-73a9-4467-8974-6c1f71e5adb8",
   "metadata": {},
   "source": [
    "学习率采用0.001，训练算法使用Adam算法，损失函数使用交叉熵损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0be2a6-c321-49a9-87de-874566d1d629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 1.7805, train acc 0.346, test acc 0.578, time_ 1.6 sec\n",
      "epoch 2, loss 0.9376, train acc 0.637, test acc 0.691, time_ 0.9 sec\n",
      "epoch 3, loss 0.7743, train acc 0.714, test acc 0.718, time_ 0.8 sec\n",
      "epoch 4, loss 0.6908, train acc 0.739, test acc 0.744, time_ 0.9 sec\n",
      "epoch 5, loss 0.6353, train acc 0.755, test acc 0.757, time_ 0.9 sec\n",
      "epoch 6, loss 0.5939, train acc 0.769, test acc 0.772, time_ 0.9 sec\n",
      "epoch 7, loss 0.5635, train acc 0.780, test acc 0.776, time_ 0.9 sec\n",
      "epoch 8, loss 0.5374, train acc 0.790, test acc 0.790, time_ 0.9 sec\n",
      "epoch 9, loss 0.5168, train acc 0.799, test acc 0.798, time_ 0.9 sec\n",
      "epoch 10, loss 0.4971, train acc 0.809, test acc 0.798, time_ 0.8 sec\n",
      "epoch 11, loss 0.4807, train acc 0.817, test acc 0.812, time_ 0.9 sec\n",
      "epoch 12, loss 0.4688, train acc 0.821, test acc 0.817, time_ 0.9 sec\n",
      "epoch 13, loss 0.4563, train acc 0.827, test acc 0.808, time_ 0.8 sec\n",
      "epoch 14, loss 0.4448, train acc 0.832, test acc 0.815, time_ 0.9 sec\n",
      "epoch 15, loss 0.4360, train acc 0.837, test acc 0.824, time_ 0.9 sec\n",
      "epoch 16, loss 0.4254, train acc 0.840, test acc 0.828, time_ 0.9 sec\n",
      "epoch 17, loss 0.4188, train acc 0.843, test acc 0.832, time_ 0.8 sec\n",
      "epoch 18, loss 0.4112, train acc 0.846, test acc 0.835, time_ 0.9 sec\n",
      "epoch 19, loss 0.4012, train acc 0.850, test acc 0.833, time_ 0.9 sec\n",
      "epoch 20, loss 0.3963, train acc 0.851, test acc 0.840, time_ 0.9 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), \n",
    "                             lr=lr\n",
    "                            )\n",
    "\n",
    "\n",
    "train_ch5(net, train_iter, test_iter, \n",
    "          batch_size, optimizer, \n",
    "          device, \n",
    "          num_epochs\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7a5f09-a0cf-4bcd-8dab-2101a2f4c719",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 卷积神经网络就是含卷积层的网络。\n",
    "* LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e0fea-02ba-4c28-972c-f1fd33fdc37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
