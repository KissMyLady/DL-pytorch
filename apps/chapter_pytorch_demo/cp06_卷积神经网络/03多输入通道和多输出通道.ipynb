{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3198b2b2-d148-4bdc-98f3-96d9aa58e1a6",
   "metadata": {},
   "source": [
    "# 5.3 多输入通道和多输出通道\n",
    "\n",
    "前面两节里我们用到的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3\\times h\\times w$的多维数组。我们将大小为3的这一维称为通道（channel）维。本节我们将介绍含多个输入通道或多个输出通道的卷积核。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0476d3-1c61-4c61-a1d6-f2685ba8c851",
   "metadata": {},
   "source": [
    "## 5.3.1 多输入通道\n",
    "\n",
    "当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。假设输入数据的通道数为$c_i$，那么卷积核的输入通道数同样为$c_i$。设卷积核窗口形状为$k_h\\times k_w$。当$c_i=1$时，我们知道卷积核只包含一个形状为$k_h\\times k_w$的二维数组。当$c_i > 1$时，我们将会为每个输入通道各分配一个形状为$k_h\\times k_w$的核数组。把这$c_i$个数组在输入通道维上连结，即得到一个形状为$c_i\\times k_h\\times k_w$的卷积核。由于输入和卷积核各有$c_i$个通道，我们可以在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组。这就是含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出。\n",
    "\n",
    "图5.4展示了含2个输入通道的二维互相关计算的例子。在每个通道上，二维输入数组与二维核数组做互相关运算，再按通道相加即得到输出。图5.4中阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：$(1\\times1+2\\times2+4\\times3+5\\times4)+(0\\times0+1\\times1+3\\times2+4\\times3)=56$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c59119-d219-477f-9b4c-06d24c93c77f",
   "metadata": {},
   "source": [
    "接下来我们实现含多个输入通道的互相关运算。我们只需要对每个通道做互相关运算，然后通过`add_n`函数来进行累加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a150635e-0907-4447-8f5e-efe1bb3958b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f516edf3-04a0-460d-b308-7d03f6dc77b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def corr2d_multi_in(X, K):\n",
    "    # 沿着X和K的第0维（通道维）分别计算再相加\n",
    "    res = d2l.corr2d(X[0, :, :], K[0, :, :])\n",
    "    \n",
    "    for i in range(1, X.shape[0]):\n",
    "        \n",
    "        res += d2l.corr2d(X[i, :, :], K[i, :, :])\n",
    "        \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44c741-28f4-48c9-b510-8fff1803d95e",
   "metadata": {},
   "source": [
    "我们可以构造图5.4中的输入数组`X`、核数组`K`来验证互相关运算的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d42f431-2093-484b-b6f9-ca0008afc90d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0, 1, 2], \n",
    "                   [3, 4, 5], \n",
    "                   [6, 7, 8]],\n",
    "                  \n",
    "                  [[1, 2, 3], \n",
    "                   [4, 5, 6], \n",
    "                   [7, 8, 9]],\n",
    "                 ])\n",
    "\n",
    "\n",
    "K = torch.tensor([[[0, 1],\n",
    "                   [2, 3]],\n",
    "                  \n",
    "                  [[1, 2],\n",
    "                   [3, 4]],\n",
    "                 ])\n",
    "\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47a209bc-e92e-46f2-925a-201e7654b3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = torch.tensor([[[0, 1, 2], \n",
    "                   [3, 4, 5], \n",
    "                   [6, 7, 8]],\n",
    "                  \n",
    "                  [[1, 2, 3], \n",
    "                   [4, 5, 6], \n",
    "                   [7, 8, 9]],\n",
    "                  \n",
    "                  [[1, 2, 3], \n",
    "                   [4, 5, 6], \n",
    "                   [7, 8, 9]]\n",
    "                 ])\n",
    "\n",
    "\n",
    "K = torch.tensor([[[0, 1], \n",
    "                   [2, 3]], \n",
    "                  \n",
    "                  [[2, 3], \n",
    "                   [4, 5]],\n",
    "                  \n",
    "                  [[6, 7], \n",
    "                   [8, 9]],\n",
    "                 ]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c887d759-3746-487d-b538-a40adced547a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 93., 119.],\n",
       "        [171., 197.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多输入 当输出\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56902c-70e5-4f39-9c87-b78033d15d7e",
   "metadata": {},
   "source": [
    "## 5.3.2 多输出通道\n",
    "\n",
    "当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\\times k_h\\times k_w$的核数组。将它们在输出通道维上连结，卷积核的形状即$c_o\\times c_i\\times k_h\\times k_w$。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。\n",
    "\n",
    "下面我们实现一个互相关运算函数来计算多个通道的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3eb2fb83-45fd-4b40-97c9-14e3c784604f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # 对 K的第 0维遍历，每次同输入X做互相关计算。所有结果使用stack函数合并在一起\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15c80f-3972-488e-a0a6-f1c791730e76",
   "metadata": {},
   "source": [
    "我们将核数组`K`同`K+1`（`K`中每个元素加一）和`K+2`连结在一起来构造一个输出通道数为3的卷积核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2db2abff-9652-4b4b-a9fd-3d246baf023b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 2, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.stack([K, K + 1, K + 2])\n",
    "\n",
    "K.shape # torch.Size([3, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cb191a8-196c-4451-a6fa-601fab6188bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0,  1],\n",
       "          [ 2,  3]],\n",
       "\n",
       "         [[ 2,  3],\n",
       "          [ 4,  5]],\n",
       "\n",
       "         [[ 6,  7],\n",
       "          [ 8,  9]]],\n",
       "\n",
       "\n",
       "        [[[ 1,  2],\n",
       "          [ 3,  4]],\n",
       "\n",
       "         [[ 3,  4],\n",
       "          [ 5,  6]],\n",
       "\n",
       "         [[ 7,  8],\n",
       "          [ 9, 10]]],\n",
       "\n",
       "\n",
       "        [[[ 2,  3],\n",
       "          [ 4,  5]],\n",
       "\n",
       "         [[ 4,  5],\n",
       "          [ 6,  7]],\n",
       "\n",
       "         [[ 8,  9],\n",
       "          [10, 11]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb8bcf0a-4ff0-4a37-8c45-7f68423159cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]],\n",
       "\n",
       "        [[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]],\n",
       "\n",
       "        [[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1684a0c-478c-48cc-8087-bc140efdb8f8",
   "metadata": {},
   "source": [
    "下面我们对输入数组`X`与核数组`K`做互相关运算。此时的输出含有3个通道。其中第一个通道的结果与之前输入数组`X`与多输入通道、单输出通道核的计算结果一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03bca182-a98a-4f0c-b9fa-87688f8fb293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[165., 215.],\n",
       "         [315., 365.]],\n",
       "\n",
       "        [[197., 259.],\n",
       "         [383., 445.]],\n",
       "\n",
       "        [[229., 303.],\n",
       "         [451., 525.]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de648da-8fdc-40ef-9677-f24424898f03",
   "metadata": {},
   "source": [
    "## 5.3.3 $1\\times 1$卷积层\n",
    "\n",
    "最后我们讨论卷积窗口形状为$1\\times 1$（$k_h=k_w=1$）的多通道卷积层。我们通常称之为$1\\times 1$卷积层，并将其中的卷积运算称为$1\\times 1$卷积。因为使用了最小窗口，$1\\times 1$卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，$1\\times 1$卷积的主要计算发生在通道维上。图5.5展示了使用输入通道数为3、输出通道数为2的$1\\times 1$卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，**那么$1\\times 1$卷积层的作用与全连接层等价**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39762ac7-0220-4b56-b8f2-d7ce5fd7fc01",
   "metadata": {},
   "source": [
    "下面我们使用全连接层中的矩阵乘法来实现$1\\times 1$卷积。这里需要在矩阵乘法运算前后对数据形状做一些调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba4ebb35-449f-4381-8ef7-6a948f294106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    \n",
    "    X = X.view(c_i, h * w)\n",
    "    K = K.view(c_o, c_i)\n",
    "    \n",
    "    # 全连接层的矩阵乘法\n",
    "    Y = torch.mm(K, X)  \n",
    "    \n",
    "    return Y.view(c_o, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f6db9-d9ff-4993-9d06-c15be6ed7cbc",
   "metadata": {},
   "source": [
    "经验证，做$1\\times 1$卷积时，以上函数与之前实现的互相关运算函数`corr2d_multi_in_out`等价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cee280ed-8186-462c-80f1-4531f6d4e0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.2012, 1.3924, 0.3437],\n",
       "         [1.3365, 1.2579, 0.8526],\n",
       "         [0.9394, 0.7968, 1.7049]],\n",
       "\n",
       "        [[0.5050, 0.5029, 0.1063],\n",
       "         [0.6304, 0.6332, 0.2502],\n",
       "         [0.5914, 0.4787, 0.6261]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(3, 3, 3)\n",
    "K = torch.rand(2, 3, 1, 1)\n",
    "\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f0c9a6d-6956-4381-ac3b-b18c47e78bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.2012, 1.3924, 0.3437],\n",
       "         [1.3365, 1.2579, 0.8526],\n",
       "         [0.9394, 0.7968, 1.7049]],\n",
       "\n",
       "        [[0.5050, 0.5029, 0.1063],\n",
       "         [0.6304, 0.6332, 0.2502],\n",
       "         [0.5914, 0.4787, 0.6261]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f01cc2c-ac8a-4504-a455-191f6e5b8f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1 - Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90cc4dae-5fe3-4265-8451-cc0293699410",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Y1 - Y2).norm().item() < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd0b5f-2839-4e7c-9833-2338bd85a46d",
   "metadata": {},
   "source": [
    "在之后的模型里我们将会看到$1\\times 1$卷积层被当作保持高和宽维度形状不变的全连接层使用。于是，我们可以通过调整网络层之间的通道数来控制模型复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ecd3f-51ad-4d56-b991-2230e7033a2f",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 使用多通道可以拓展卷积层的模型参数。\n",
    "* 假设将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\\times 1$卷积层的作用与全连接层等价。\n",
    "* $1\\times 1$卷积层通常用来调整网络层之间的通道数，并控制模型复杂度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36118e3-40e0-4219-94b6-fe38c6c6836e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
