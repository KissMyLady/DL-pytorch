{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9233fe-16e4-40e2-b9e6-d745abfe55df",
   "metadata": {},
   "source": [
    "# 5.12 稠密连接网络（DenseNet）\n",
    "\n",
    "ResNet中的跨层连接设计引申出了数个后续工作。本节我们介绍其中的一个：稠密连接网络（DenseNet） [1]。 它与ResNet的主要区别如图5.10所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e8d1b-57c1-42da-b299-8d974256c63e",
   "metadata": {},
   "source": [
    "图5.10中将部分前后相邻的运算抽象为模块$A$和模块$B$。与ResNet的主要区别在于，DenseNet里模块$B$的输出不是像ResNet那样和模块$A$的输出相加，而是在通道维上连结。这样模块$A$的输出可以直接传入模块$B$后面的层。在这个设计里，模块$A$直接跟模块$B$后面的所有层连接在了一起。这也是它被称为“稠密连接”的原因。\n",
    "\n",
    "DenseNet的主要构建模块是稠密块（dense block）和过渡层（transition layer）。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fd3c2-9fa8-4cff-ad23-0decbcb0f74f",
   "metadata": {},
   "source": [
    "## 5.12.1 稠密块\n",
    "\n",
    "DenseNet使用了ResNet改良版的“批量归一化、激活和卷积”结构，我们首先在`conv_block`函数里实现这个结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007d413e-f98c-4463-be1c-4f133904a900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jetson/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49309fd-9d81-4361-9ed1-f2c26b6c3972",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels):\n",
    "    blk = nn.Sequential(nn.BatchNorm2d(in_channels), \n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(in_channels, \n",
    "                                  out_channels, \n",
    "                                  kernel_size=3, \n",
    "                                  padding=1)\n",
    "                       )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf3febd-bdb1-4712-b767-512c7ac54308",
   "metadata": {},
   "source": [
    "稠密块由多个`conv_block`组成，每块使用相同的输出通道数。但在前向计算时，我们将每块的输入和输出在通道维上连结。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1037f9c7-cd9c-47ee-b34b-1bae667bbcea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_convs, in_channels, out_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        \n",
    "        net = []\n",
    "        \n",
    "        for i in range(num_convs):\n",
    "            in_c = in_channels + i * out_channels\n",
    "            net.append(conv_block(in_c, out_channels))\n",
    "            pass\n",
    "        \n",
    "        self.net = nn.ModuleList(net)\n",
    "        \n",
    "        # 计算输出通道数\n",
    "        self.out_channels = in_channels + num_convs * out_channels \n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # 在通道维上将输入和输出连结\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598e4b4-7c35-467a-a336-8f6b9bcb4653",
   "metadata": {},
   "source": [
    "在下面的例子中，我们定义一个有2个输出通道数为10的卷积块。使用通道数为3的输入时，我们会得到通道数为$3+2\\times 10=23$的输出。卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e82d94-b2b9-498f-b839-bc82f3408c03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(2, 3, 10)\n",
    "X = torch.rand(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "\n",
    "Y.shape # torch.Size([4, 23, 8, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ca898-b69c-4a21-922f-41c588a78920",
   "metadata": {},
   "source": [
    "## 5.12.2 过渡层\n",
    "\n",
    "由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡层用来控制模型复杂度。它通过$1\\times1$卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3a96f9-8812-49fa-9a99-56fde4685b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transition_block(in_channels, out_channels):\n",
    "    \n",
    "    blk = nn.Sequential(nn.BatchNorm2d(in_channels), \n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                        nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "                       )\n",
    "    \n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e801b-c294-474d-9326-e677abb5962e",
   "metadata": {},
   "source": [
    "对上一个例子中稠密块的输出使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a672ce3f-4422-4ea4-8542-0c772f77d801",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(23, 10)\n",
    "                       \n",
    "                       \n",
    "blk(Y).shape # torch.Size([4, 10, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5b593-0e87-4fd1-8a4f-e6f9410d4232",
   "metadata": {},
   "source": [
    "## 5.12.3 DenseNet模型\n",
    "\n",
    "我们来构造DenseNet模型。DenseNet首先使用同ResNet一样的单卷积层和最大池化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a327dd9-8d57-4aab-82ba-37fa7f21bc1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64), \n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecc790a3-94d0-4a3f-b11c-30cc0547fde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ec6e2-1227-44b0-ab7c-34f04949d661",
   "metadata": {},
   "source": [
    "类似于ResNet接下来使用的4个残差块，DenseNet使用的是4个稠密块。同ResNet一样，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与上一节的ResNet-18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。\n",
    "\n",
    "ResNet里通过步幅为2的残差块在每个模块之间减小高和宽。这里我们则使用过渡层来减半高和宽，并减半通道数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88448c85-b16d-408b-88c5-3a086b495620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_channels, growth_rate = 64, 32  # num_channels为当前的通道数\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    DB = DenseBlock(num_convs, num_channels, growth_rate)\n",
    "    net.add_module(\"DenseBlosk_%d\" % i, DB)\n",
    "    \n",
    "    # 上一个稠密块的输出通道数\n",
    "    num_channels = DB.out_channels\n",
    "    \n",
    "    # 在稠密块之间加入通道数减半的过渡层\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        net.add_module(\"transition_block_%d\" % i, transition_block(num_channels, \n",
    "                                                                   num_channels // 2)\n",
    "                      )\n",
    "        num_channels = num_channels // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39caf7ef-c775-4bd5-9cb5-c6380dcee288",
   "metadata": {},
   "source": [
    "同ResNet一样，最后接上全局池化层和全连接层来输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d2c99a-b780-41da-bab8-d028108e7187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.add_module(\"BN\", nn.BatchNorm2d(num_channels))\n",
    "net.add_module(\"relu\", nn.ReLU())\n",
    "\n",
    "# GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)\n",
    "net.add_module(\"global_avg_pool\", d2l.GlobalAvgPool2d()) \n",
    "\n",
    "net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(), \n",
    "                                   nn.Linear(num_channels, 10))\n",
    "              ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167e70a-5d1a-4004-bff7-eb7c52e0c47f",
   "metadata": {},
   "source": [
    "我们尝试打印每个子模块的X = torch.rand((1, 1, 96, 96))\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, ' output shape:\\t', X.shape)输出维度确保网络无误："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "465540a2-dba2-48c7-8d40-6dbedd00d845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  output shape:\t torch.Size([1, 64, 48, 48])\n",
      "1  output shape:\t torch.Size([1, 64, 48, 48])\n",
      "2  output shape:\t torch.Size([1, 64, 48, 48])\n",
      "3  output shape:\t torch.Size([1, 64, 24, 24])\n",
      "DenseBlosk_0  output shape:\t torch.Size([1, 192, 24, 24])\n",
      "transition_block_0  output shape:\t torch.Size([1, 96, 12, 12])\n",
      "DenseBlosk_1  output shape:\t torch.Size([1, 224, 12, 12])\n",
      "transition_block_1  output shape:\t torch.Size([1, 112, 6, 6])\n",
      "DenseBlosk_2  output shape:\t torch.Size([1, 240, 6, 6])\n",
      "transition_block_2  output shape:\t torch.Size([1, 120, 3, 3])\n",
      "DenseBlosk_3  output shape:\t torch.Size([1, 248, 3, 3])\n",
      "BN  output shape:\t torch.Size([1, 248, 3, 3])\n",
      "relu  output shape:\t torch.Size([1, 248, 3, 3])\n",
      "global_avg_pool  output shape:\t torch.Size([1, 248, 1, 1])\n",
      "fc  output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((1, 1, 96, 96))\n",
    "\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    \n",
    "    print(name, ' output shape:\\t', X.shape)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ccb87d-5d96-4bb4-9f38-cee46890101f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (DenseBlosk_0): DenseBlock(\n",
       "    (net): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_block_0): Sequential(\n",
       "    (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (DenseBlosk_1): DenseBlock(\n",
       "    (net): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_block_1): Sequential(\n",
       "    (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(224, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (DenseBlosk_2): DenseBlock(\n",
       "    (net): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(112, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(144, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(176, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(208, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_block_2): Sequential(\n",
       "    (0): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(240, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (DenseBlosk_3): DenseBlock(\n",
       "    (net): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(120, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(152, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(184, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(216, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (BN): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (global_avg_pool): GlobalAvgPool2d()\n",
       "  (fc): Sequential(\n",
       "    (0): FlattenLayer()\n",
       "    (1): Linear(in_features=248, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e158712c-e9ac-4e1c-9829-7940c2984100",
   "metadata": {},
   "source": [
    "## 5.12.4 获取数据并训练模型\n",
    "\n",
    "由于这里使用了比较深的网络，本节里我们将输入高和宽从224降到96来简化计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6f1841-9fdc-4628-a04c-b9fafdc4eb9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "rootPath = r\"/home/jetson/files/ai_data/FashionMNIST\"\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, \n",
    "                                                    resize=96,\n",
    "                                                    root=rootPath)\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d56886c-994b-4f19-9800-32328276beb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.4640, train acc 0.836, test acc 0.805, time 18.1 sec\n",
      "epoch 2, loss 0.2773, train acc 0.898, test acc 0.808, time 17.3 sec\n",
      "epoch 3, loss 0.2332, train acc 0.915, test acc 0.901, time 17.3 sec\n",
      "epoch 4, loss 0.2080, train acc 0.924, test acc 0.913, time 17.4 sec\n",
      "epoch 5, loss 0.1933, train acc 0.928, test acc 0.875, time 17.4 sec\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "d2l.train_ch5(net, train_iter, test_iter, \n",
    "              batch_size, \n",
    "              optimizer, \n",
    "              device, \n",
    "              num_epochs\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c15a09-5d18-4e21-bfc3-9e0ce5d9e918",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 在跨层连接上，不同于ResNet中将输入与输出相加，DenseNet在通道维上连结输入与输出。\n",
    "* DenseNet的主要构建模块是稠密块和过渡层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560dd279-7cb6-4a41-af07-3bd0c8761c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
