{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6dc597-2b5a-446e-b3a8-368f32891dbe",
   "metadata": {},
   "source": [
    "# 自然语言推断：微调BERT\n",
    ":label:`sec_natural-language-inference-bert`\n",
    "\n",
    "在本章的前面几节中，我们已经为SNLI数据集（ :numref:`sec_natural-language-inference-and-dataset`）上的自然语言推断任务设计了一个基于注意力的结构（ :numref:`sec_natural-language-inference-attention`）。现在，我们通过微调BERT来重新审视这项任务。正如在 :numref:`sec_finetuning-bert`中讨论的那样，自然语言推断是一个序列级别的文本对分类问题，而微调BERT只需要一个额外的基于多层感知机的架构，如 :numref:`fig_nlp-map-nli-bert`中所示。\n",
    "\n",
    "本节将下载一个预训练好的小版本的BERT，然后对其进行微调，以便在SNLI数据集上进行自然语言推断。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c348cb0-31a9-409f-bf05-7ae66ccf0d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# import d2lzh_pytorch.torch as d2l\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a689d856-d206-4d55-b767-9dbb5729e10c",
   "metadata": {},
   "source": [
    "## [**加载预训练的BERT**]\n",
    "\n",
    "我们已经在 :numref:`sec_bert-dataset`和 :numref:`sec_bert-pretraining`WikiText-2数据集上预训练BERT（请注意，原始的BERT模型是在更大的语料库上预训练的）。正如在 :numref:`sec_bert-pretraining`中所讨论的，原始的BERT模型有数以亿计的参数。在下面，我们提供了两个版本的预训练的BERT：“bert.base”与原始的BERT基础模型一样大，需要大量的计算资源才能进行微调，而“bert.small”是一个小版本，以便于演示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ab753d-ba94-4345-b135-1dd88270543d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['bert.base'] = (\n",
    "    d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "    '7b3820b35da691042e5d34c0971ac3edbd80d3f4'\n",
    ")\n",
    "\n",
    "\n",
    "data_dir = r\"/home/mylady/code/python/DL-pytorch/apps/chapter_pytorch_demo/data/bert.base.torch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae532c-3b7c-4518-aee4-fd9498582968",
   "metadata": {},
   "source": [
    "两个预训练好的BERT模型都包含一个定义词表的“vocab.json”文件和一个预训练参数的“pretrained.params”文件。我们实现了以下`load_pretrained_model`函数来[**加载预先训练好的BERT参数**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8aeade7-7b3d-4c9b-aa89-649355995025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# 源地址:\n",
    "https://github.com/d2l-ai/d2l-en/blob/master/chapter_natural-language-processing-applications/natural-language-inference-bert.md\n",
    "\n",
    "'''\n",
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                          num_heads, num_layers, dropout, max_len, devices):\n",
    "    #  data_dir = d2l.download_extract(pretrained_model)\n",
    "    data_dir = r\"/home/mylady/code/python/DL-pytorch/apps/chapter_pytorch_demo/data/bert.base.torch\"\n",
    "\n",
    "    \n",
    "    # 定义空词表以加载预定义词表\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, \n",
    "        'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "        vocab.idx_to_token)}\n",
    "    bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256],\n",
    "                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                         num_heads=4, num_layers=2, dropout=0.2,\n",
    "                         max_len=max_len, key_size=256, query_size=256,\n",
    "                         value_size=256, hid_in_features=256,\n",
    "                         mlm_in_features=256, nsp_in_features=256)\n",
    "    # 加载预训练BERT参数\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir,\n",
    "                                                 'pretrained.params')))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033bc4bd-74eb-408e-86f9-81e8f0a5fa15",
   "metadata": {},
   "source": [
    "为了便于在大多数机器上演示，我们将在本节中加载和微调经过预训练BERT的小版本（“bert.small”）。在练习中，我们将展示如何微调大得多的“bert.base”以显著提高测试精度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb477d42-756c-40e2-8362-d42db6248e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BERTModel:\n\tUnexpected key(s) in state_dict: \"encoder.blks.2.attention.W_q.weight\", \"encoder.blks.2.attention.W_q.bias\", \"encoder.blks.2.attention.W_k.weight\", \"encoder.blks.2.attention.W_k.bias\", \"encoder.blks.2.attention.W_v.weight\", \"encoder.blks.2.attention.W_v.bias\", \"encoder.blks.2.attention.W_o.weight\", \"encoder.blks.2.attention.W_o.bias\", \"encoder.blks.2.addnorm1.ln.weight\", \"encoder.blks.2.addnorm1.ln.bias\", \"encoder.blks.2.ffn.dense1.weight\", \"encoder.blks.2.ffn.dense1.bias\", \"encoder.blks.2.ffn.dense2.weight\", \"encoder.blks.2.ffn.dense2.bias\", \"encoder.blks.2.addnorm2.ln.weight\", \"encoder.blks.2.addnorm2.ln.bias\", \"encoder.blks.3.attention.W_q.weight\", \"encoder.blks.3.attention.W_q.bias\", \"encoder.blks.3.attention.W_k.weight\", \"encoder.blks.3.attention.W_k.bias\", \"encoder.blks.3.attention.W_v.weight\", \"encoder.blks.3.attention.W_v.bias\", \"encoder.blks.3.attention.W_o.weight\", \"encoder.blks.3.attention.W_o.bias\", \"encoder.blks.3.addnorm1.ln.weight\", \"encoder.blks.3.addnorm1.ln.bias\", \"encoder.blks.3.ffn.dense1.weight\", \"encoder.blks.3.ffn.dense1.bias\", \"encoder.blks.3.ffn.dense2.weight\", \"encoder.blks.3.ffn.dense2.bias\", \"encoder.blks.3.addnorm2.ln.weight\", \"encoder.blks.3.addnorm2.ln.bias\", \"encoder.blks.4.attention.W_q.weight\", \"encoder.blks.4.attention.W_q.bias\", \"encoder.blks.4.attention.W_k.weight\", \"encoder.blks.4.attention.W_k.bias\", \"encoder.blks.4.attention.W_v.weight\", \"encoder.blks.4.attention.W_v.bias\", \"encoder.blks.4.attention.W_o.weight\", \"encoder.blks.4.attention.W_o.bias\", \"encoder.blks.4.addnorm1.ln.weight\", \"encoder.blks.4.addnorm1.ln.bias\", \"encoder.blks.4.ffn.dense1.weight\", \"encoder.blks.4.ffn.dense1.bias\", \"encoder.blks.4.ffn.dense2.weight\", \"encoder.blks.4.ffn.dense2.bias\", \"encoder.blks.4.addnorm2.ln.weight\", \"encoder.blks.4.addnorm2.ln.bias\", \"encoder.blks.5.attention.W_q.weight\", \"encoder.blks.5.attention.W_q.bias\", \"encoder.blks.5.attention.W_k.weight\", \"encoder.blks.5.attention.W_k.bias\", \"encoder.blks.5.attention.W_v.weight\", \"encoder.blks.5.attention.W_v.bias\", \"encoder.blks.5.attention.W_o.weight\", \"encoder.blks.5.attention.W_o.bias\", \"encoder.blks.5.addnorm1.ln.weight\", \"encoder.blks.5.addnorm1.ln.bias\", \"encoder.blks.5.ffn.dense1.weight\", \"encoder.blks.5.ffn.dense1.bias\", \"encoder.blks.5.ffn.dense2.weight\", \"encoder.blks.5.ffn.dense2.bias\", \"encoder.blks.5.addnorm2.ln.weight\", \"encoder.blks.5.addnorm2.ln.bias\", \"encoder.blks.6.attention.W_q.weight\", \"encoder.blks.6.attention.W_q.bias\", \"encoder.blks.6.attention.W_k.weight\", \"encoder.blks.6.attention.W_k.bias\", \"encoder.blks.6.attention.W_v.weight\", \"encoder.blks.6.attention.W_v.bias\", \"encoder.blks.6.attention.W_o.weight\", \"encoder.blks.6.attention.W_o.bias\", \"encoder.blks.6.addnorm1.ln.weight\", \"encoder.blks.6.addnorm1.ln.bias\", \"encoder.blks.6.ffn.dense1.weight\", \"encoder.blks.6.ffn.dense1.bias\", \"encoder.blks.6.ffn.dense2.weight\", \"encoder.blks.6.ffn.dense2.bias\", \"encoder.blks.6.addnorm2.ln.weight\", \"encoder.blks.6.addnorm2.ln.bias\", \"encoder.blks.7.attention.W_q.weight\", \"encoder.blks.7.attention.W_q.bias\", \"encoder.blks.7.attention.W_k.weight\", \"encoder.blks.7.attention.W_k.bias\", \"encoder.blks.7.attention.W_v.weight\", \"encoder.blks.7.attention.W_v.bias\", \"encoder.blks.7.attention.W_o.weight\", \"encoder.blks.7.attention.W_o.bias\", \"encoder.blks.7.addnorm1.ln.weight\", \"encoder.blks.7.addnorm1.ln.bias\", \"encoder.blks.7.ffn.dense1.weight\", \"encoder.blks.7.ffn.dense1.bias\", \"encoder.blks.7.ffn.dense2.weight\", \"encoder.blks.7.ffn.dense2.bias\", \"encoder.blks.7.addnorm2.ln.weight\", \"encoder.blks.7.addnorm2.ln.bias\", \"encoder.blks.8.attention.W_q.weight\", \"encoder.blks.8.attention.W_q.bias\", \"encoder.blks.8.attention.W_k.weight\", \"encoder.blks.8.attention.W_k.bias\", \"encoder.blks.8.attention.W_v.weight\", \"encoder.blks.8.attention.W_v.bias\", \"encoder.blks.8.attention.W_o.weight\", \"encoder.blks.8.attention.W_o.bias\", \"encoder.blks.8.addnorm1.ln.weight\", \"encoder.blks.8.addnorm1.ln.bias\", \"encoder.blks.8.ffn.dense1.weight\", \"encoder.blks.8.ffn.dense1.bias\", \"encoder.blks.8.ffn.dense2.weight\", \"encoder.blks.8.ffn.dense2.bias\", \"encoder.blks.8.addnorm2.ln.weight\", \"encoder.blks.8.addnorm2.ln.bias\", \"encoder.blks.9.attention.W_q.weight\", \"encoder.blks.9.attention.W_q.bias\", \"encoder.blks.9.attention.W_k.weight\", \"encoder.blks.9.attention.W_k.bias\", \"encoder.blks.9.attention.W_v.weight\", \"encoder.blks.9.attention.W_v.bias\", \"encoder.blks.9.attention.W_o.weight\", \"encoder.blks.9.attention.W_o.bias\", \"encoder.blks.9.addnorm1.ln.weight\", \"encoder.blks.9.addnorm1.ln.bias\", \"encoder.blks.9.ffn.dense1.weight\", \"encoder.blks.9.ffn.dense1.bias\", \"encoder.blks.9.ffn.dense2.weight\", \"encoder.blks.9.ffn.dense2.bias\", \"encoder.blks.9.addnorm2.ln.weight\", \"encoder.blks.9.addnorm2.ln.bias\", \"encoder.blks.10.attention.W_q.weight\", \"encoder.blks.10.attention.W_q.bias\", \"encoder.blks.10.attention.W_k.weight\", \"encoder.blks.10.attention.W_k.bias\", \"encoder.blks.10.attention.W_v.weight\", \"encoder.blks.10.attention.W_v.bias\", \"encoder.blks.10.attention.W_o.weight\", \"encoder.blks.10.attention.W_o.bias\", \"encoder.blks.10.addnorm1.ln.weight\", \"encoder.blks.10.addnorm1.ln.bias\", \"encoder.blks.10.ffn.dense1.weight\", \"encoder.blks.10.ffn.dense1.bias\", \"encoder.blks.10.ffn.dense2.weight\", \"encoder.blks.10.ffn.dense2.bias\", \"encoder.blks.10.addnorm2.ln.weight\", \"encoder.blks.10.addnorm2.ln.bias\", \"encoder.blks.11.attention.W_q.weight\", \"encoder.blks.11.attention.W_q.bias\", \"encoder.blks.11.attention.W_k.weight\", \"encoder.blks.11.attention.W_k.bias\", \"encoder.blks.11.attention.W_v.weight\", \"encoder.blks.11.attention.W_v.bias\", \"encoder.blks.11.attention.W_o.weight\", \"encoder.blks.11.attention.W_o.bias\", \"encoder.blks.11.addnorm1.ln.weight\", \"encoder.blks.11.addnorm1.ln.bias\", \"encoder.blks.11.ffn.dense1.weight\", \"encoder.blks.11.ffn.dense1.bias\", \"encoder.blks.11.ffn.dense2.weight\", \"encoder.blks.11.ffn.dense2.bias\", \"encoder.blks.11.addnorm2.ln.weight\", \"encoder.blks.11.addnorm2.ln.bias\". \n\tsize mismatch for encoder.pos_embedding: copying a param with shape torch.Size([1, 512, 768]) from checkpoint, the shape in current model is torch.Size([1, 512, 256]).\n\tsize mismatch for encoder.token_embedding.weight: copying a param with shape torch.Size([60005, 768]) from checkpoint, the shape in current model is torch.Size([60005, 256]).\n\tsize mismatch for encoder.segment_embedding.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 256]).\n\tsize mismatch for encoder.blks.0.attention.W_q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.0.attention.W_q.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.attention.W_k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.0.attention.W_k.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.attention.W_v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.0.attention.W_v.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.attention.W_o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.0.attention.W_o.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.addnorm1.ln.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.addnorm1.ln.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.ffn.dense1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for encoder.blks.0.ffn.dense1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.blks.0.ffn.dense2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for encoder.blks.0.ffn.dense2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.addnorm2.ln.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.addnorm2.ln.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.attention.W_q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.1.attention.W_q.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.attention.W_k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.1.attention.W_k.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.attention.W_v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.1.attention.W_v.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.attention.W_o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.1.attention.W_o.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.addnorm1.ln.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.addnorm1.ln.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.ffn.dense1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for encoder.blks.1.ffn.dense1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.blks.1.ffn.dense2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for encoder.blks.1.ffn.dense2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.addnorm2.ln.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.addnorm2.ln.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for hidden.0.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for hidden.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mlm.mlp.0.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for mlm.mlp.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mlm.mlp.2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mlm.mlp.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mlm.mlp.3.weight: copying a param with shape torch.Size([60005, 768]) from checkpoint, the shape in current model is torch.Size([60005, 256]).\n\tsize mismatch for nsp.output.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m devices \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mtry_all_gpus()\n\u001b[0;32m----> 4\u001b[0m bert, vocab \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert.best\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mnum_hiddens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mffn_num_hiddens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# bert, vocab = load_pretrained_model('bert.best', \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#                                     num_hiddens=768, \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#                                     ffn_num_hiddens=3072, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#                                     devices=devices\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#                                    )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mload_pretrained_model\u001b[0;34m(pretrained_model, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, devices)\u001b[0m\n\u001b[1;32m     18\u001b[0m bert \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mBERTModel(\u001b[38;5;28mlen\u001b[39m(vocab), num_hiddens, norm_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m],\n\u001b[1;32m     19\u001b[0m                      ffn_num_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, ffn_num_hiddens\u001b[38;5;241m=\u001b[39mffn_num_hiddens,\n\u001b[1;32m     20\u001b[0m                      num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     21\u001b[0m                      max_len\u001b[38;5;241m=\u001b[39mmax_len, key_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, query_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     22\u001b[0m                      value_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, hid_in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     23\u001b[0m                      mlm_in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, nsp_in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 加载预训练BERT参数\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpretrained.params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bert, vocab\n",
      "File \u001b[0;32m/home/mylady/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BERTModel:\n\tUnexpected key(s) in state_dict: \"encoder.blks.2.attention.W_q.weight\", \"encoder.blks.2.attention.W_q.bias\", \"encoder.blks.2.attention.W_k.weight\", \"encoder.blks.2.attention.W_k.bias\", \"encoder.blks.2.attention.W_v.weight\", \"encoder.blks.2.attention.W_v.bias\", \"encoder.blks.2.attention.W_o.weight\", \"encoder.blks.2.attention.W_o.bias\", \"encoder.blks.2.addnorm1.ln.weight\", \"encoder.blks.2.addnorm1.ln.bias\", \"encoder.blks.2.ffn.dense1.weight\", \"encoder.blks.2.ffn.dense1.bias\", \"encoder.blks.2.ffn.dense2.weight\", \"encoder.blks.2.ffn.dense2.bias\", \"encoder.blks.2.addnorm2.ln.weight\", \"encoder.blks.2.addnorm2.ln.bias\", \"encoder.blks.3.attention.W_q.weight\", \"encoder.blks.3.attention.W_q.bias\", \"encoder.blks.3.attention.W_k.weight\", \"encoder.blks.3.attention.W_k.bias\", \"encoder.blks.3.attention.W_v.weight\", \"encoder.blks.3.attention.W_v.bias\", \"encoder.blks.3.attention.W_o.weight\", \"encoder.blks.3.attention.W_o.bias\", \"encoder.blks.3.addnorm1.ln.weight\", \"encoder.blks.3.addnorm1.ln.bias\", \"encoder.blks.3.ffn.dense1.weight\", \"encoder.blks.3.ffn.dense1.bias\", \"encoder.blks.3.ffn.dense2.weight\", \"encoder.blks.3.ffn.dense2.bias\", \"encoder.blks.3.addnorm2.ln.weight\", \"encoder.blks.3.addnorm2.ln.bias\", \"encoder.blks.4.attention.W_q.weight\", \"encoder.blks.4.attention.W_q.bias\", \"encoder.blks.4.attention.W_k.weight\", \"encoder.blks.4.attention.W_k.bias\", \"encoder.blks.4.attention.W_v.weight\", \"encoder.blks.4.attention.W_v.bias\", \"encoder.blks.4.attention.W_o.weight\", \"encoder.blks.4.attention.W_o.bias\", \"encoder.blks.4.addnorm1.ln.weight\", \"encoder.blks.4.addnorm1.ln.bias\", \"encoder.blks.4.ffn.dense1.weight\", \"encoder.blks.4.ffn.dense1.bias\", \"encoder.blks.4.ffn.dense2.weight\", \"encoder.blks.4.ffn.dense2.bias\", \"encoder.blks.4.addnorm2.ln.weight\", \"encoder.blks.4.addnorm2.ln.bias\", \"encoder.blks.5.attention.W_q.weight\", \"encoder.blks.5.attention.W_q.bias\", \"encoder.blks.5.attention.W_k.weight\", \"encoder.blks.5.attention.W_k.bias\", \"encoder.blks.5.attention.W_v.weight\", \"encoder.blks.5.attention.W_v.bias\", \"encoder.blks.5.attention.W_o.weight\", \"encoder.blks.5.attention.W_o.bias\", \"encoder.blks.5.addnorm1.ln.weight\", \"encoder.blks.5.addnorm1.ln.bias\", \"encoder.blks.5.ffn.dense1.weight\", \"encoder.blks.5.ffn.dense1.bias\", \"encoder.blks.5.ffn.dense2.weight\", \"encoder.blks.5.ffn.dense2.bias\", \"encoder.blks.5.addnorm2.ln.weight\", \"encoder.blks.5.addnorm2.ln.bias\", \"encoder.blks.6.attention.W_q.weight\", \"encoder.blks.6.attention.W_q.bias\", \"encoder.blks.6.attention.W_k.weight\", \"encoder.blks.6.attention.W_k.bias\", \"encoder.blks.6.attention.W_v.weight\", \"encoder.blks.6.attention.W_v.bias\", \"encoder.blks.6.attention.W_o.weight\", \"encoder.blks.6.attention.W_o.bias\", \"encoder.blks.6.addnorm1.ln.weight\", \"encoder.blks.6.addnorm1.ln.bias\", \"encoder.blks.6.ffn.dense1.weight\", \"encoder.blks.6.ffn.dense1.bias\", \"encoder.blks.6.ffn.dense2.weight\", \"encoder.blks.6.ffn.dense2.bias\", \"encoder.blks.6.addnorm2.ln.weight\", \"encoder.blks.6.addnorm2.ln.bias\", \"encoder.blks.7.attention.W_q.weight\", \"encoder.blks.7.attention.W_q.bias\", \"encoder.blks.7.attention.W_k.weight\", \"encoder.blks.7.attention.W_k.bias\", \"encoder.blks.7.attention.W_v.weight\", \"encoder.blks.7.attention.W_v.bias\", \"encoder.blks.7.attention.W_o.weight\", \"encoder.blks.7.attention.W_o.bias\", \"encoder.blks.7.addnorm1.ln.weight\", \"encoder.blks.7.addnorm1.ln.bias\", \"encoder.blks.7.ffn.dense1.weight\", \"encoder.blks.7.ffn.dense1.bias\", \"encoder.blks.7.ffn.dense2.weight\", \"encoder.blks.7.ffn.dense2.bias\", \"encoder.blks.7.addnorm2.ln.weight\", \"encoder.blks.7.addnorm2.ln.bias\", \"encoder.blks.8.attention.W_q.weight\", \"encoder.blks.8.attention.W_q.bias\", \"encoder.blks.8.attention.W_k.weight\", \"encoder.blks.8.attention.W_k.bias\", \"encoder.blks.8.attention.W_v.weight\", \"encoder.blks.8.attention.W_v.bias\", \"encoder.blks.8.attention.W_o.weight\", \"encoder.blks.8.attention.W_o.bias\", \"encoder.blks.8.addnorm1.ln.weight\", \"encoder.blks.8.addnorm1.ln.bias\", \"encoder.blks.8.ffn.dense1.weight\", \"encoder.blks.8.ffn.dense1.bias\", \"encoder.blks.8.ffn.dense2.weight\", \"encoder.blks.8.ffn.dense2.bias\", \"encoder.blks.8.addnorm2.ln.weight\", \"encoder.blks.8.addnorm2.ln.bias\", \"encoder.blks.9.attention.W_q.weight\", \"encoder.blks.9.attention.W_q.bias\", \"encoder.blks.9.attention.W_k.weight\", \"encoder.blks.9.attention.W_k.bias\", \"encoder.blks.9.attention.W_v.weight\", \"encoder.blks.9.attention.W_v.bias\", \"encoder.blks.9.attention.W_o.weight\", \"encoder.blks.9.attention.W_o.bias\", \"encoder.blks.9.addnorm1.ln.weight\", \"encoder.blks.9.addnorm1.ln.bias\", \"encoder.blks.9.ffn.dense1.weight\", \"encoder.blks.9.ffn.dense1.bias\", \"encoder.blks.9.ffn.dense2.weight\", \"encoder.blks.9.ffn.dense2.bias\", \"encoder.blks.9.addnorm2.ln.weight\", \"encoder.blks.9.addnorm2.ln.bias\", \"encoder.blks.10.attention.W_q.weight\", \"encoder.blks.10.attention.W_q.bias\", \"encoder.blks.10.attention.W_k.weight\", \"encoder.blks.10.attention.W_k.bias\", \"encoder.blks.10.attention.W_v.weight\", \"encoder.blks.10.attention.W_v.bias\", \"encoder.blks.10.attention.W_o.weight\", \"encoder.blks.10.attention.W_o.bias\", \"encoder.blks.10.addnorm1.ln.weight\", \"encoder.blks.10.addnorm1.ln.bias\", \"encoder.blks.10.ffn.dense1.weight\", \"encoder.blks.10.ffn.dense1.bias\", \"encoder.blks.10.ffn.dense2.weight\", \"encoder.blks.10.ffn.dense2.bias\", \"encoder.blks.10.addnorm2.ln.weight\", \"encoder.blks.10.addnorm2.ln.bias\", \"encoder.blks.11.attention.W_q.weight\", \"encoder.blks.11.attention.W_q.bias\", \"encoder.blks.11.attention.W_k.weight\", \"encoder.blks.11.attention.W_k.bias\", \"encoder.blks.11.attention.W_v.weight\", \"encoder.blks.11.attention.W_v.bias\", \"encoder.blks.11.attention.W_o.weight\", \"encoder.blks.11.attention.W_o.bias\", \"encoder.blks.11.addnorm1.ln.weight\", \"encoder.blks.11.addnorm1.ln.bias\", \"encoder.blks.11.ffn.dense1.weight\", \"encoder.blks.11.ffn.dense1.bias\", \"encoder.blks.11.ffn.dense2.weight\", \"encoder.blks.11.ffn.dense2.bias\", \"encoder.blks.11.addnorm2.ln.weight\", \"encoder.blks.11.addnorm2.ln.bias\". \n\tsize mismatch for encoder.pos_embedding: copying a param with shape torch.Size([1, 512, 768]) from checkpoint, the shape in current model is torch.Size([1, 512, 256]).\n\tsize mismatch for encoder.token_embedding.weight: copying a param with shape torch.Size([60005, 768]) from checkpoint, the shape in current model is torch.Size([60005, 256]).\n\tsize mismatch for encoder.segment_embedding.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 256]).\n\tsize mismatch for encoder.blks.0.attention.W_q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.0.attention.W_q.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.attention.W_k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.0.attention.W_k.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.attention.W_v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.0.attention.W_v.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.attention.W_o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.0.attention.W_o.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.addnorm1.ln.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.addnorm1.ln.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.ffn.dense1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for encoder.blks.0.ffn.dense1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.blks.0.ffn.dense2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for encoder.blks.0.ffn.dense2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.addnorm2.ln.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.0.addnorm2.ln.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.attention.W_q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.1.attention.W_q.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.attention.W_k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.1.attention.W_k.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.attention.W_v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.1.attention.W_v.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.attention.W_o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.blks.1.attention.W_o.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.addnorm1.ln.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.addnorm1.ln.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.ffn.dense1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for encoder.blks.1.ffn.dense1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.blks.1.ffn.dense2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for encoder.blks.1.ffn.dense2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.addnorm2.ln.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.blks.1.addnorm2.ln.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for hidden.0.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for hidden.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mlm.mlp.0.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for mlm.mlp.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mlm.mlp.2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mlm.mlp.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mlm.mlp.3.weight: copying a param with shape torch.Size([60005, 768]) from checkpoint, the shape in current model is torch.Size([60005, 256]).\n\tsize mismatch for nsp.output.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 256])."
     ]
    }
   ],
   "source": [
    "devices = d2l.try_all_gpus()\n",
    "\n",
    "\n",
    "# bert, vocab = load_pretrained_model('bert.best', \n",
    "#                                     num_hiddens=256, \n",
    "#                                     ffn_num_hiddens=512, \n",
    "#                                     num_heads=4,\n",
    "#                                     num_layers=2, \n",
    "#                                     dropout=0.1, \n",
    "#                                     max_len=512, \n",
    "#                                     devices=devices\n",
    "#                                    )\n",
    "\n",
    "\n",
    "bert, vocab = load_pretrained_model('bert.best', \n",
    "                                    num_hiddens=768, \n",
    "                                    ffn_num_hiddens=3072, \n",
    "                                    num_heads=12,\n",
    "                                    num_layers=12, \n",
    "                                    dropout=0.1, \n",
    "                                    max_len=512, \n",
    "                                    devices=devices\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ec60c-b4a5-4ddc-8dc9-69aab70036dd",
   "metadata": {},
   "source": [
    "## [**微调BERT的数据集**]\n",
    "\n",
    "对于SNLI数据集的下游任务自然语言推断，我们定义了一个定制的数据集类`SNLIBERTDataset`。在每个样本中，前提和假设形成一对文本序列，并被打包成一个BERT输入序列，如 :numref:`fig_bert-two-seqs`所示。回想 :numref:`subsec_bert_input_rep`，片段索引用于区分BERT输入序列中的前提和假设。利用预定义的BERT输入序列的最大长度（`max_len`），持续移除输入文本对中较长文本的最后一个标记，直到满足`max_len`。为了加速生成用于微调BERT的SNLI数据集，我们使用4个工作进程并行生成训练或测试样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa7256a9-a4dd-479a-a347-fb87044645de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [\n",
    "            [p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        (self.all_token_ids, \n",
    "         self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        \n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # 使用4个进程\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        all_token_ids = [token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        \n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a9733-4633-48d4-ac5b-00f8c7cdaf32",
   "metadata": {},
   "source": [
    "下载完SNLI数据集后，我们通过实例化`SNLIBERTDataset`类来[**生成训练和测试样本**]。这些样本将在自然语言推断的训练和测试期间进行小批量读取。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e947b2c1-ff08-40bc-a2a9-5cfe57b885f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    }
   ],
   "source": [
    "# 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "batch_size = 512\n",
    "max_len = 128\n",
    "num_workers = d2l.get_dataloader_workers()\n",
    "\n",
    "\n",
    "data_dir = d2l.download_extract('SNLI')\n",
    "\n",
    "\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "train_iter = torch.utils.data.DataLoader(train_set, \n",
    "                                         batch_size, \n",
    "                                         shuffle=True,\n",
    "                                         num_workers=num_workers)\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(test_set, \n",
    "                                        batch_size,\n",
    "                                        num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd82696-434f-4efb-b5aa-fca8a985eec7",
   "metadata": {},
   "source": [
    "## 微调BERT\n",
    "\n",
    "如 :numref:`fig_bert-two-seqs`所示，用于自然语言推断的微调BERT只需要一个额外的多层感知机，该多层感知机由两个全连接层组成（请参见下面`BERTClassifier`类中的`self.hidden`和`self.output`）。[**这个多层感知机将特殊的“&lt;cls&gt;”词元**]的BERT表示进行了转换，该词元同时编码前提和假设的信息(**为自然语言推断的三个输出**)：蕴涵、矛盾和中性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65f2c9dc-9498-46cf-b239-4e339268e1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c34e3-18c5-4ce6-91ee-55a771157be9",
   "metadata": {},
   "source": [
    "在下文中，预训练的BERT模型`bert`被送到用于下游应用的`BERTClassifier`实例`net`中。在BERT微调的常见实现中，只有额外的多层感知机（`net.output`）的输出层的参数将从零开始学习。预训练BERT编码器（`net.encoder`）和额外的多层感知机的隐藏层（`net.hidden`）的所有参数都将进行微调。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a38eadc4-a4ec-4d7d-a31b-e2f1b8c8574a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = BERTClassifier(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b738140d-6dad-490d-bb02-1141b5649671",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClassifier(\n",
       "  (encoder): BERTEncoder(\n",
       "    (token_embedding): Embedding(60005, 768)\n",
       "    (segment_embedding): Embedding(2, 768)\n",
       "    (blks): Sequential(\n",
       "      (0): EncoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (W_q): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (W_k): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (W_v): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (W_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=256, out_features=3072, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (W_q): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (W_k): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (W_v): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (W_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=256, out_features=3072, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (hidden): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (output): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b12488-0070-45a1-9f21-afd9855882ab",
   "metadata": {},
   "source": [
    "回想一下，在 :numref:`sec_bert`中，`MaskLM`类和`NextSentencePred`类在其使用的多层感知机中都有一些参数。这些参数是预训练BERT模型`bert`中参数的一部分，因此是`net`中的参数的一部分。然而，这些参数仅用于计算预训练过程中的遮蔽语言模型损失和下一句预测损失。这两个损失函数与微调下游应用无关，因此当BERT微调时，`MaskLM`和`NextSentencePred`中采用的多层感知机的参数不会更新（陈旧的，staled）。\n",
    "\n",
    "为了允许具有陈旧梯度的参数，标志`ignore_stale_grad=True`在`step`函数`d2l.train_batch_ch13`中被设置。我们通过该函数使用SNLI的训练集（`train_iter`）和测试集（`test_iter`）对`net`模型进行训练和评估。由于计算资源有限，[**训练**]和测试精度可以进一步提高：我们把对它的讨论留在练习中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb20bcf9-e29c-4da5-8434-cf900d4ace4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2624a59b-d666-4b04-9287-5cfb06baaa28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (65536x768 and 256x768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 微调\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ch13\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m               \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m               \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m               \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m               \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m               \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m               \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/python/DL-pytorch/apps/chapter_pytorch_demo/cp15_自然语言处理_应用/../d2lzh_pytorch/torch.py:1594\u001b[0m, in \u001b[0;36mtrain_ch13\u001b[0;34m(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (features, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_iter):\n\u001b[1;32m   1593\u001b[0m     timer\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 1594\u001b[0m     l, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch_ch13\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1596\u001b[0m     metric\u001b[38;5;241m.\u001b[39madd(l, acc, labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], labels\u001b[38;5;241m.\u001b[39mnumel())\n\u001b[1;32m   1597\u001b[0m     timer\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/code/python/DL-pytorch/apps/chapter_pytorch_demo/cp15_自然语言处理_应用/../d2lzh_pytorch/torch.py:1571\u001b[0m, in \u001b[0;36mtrain_batch_ch13\u001b[0;34m(net, X, y, loss, trainer, devices)\u001b[0m\n\u001b[1;32m   1569\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m   1570\u001b[0m trainer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m-> 1571\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1572\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(pred, y)\n\u001b[1;32m   1573\u001b[0m l\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:169\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m, in \u001b[0;36mBERTClassifier.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     10\u001b[0m     tokens_X, segments_X, valid_lens_x \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m---> 11\u001b[0m     encoded_X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_lens_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden(encoded_X[:, \u001b[38;5;241m0\u001b[39m, :]))\n",
      "File \u001b[0;32m~/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/python/DL-pytorch/apps/chapter_pytorch_demo/cp15_自然语言处理_应用/../d2lzh_pytorch/torch.py:2304\u001b[0m, in \u001b[0;36mBERTEncoder.forward\u001b[0;34m(self, tokens, segments, valid_lens)\u001b[0m\n\u001b[1;32m   2302\u001b[0m X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\u001b[38;5;241m.\u001b[39mdata[:, :X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], :]\n\u001b[1;32m   2303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblks:\n\u001b[0;32m-> 2304\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/python/DL-pytorch/apps/chapter_pytorch_demo/cp15_自然语言处理_应用/../d2lzh_pytorch/torch.py:1374\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, X, valid_lens)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, valid_lens):\n\u001b[0;32m-> 1374\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddnorm1(X, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_lens\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddnorm2(Y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(Y))\n",
      "File \u001b[0;32m~/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/python/DL-pytorch/apps/chapter_pytorch_demo/cp15_自然语言处理_应用/../d2lzh_pytorch/torch.py:1263\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, queries, keys, values, valid_lens)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, queries, keys, values, valid_lens):\n\u001b[1;32m   1256\u001b[0m     \u001b[38;5;66;03m# Shape of `queries`, `keys`, or `values`:\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;66;03m# (`batch_size`, no. of queries or key-value pairs, `num_hiddens`)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;66;03m# (`batch_size` * `num_heads`, no. of queries or key-value pairs,\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;66;03m# `num_hiddens` / `num_heads`)\u001b[39;00m\n\u001b[0;32m-> 1263\u001b[0m     queries \u001b[38;5;241m=\u001b[39m transpose_qkv(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m   1264\u001b[0m     keys \u001b[38;5;241m=\u001b[39m transpose_qkv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_k(keys), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m   1265\u001b[0m     values \u001b[38;5;241m=\u001b[39m transpose_qkv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_v(values), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n",
      "File \u001b[0;32m~/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (65536x768 and 256x768)"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"240.554688pt\" height=\"173.477344pt\" viewBox=\"0 0 240.554688 173.477344\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-04-07T00:00:57.468851</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 173.477344 \n",
       "L 240.554688 173.477344 \n",
       "L 240.554688 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 225.403125 149.599219 \n",
       "L 225.403125 10.999219 \n",
       "L 30.103125 10.999219 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m3a8f6af113\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3a8f6af113\" x=\"30.103125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(22.151563 164.197656)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3a8f6af113\" x=\"69.163125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(61.211563 164.197656)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3a8f6af113\" x=\"108.223125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(100.271563 164.197656)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3a8f6af113\" x=\"147.283125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(139.331563 164.197656)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3a8f6af113\" x=\"186.343125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(178.391563 164.197656)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3a8f6af113\" x=\"225.403125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(217.451563 164.197656)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m33d6f24ecf\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m33d6f24ecf\" x=\"30.103125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(7.2 153.398438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m33d6f24ecf\" x=\"30.103125\" y=\"121.879219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(7.2 125.678438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m33d6f24ecf\" x=\"30.103125\" y=\"94.159219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(7.2 97.958438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m33d6f24ecf\" x=\"30.103125\" y=\"66.439219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(7.2 70.238438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m33d6f24ecf\" x=\"30.103125\" y=\"38.719219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(7.2 42.518438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m33d6f24ecf\" x=\"30.103125\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 30.103125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 225.403125 149.599219 \n",
       "L 225.403125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 225.403125 149.599219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 10.999219 \n",
       "L 225.403125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 微调\n",
    "d2l.train_ch13(net, \n",
    "               train_iter, \n",
    "               test_iter, \n",
    "               loss, \n",
    "               trainer, \n",
    "               num_epochs,\n",
    "               devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2522749-d639-42da-ad54-8d313abe3b2b",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们可以针对下游应用对预训练的BERT模型进行微调，例如在SNLI数据集上进行自然语言推断。\n",
    "* 在微调过程中，BERT模型成为下游应用模型的一部分。仅与训练前损失相关的参数在微调期间不会更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80c487-62a9-4f56-84f9-99acb123eb21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
