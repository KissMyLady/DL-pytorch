{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e07cca60-8e36-4b8c-b790-e6c90100a7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from d2lzh_pytorch.BERT_unit_v3.use_BERT import load_pretrained_model, \\\n",
    "SNLIBERTDataset, BERTClassifier, load_data\n",
    "\n",
    "\n",
    "from d2lzh_pytorch.myUtils import try_all_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a6e357-026f-4446-871f-a520a9ee0906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入模型\n",
    "from d2lzh_pytorch.BERT_unit.BERT_model import BERTModel\n",
    "\n",
    "\n",
    "vocab_len = 60005\n",
    "\n",
    "num_hiddens = 256\n",
    "\n",
    "ffn_num_hiddens = 512\n",
    "\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1a9262c7-88b5-4c2d-b795-4839a84d1514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# bert模型\n",
    "bert = BERTModel(vocab_size=60005,\n",
    "                 num_hiddens=768,\n",
    "                 norm_shape=[768],\n",
    "                 \n",
    "                 ffn_num_input=768,\n",
    "                 ffn_num_hiddens=3072,\n",
    "                 \n",
    "                 num_heads=4,\n",
    "                 num_layers=2,\n",
    "                 dropout=0.2,\n",
    "                 max_len=512,\n",
    "                 \n",
    "                 key_size=768,\n",
    "                 query_size=768,\n",
    "                 value_size=768,\n",
    "                 \n",
    "                 hid_in_features=768,\n",
    "                 mlm_in_features=768,\n",
    "                 nsp_in_features=768\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb08c3d-bc3c-4e25-9812-7d48cdd07902",
   "metadata": {},
   "source": [
    "## 加载 bert.base 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d7961cc9-4192-46e3-b39b-7133879a09f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 路径配置\n",
    "base_path = r\"/home/mylady/code/python/DL-pytorch/apps/chapter_pytorch_demo/data\"\n",
    "target_path = r\"bert.base.torch/pretrained.params\"\n",
    "\n",
    "data_dir = os.path.join(base_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4db874e-2e4e-4d64-b353-2fc273c55f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.load(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef350603-91ec-4e45-b51e-fd7ec49d7cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BERTModel:\n\tUnexpected key(s) in state_dict: \"encoder.blks.2.attention.W_q.weight\", \"encoder.blks.2.attention.W_q.bias\", \"encoder.blks.2.attention.W_k.weight\", \"encoder.blks.2.attention.W_k.bias\", \"encoder.blks.2.attention.W_v.weight\", \"encoder.blks.2.attention.W_v.bias\", \"encoder.blks.2.attention.W_o.weight\", \"encoder.blks.2.attention.W_o.bias\", \"encoder.blks.2.addnorm1.ln.weight\", \"encoder.blks.2.addnorm1.ln.bias\", \"encoder.blks.2.ffn.dense1.weight\", \"encoder.blks.2.ffn.dense1.bias\", \"encoder.blks.2.ffn.dense2.weight\", \"encoder.blks.2.ffn.dense2.bias\", \"encoder.blks.2.addnorm2.ln.weight\", \"encoder.blks.2.addnorm2.ln.bias\", \"encoder.blks.3.attention.W_q.weight\", \"encoder.blks.3.attention.W_q.bias\", \"encoder.blks.3.attention.W_k.weight\", \"encoder.blks.3.attention.W_k.bias\", \"encoder.blks.3.attention.W_v.weight\", \"encoder.blks.3.attention.W_v.bias\", \"encoder.blks.3.attention.W_o.weight\", \"encoder.blks.3.attention.W_o.bias\", \"encoder.blks.3.addnorm1.ln.weight\", \"encoder.blks.3.addnorm1.ln.bias\", \"encoder.blks.3.ffn.dense1.weight\", \"encoder.blks.3.ffn.dense1.bias\", \"encoder.blks.3.ffn.dense2.weight\", \"encoder.blks.3.ffn.dense2.bias\", \"encoder.blks.3.addnorm2.ln.weight\", \"encoder.blks.3.addnorm2.ln.bias\", \"encoder.blks.4.attention.W_q.weight\", \"encoder.blks.4.attention.W_q.bias\", \"encoder.blks.4.attention.W_k.weight\", \"encoder.blks.4.attention.W_k.bias\", \"encoder.blks.4.attention.W_v.weight\", \"encoder.blks.4.attention.W_v.bias\", \"encoder.blks.4.attention.W_o.weight\", \"encoder.blks.4.attention.W_o.bias\", \"encoder.blks.4.addnorm1.ln.weight\", \"encoder.blks.4.addnorm1.ln.bias\", \"encoder.blks.4.ffn.dense1.weight\", \"encoder.blks.4.ffn.dense1.bias\", \"encoder.blks.4.ffn.dense2.weight\", \"encoder.blks.4.ffn.dense2.bias\", \"encoder.blks.4.addnorm2.ln.weight\", \"encoder.blks.4.addnorm2.ln.bias\", \"encoder.blks.5.attention.W_q.weight\", \"encoder.blks.5.attention.W_q.bias\", \"encoder.blks.5.attention.W_k.weight\", \"encoder.blks.5.attention.W_k.bias\", \"encoder.blks.5.attention.W_v.weight\", \"encoder.blks.5.attention.W_v.bias\", \"encoder.blks.5.attention.W_o.weight\", \"encoder.blks.5.attention.W_o.bias\", \"encoder.blks.5.addnorm1.ln.weight\", \"encoder.blks.5.addnorm1.ln.bias\", \"encoder.blks.5.ffn.dense1.weight\", \"encoder.blks.5.ffn.dense1.bias\", \"encoder.blks.5.ffn.dense2.weight\", \"encoder.blks.5.ffn.dense2.bias\", \"encoder.blks.5.addnorm2.ln.weight\", \"encoder.blks.5.addnorm2.ln.bias\", \"encoder.blks.6.attention.W_q.weight\", \"encoder.blks.6.attention.W_q.bias\", \"encoder.blks.6.attention.W_k.weight\", \"encoder.blks.6.attention.W_k.bias\", \"encoder.blks.6.attention.W_v.weight\", \"encoder.blks.6.attention.W_v.bias\", \"encoder.blks.6.attention.W_o.weight\", \"encoder.blks.6.attention.W_o.bias\", \"encoder.blks.6.addnorm1.ln.weight\", \"encoder.blks.6.addnorm1.ln.bias\", \"encoder.blks.6.ffn.dense1.weight\", \"encoder.blks.6.ffn.dense1.bias\", \"encoder.blks.6.ffn.dense2.weight\", \"encoder.blks.6.ffn.dense2.bias\", \"encoder.blks.6.addnorm2.ln.weight\", \"encoder.blks.6.addnorm2.ln.bias\", \"encoder.blks.7.attention.W_q.weight\", \"encoder.blks.7.attention.W_q.bias\", \"encoder.blks.7.attention.W_k.weight\", \"encoder.blks.7.attention.W_k.bias\", \"encoder.blks.7.attention.W_v.weight\", \"encoder.blks.7.attention.W_v.bias\", \"encoder.blks.7.attention.W_o.weight\", \"encoder.blks.7.attention.W_o.bias\", \"encoder.blks.7.addnorm1.ln.weight\", \"encoder.blks.7.addnorm1.ln.bias\", \"encoder.blks.7.ffn.dense1.weight\", \"encoder.blks.7.ffn.dense1.bias\", \"encoder.blks.7.ffn.dense2.weight\", \"encoder.blks.7.ffn.dense2.bias\", \"encoder.blks.7.addnorm2.ln.weight\", \"encoder.blks.7.addnorm2.ln.bias\", \"encoder.blks.8.attention.W_q.weight\", \"encoder.blks.8.attention.W_q.bias\", \"encoder.blks.8.attention.W_k.weight\", \"encoder.blks.8.attention.W_k.bias\", \"encoder.blks.8.attention.W_v.weight\", \"encoder.blks.8.attention.W_v.bias\", \"encoder.blks.8.attention.W_o.weight\", \"encoder.blks.8.attention.W_o.bias\", \"encoder.blks.8.addnorm1.ln.weight\", \"encoder.blks.8.addnorm1.ln.bias\", \"encoder.blks.8.ffn.dense1.weight\", \"encoder.blks.8.ffn.dense1.bias\", \"encoder.blks.8.ffn.dense2.weight\", \"encoder.blks.8.ffn.dense2.bias\", \"encoder.blks.8.addnorm2.ln.weight\", \"encoder.blks.8.addnorm2.ln.bias\", \"encoder.blks.9.attention.W_q.weight\", \"encoder.blks.9.attention.W_q.bias\", \"encoder.blks.9.attention.W_k.weight\", \"encoder.blks.9.attention.W_k.bias\", \"encoder.blks.9.attention.W_v.weight\", \"encoder.blks.9.attention.W_v.bias\", \"encoder.blks.9.attention.W_o.weight\", \"encoder.blks.9.attention.W_o.bias\", \"encoder.blks.9.addnorm1.ln.weight\", \"encoder.blks.9.addnorm1.ln.bias\", \"encoder.blks.9.ffn.dense1.weight\", \"encoder.blks.9.ffn.dense1.bias\", \"encoder.blks.9.ffn.dense2.weight\", \"encoder.blks.9.ffn.dense2.bias\", \"encoder.blks.9.addnorm2.ln.weight\", \"encoder.blks.9.addnorm2.ln.bias\", \"encoder.blks.10.attention.W_q.weight\", \"encoder.blks.10.attention.W_q.bias\", \"encoder.blks.10.attention.W_k.weight\", \"encoder.blks.10.attention.W_k.bias\", \"encoder.blks.10.attention.W_v.weight\", \"encoder.blks.10.attention.W_v.bias\", \"encoder.blks.10.attention.W_o.weight\", \"encoder.blks.10.attention.W_o.bias\", \"encoder.blks.10.addnorm1.ln.weight\", \"encoder.blks.10.addnorm1.ln.bias\", \"encoder.blks.10.ffn.dense1.weight\", \"encoder.blks.10.ffn.dense1.bias\", \"encoder.blks.10.ffn.dense2.weight\", \"encoder.blks.10.ffn.dense2.bias\", \"encoder.blks.10.addnorm2.ln.weight\", \"encoder.blks.10.addnorm2.ln.bias\", \"encoder.blks.11.attention.W_q.weight\", \"encoder.blks.11.attention.W_q.bias\", \"encoder.blks.11.attention.W_k.weight\", \"encoder.blks.11.attention.W_k.bias\", \"encoder.blks.11.attention.W_v.weight\", \"encoder.blks.11.attention.W_v.bias\", \"encoder.blks.11.attention.W_o.weight\", \"encoder.blks.11.attention.W_o.bias\", \"encoder.blks.11.addnorm1.ln.weight\", \"encoder.blks.11.addnorm1.ln.bias\", \"encoder.blks.11.ffn.dense1.weight\", \"encoder.blks.11.ffn.dense1.bias\", \"encoder.blks.11.ffn.dense2.weight\", \"encoder.blks.11.ffn.dense2.bias\", \"encoder.blks.11.addnorm2.ln.weight\", \"encoder.blks.11.addnorm2.ln.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 加载\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BERTModel:\n\tUnexpected key(s) in state_dict: \"encoder.blks.2.attention.W_q.weight\", \"encoder.blks.2.attention.W_q.bias\", \"encoder.blks.2.attention.W_k.weight\", \"encoder.blks.2.attention.W_k.bias\", \"encoder.blks.2.attention.W_v.weight\", \"encoder.blks.2.attention.W_v.bias\", \"encoder.blks.2.attention.W_o.weight\", \"encoder.blks.2.attention.W_o.bias\", \"encoder.blks.2.addnorm1.ln.weight\", \"encoder.blks.2.addnorm1.ln.bias\", \"encoder.blks.2.ffn.dense1.weight\", \"encoder.blks.2.ffn.dense1.bias\", \"encoder.blks.2.ffn.dense2.weight\", \"encoder.blks.2.ffn.dense2.bias\", \"encoder.blks.2.addnorm2.ln.weight\", \"encoder.blks.2.addnorm2.ln.bias\", \"encoder.blks.3.attention.W_q.weight\", \"encoder.blks.3.attention.W_q.bias\", \"encoder.blks.3.attention.W_k.weight\", \"encoder.blks.3.attention.W_k.bias\", \"encoder.blks.3.attention.W_v.weight\", \"encoder.blks.3.attention.W_v.bias\", \"encoder.blks.3.attention.W_o.weight\", \"encoder.blks.3.attention.W_o.bias\", \"encoder.blks.3.addnorm1.ln.weight\", \"encoder.blks.3.addnorm1.ln.bias\", \"encoder.blks.3.ffn.dense1.weight\", \"encoder.blks.3.ffn.dense1.bias\", \"encoder.blks.3.ffn.dense2.weight\", \"encoder.blks.3.ffn.dense2.bias\", \"encoder.blks.3.addnorm2.ln.weight\", \"encoder.blks.3.addnorm2.ln.bias\", \"encoder.blks.4.attention.W_q.weight\", \"encoder.blks.4.attention.W_q.bias\", \"encoder.blks.4.attention.W_k.weight\", \"encoder.blks.4.attention.W_k.bias\", \"encoder.blks.4.attention.W_v.weight\", \"encoder.blks.4.attention.W_v.bias\", \"encoder.blks.4.attention.W_o.weight\", \"encoder.blks.4.attention.W_o.bias\", \"encoder.blks.4.addnorm1.ln.weight\", \"encoder.blks.4.addnorm1.ln.bias\", \"encoder.blks.4.ffn.dense1.weight\", \"encoder.blks.4.ffn.dense1.bias\", \"encoder.blks.4.ffn.dense2.weight\", \"encoder.blks.4.ffn.dense2.bias\", \"encoder.blks.4.addnorm2.ln.weight\", \"encoder.blks.4.addnorm2.ln.bias\", \"encoder.blks.5.attention.W_q.weight\", \"encoder.blks.5.attention.W_q.bias\", \"encoder.blks.5.attention.W_k.weight\", \"encoder.blks.5.attention.W_k.bias\", \"encoder.blks.5.attention.W_v.weight\", \"encoder.blks.5.attention.W_v.bias\", \"encoder.blks.5.attention.W_o.weight\", \"encoder.blks.5.attention.W_o.bias\", \"encoder.blks.5.addnorm1.ln.weight\", \"encoder.blks.5.addnorm1.ln.bias\", \"encoder.blks.5.ffn.dense1.weight\", \"encoder.blks.5.ffn.dense1.bias\", \"encoder.blks.5.ffn.dense2.weight\", \"encoder.blks.5.ffn.dense2.bias\", \"encoder.blks.5.addnorm2.ln.weight\", \"encoder.blks.5.addnorm2.ln.bias\", \"encoder.blks.6.attention.W_q.weight\", \"encoder.blks.6.attention.W_q.bias\", \"encoder.blks.6.attention.W_k.weight\", \"encoder.blks.6.attention.W_k.bias\", \"encoder.blks.6.attention.W_v.weight\", \"encoder.blks.6.attention.W_v.bias\", \"encoder.blks.6.attention.W_o.weight\", \"encoder.blks.6.attention.W_o.bias\", \"encoder.blks.6.addnorm1.ln.weight\", \"encoder.blks.6.addnorm1.ln.bias\", \"encoder.blks.6.ffn.dense1.weight\", \"encoder.blks.6.ffn.dense1.bias\", \"encoder.blks.6.ffn.dense2.weight\", \"encoder.blks.6.ffn.dense2.bias\", \"encoder.blks.6.addnorm2.ln.weight\", \"encoder.blks.6.addnorm2.ln.bias\", \"encoder.blks.7.attention.W_q.weight\", \"encoder.blks.7.attention.W_q.bias\", \"encoder.blks.7.attention.W_k.weight\", \"encoder.blks.7.attention.W_k.bias\", \"encoder.blks.7.attention.W_v.weight\", \"encoder.blks.7.attention.W_v.bias\", \"encoder.blks.7.attention.W_o.weight\", \"encoder.blks.7.attention.W_o.bias\", \"encoder.blks.7.addnorm1.ln.weight\", \"encoder.blks.7.addnorm1.ln.bias\", \"encoder.blks.7.ffn.dense1.weight\", \"encoder.blks.7.ffn.dense1.bias\", \"encoder.blks.7.ffn.dense2.weight\", \"encoder.blks.7.ffn.dense2.bias\", \"encoder.blks.7.addnorm2.ln.weight\", \"encoder.blks.7.addnorm2.ln.bias\", \"encoder.blks.8.attention.W_q.weight\", \"encoder.blks.8.attention.W_q.bias\", \"encoder.blks.8.attention.W_k.weight\", \"encoder.blks.8.attention.W_k.bias\", \"encoder.blks.8.attention.W_v.weight\", \"encoder.blks.8.attention.W_v.bias\", \"encoder.blks.8.attention.W_o.weight\", \"encoder.blks.8.attention.W_o.bias\", \"encoder.blks.8.addnorm1.ln.weight\", \"encoder.blks.8.addnorm1.ln.bias\", \"encoder.blks.8.ffn.dense1.weight\", \"encoder.blks.8.ffn.dense1.bias\", \"encoder.blks.8.ffn.dense2.weight\", \"encoder.blks.8.ffn.dense2.bias\", \"encoder.blks.8.addnorm2.ln.weight\", \"encoder.blks.8.addnorm2.ln.bias\", \"encoder.blks.9.attention.W_q.weight\", \"encoder.blks.9.attention.W_q.bias\", \"encoder.blks.9.attention.W_k.weight\", \"encoder.blks.9.attention.W_k.bias\", \"encoder.blks.9.attention.W_v.weight\", \"encoder.blks.9.attention.W_v.bias\", \"encoder.blks.9.attention.W_o.weight\", \"encoder.blks.9.attention.W_o.bias\", \"encoder.blks.9.addnorm1.ln.weight\", \"encoder.blks.9.addnorm1.ln.bias\", \"encoder.blks.9.ffn.dense1.weight\", \"encoder.blks.9.ffn.dense1.bias\", \"encoder.blks.9.ffn.dense2.weight\", \"encoder.blks.9.ffn.dense2.bias\", \"encoder.blks.9.addnorm2.ln.weight\", \"encoder.blks.9.addnorm2.ln.bias\", \"encoder.blks.10.attention.W_q.weight\", \"encoder.blks.10.attention.W_q.bias\", \"encoder.blks.10.attention.W_k.weight\", \"encoder.blks.10.attention.W_k.bias\", \"encoder.blks.10.attention.W_v.weight\", \"encoder.blks.10.attention.W_v.bias\", \"encoder.blks.10.attention.W_o.weight\", \"encoder.blks.10.attention.W_o.bias\", \"encoder.blks.10.addnorm1.ln.weight\", \"encoder.blks.10.addnorm1.ln.bias\", \"encoder.blks.10.ffn.dense1.weight\", \"encoder.blks.10.ffn.dense1.bias\", \"encoder.blks.10.ffn.dense2.weight\", \"encoder.blks.10.ffn.dense2.bias\", \"encoder.blks.10.addnorm2.ln.weight\", \"encoder.blks.10.addnorm2.ln.bias\", \"encoder.blks.11.attention.W_q.weight\", \"encoder.blks.11.attention.W_q.bias\", \"encoder.blks.11.attention.W_k.weight\", \"encoder.blks.11.attention.W_k.bias\", \"encoder.blks.11.attention.W_v.weight\", \"encoder.blks.11.attention.W_v.bias\", \"encoder.blks.11.attention.W_o.weight\", \"encoder.blks.11.attention.W_o.bias\", \"encoder.blks.11.addnorm1.ln.weight\", \"encoder.blks.11.addnorm1.ln.bias\", \"encoder.blks.11.ffn.dense1.weight\", \"encoder.blks.11.ffn.dense1.bias\", \"encoder.blks.11.ffn.dense2.weight\", \"encoder.blks.11.ffn.dense2.bias\", \"encoder.blks.11.addnorm2.ln.weight\", \"encoder.blks.11.addnorm2.ln.bias\". "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 加载\n",
    "bert.load_state_dict(torch.load(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dc9c8bb-6bd5-490a-b2d1-5b75b29f3528",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载数据\n",
    "train_iter, test_iter = load_data(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2fe10e6-55d1-48de-aebc-5c63ab94a213",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1073, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter), len(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b485c-e04e-4879-b4c2-07ef3be2f810",
   "metadata": {},
   "source": [
    "打印 train_iter 数据看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a1b9c7b-f374-47c9-a887-5b0090e46f52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 128])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "count:  1\n",
      "\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "count:  2\n",
      "\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "end:  3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 打印一下数据\n",
    "count = 0\n",
    "for X, Y in train_iter:\n",
    "    \n",
    "    print(X[0].shape)  # [512, 128]\n",
    "    print(X[1].shape)  # [512, 128]\n",
    "    print(Y.shape)     # [512]\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count >= 3:\n",
    "        print('end: ', count)\n",
    "        break\n",
    "    print('count: ', count)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be650195-d891-44e0-8645-8ac138ad4e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型及参数\n",
    "net = BERTClassifier(bert)\n",
    "lr = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2377c-6ae6-4fc2-b263-6feed5a05066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "batch_size = 512\n",
    "max_len = 128\n",
    "num_workers = 4\n",
    "\n",
    "\n",
    "# 开始微调\n",
    "train_ch13(net,\n",
    "           train_iter, \n",
    "           test_iter,\n",
    "           loss, \n",
    "           trainer,\n",
    "           num_epochs,\n",
    "           devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f59399-3d5c-479b-83f1-dc6de30aa30b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
