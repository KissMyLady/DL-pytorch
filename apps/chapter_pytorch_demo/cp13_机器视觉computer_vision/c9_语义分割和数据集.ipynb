{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac52485-f964-4ba6-b93f-82d463554bf8",
   "metadata": {},
   "source": [
    "# 9.9 语义分割和数据集\n",
    "\n",
    "在前几节讨论的目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。本节将探讨语义分割（semantic segmentation）问题，它关注如何将图像分割成属于不同语义类别的区域。值得一提的是，这些语义区域的标注和预测都是像素级的。图9.10展示了语义分割中图像有关狗、猫和背景的标签。可以看到，与目标检测相比，语义分割标注的像素级的边框显然更加精细。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5390e18-c3c3-4644-96f7-5f8ac9fff77a",
   "metadata": {},
   "source": [
    "## 9.9.1 图像分割和实例分割\n",
    "\n",
    "计算机视觉领域还有2个与语义分割相似的重要问题，即图像分割（image segmentation）和实例分割（instance segmentation）。我们在这里将它们与语义分割简单区分一下。\n",
    "\n",
    "* 图像分割将图像分割成若干组成区域。这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以图9.10的图像为输入，图像分割可能将狗分割成两个区域：一个覆盖以黑色为主的嘴巴和眼睛，而另一个覆盖以黄色为主的其余部分身体。\n",
    "* 实例分割又叫同时检测并分割（simultaneous detection and segmentation）。它研究如何识别图像中各个目标实例的像素级区域。与语义分割有所不同，实例分割不仅需要区分语义，还要区分不同的目标实例。如果图像中有两只狗，实例分割需要区分像素属于这两只狗中的哪一只。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aec6f7-8eee-499d-be90-03ad7cbdccc4",
   "metadata": {},
   "source": [
    "## 9.9.2 Pascal VOC2012语义分割数据集\n",
    "\n",
    "语义分割的一个重要数据集叫作Pascal VOC2012 [1]。为了更好地了解这个数据集，我们先导入实验所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0021bbb-e0d6-432e-9352-1c899e04e0be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936d015-26a2-488c-a572-5bd5dbd517cf",
   "metadata": {},
   "source": [
    "我们先下载这个数据集的压缩包（[下载地址](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar)）。压缩包大小是2 GB左右，下载需要一定时间。下载后解压得到`VOCdevkit/VOC2012`文件夹，然后将其放置在`data`文件夹下。\n",
    "\n",
    "```sh\n",
    "$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036aef51-8380-4cc3-b1b6-49abe69bfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../data/VOCdevkit/VOC2012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8abe40-4096-4b89-934b-b8688265a199",
   "metadata": {},
   "source": [
    "进入`../../data/VOCdevkit/VOC2012`路径后，我们可以获取数据集的不同组成部分。其中`ImageSets/Segmentation`路径包含了指定训练和测试样本的文本文件，而`JPEGImages`和`SegmentationClass`路径下分别包含了样本的输入图像和标签。这里的标签也是图像格式，其尺寸和它所标注的输入图像的尺寸相同。标签中颜色相同的像素属于同一个语义类别。下面定义`read_voc_images`函数将输入图像和标签读进内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1ef56-1663-49d1-ad6e-3c0768e0eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch中方便以后使用\n",
    "def read_voc_images(root=\"../../data/VOCdevkit/VOC2012\", \n",
    "                    is_train=True, max_num=None):\n",
    "    \n",
    "    txt_fname = '%s/ImageSets/Segmentation/%s' \n",
    "    % ( root,  'train.txt' if is_train else 'val.txt')\n",
    "    \n",
    "    with open(txt_fname, 'r') as f:\n",
    "        images = f.read().split()\n",
    "        \n",
    "    if max_num is not None:\n",
    "        images = images[:min(max_num, len(images))]\n",
    "        \n",
    "    features, labels = [None] * len(images), [None] * len(images)\n",
    "    \n",
    "    for i, fname in tqdm(enumerate(images)):\n",
    "        features[i] = Image.open('%s/JPEGImages/%s.jpg' % (root, fname)).convert(\"RGB\")\n",
    "        labels[i] = Image.open('%s/SegmentationClass/%s.png' % (root, fname)).convert(\"RGB\")\n",
    "    \n",
    "    return features, labels # PIL image\n",
    "\n",
    "\n",
    "voc_dir = \"../../data/VOCdevkit/VOC2012\"\n",
    "train_features, train_labels = read_voc_images(voc_dir, max_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929880fd-6914-40bf-9f63-047e0d70b47c",
   "metadata": {},
   "source": [
    "我们画出前5张输入图像和它们的标签。在标签图像中，白色和黑色分别代表边框和背景，而其他不同的颜色则对应不同的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df79f91-8345-4640-8439-3e5503e7f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "imgs = train_features[0:n] + train_labels[0:n]\n",
    "d2l.show_images(imgs, 2, n);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5b439-cb6b-476d-9257-956d353cb6fb",
   "metadata": {},
   "source": [
    "接下来，我们列出标签中每个RGB颜色的值及其标注的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc02095-8e56-46f7-a340-4fd4129c7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch中方便以后使用\n",
    "VOC_COLORMAP = [[0, 0, 0], \n",
    "                [128, 0, 0], \n",
    "                [0, 128, 0], \n",
    "                [128, 128, 0],\n",
    "                \n",
    "                [0, 0, 128], \n",
    "                [128, 0, 128], \n",
    "                [0, 128, 128], \n",
    "                [128, 128, 128],\n",
    "                \n",
    "                [64, 0, 0], \n",
    "                [192, 0, 0], \n",
    "                [64, 128, 0], \n",
    "                [192, 128, 0],\n",
    "                \n",
    "                [64, 0, 128], \n",
    "                [192, 0, 128], \n",
    "                [64, 128, 128], \n",
    "                [192, 128, 128],\n",
    "                \n",
    "                [0, 64, 0], \n",
    "                [128, 64, 0], \n",
    "                [0, 192, 0], \n",
    "                [128, 192, 0],\n",
    "                [0, 64, 128]\n",
    "               ]\n",
    "\n",
    "\n",
    "# 本函数已保存在d2lzh_pytorch中方便以后使用\n",
    "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cffc6-7f84-4a53-9fd0-e46fb5eba50f",
   "metadata": {},
   "source": [
    "有了上面定义的两个常量以后，我们可以很容易地查找标签中每个像素的类别索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ec7a5-d630-4052-9cc7-c61066b963e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap2label = torch.zeros(256 ** 3, dtype=torch.uint8)\n",
    "\n",
    "\n",
    "for i, colormap in enumerate(VOC_COLORMAP):\n",
    "    colormap2label[(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
    "\n",
    "    \n",
    "# 本函数已保存在d2lzh_pytorch中方便以后使用\n",
    "def voc_label_indices(colormap, colormap2label):\n",
    "    \"\"\"\n",
    "    convert colormap (PIL image) to colormap2label (uint8 tensor).\n",
    "    \"\"\"\n",
    "    colormap = np.array(colormap.convert(\"RGB\")).astype('int32')\n",
    "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256+ colormap[:, :, 2])\n",
    "    \n",
    "    return colormap2label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7cec5-b91c-45bf-9ec0-636cf2c44904",
   "metadata": {},
   "source": [
    "例如，第一张样本图像中飞机头部区域的类别索引为1，而背景全是0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f1079-d296-47cf-9c48-e7a12a66b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = voc_label_indices(train_labels[0], colormap2label)\n",
    "\n",
    "\n",
    "y[105:115, 130:140], VOC_CLASSES[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9f9c8-53ca-4a5f-a898-7011932636c1",
   "metadata": {},
   "source": [
    "### 9.9.2.1 预处理数据\n",
    "\n",
    "在之前的章节中，我们通过缩放图像使其符合模型的输入形状。然而在语义分割里，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射难以做到精确，尤其在不同语义的分割区域。为了避免这个问题，我们将图像裁剪成固定尺寸而不是缩放。具体来说，我们使用图像增广里的随机裁剪，并对输入图像和标签裁剪相同区域。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3061b642-37e1-4953-9b14-7658b3ae4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch中方便以后使用\n",
    "def voc_rand_crop(feature, label, height, width):\n",
    "    \"\"\"\n",
    "    Random crop feature (PIL image) and label (PIL image).\n",
    "    \"\"\"\n",
    "    \n",
    "    i, j, h, w = torchvision.transforms.RandomCrop.get_params(\n",
    "        feature,\n",
    "        output_size=(height, width)\n",
    "    )\n",
    "    \n",
    "    feature = torchvision.transforms.functional.crop(feature, i, j, h, w)\n",
    "    label = torchvision.transforms.functional.crop(label, i, j, h, w)    \n",
    "\n",
    "    return feature, label\n",
    "\n",
    "\n",
    "imgs = []\n",
    "for _ in range(n):\n",
    "    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)\n",
    "\n",
    "    \n",
    "d2l.show_images(imgs[::2] + imgs[1::2], 2, n);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20405210-4e38-4290-8793-604f3ed1aaf6",
   "metadata": {},
   "source": [
    "### 9.9.2.2 自定义语义分割数据集类\n",
    "\n",
    "我们通过继承PyTorch提供的`Dataset`类自定义了一个语义分割数据集类`VOCSegDataset`。通过实现`__getitem__`函数，我们可以任意访问数据集中索引为`idx`的输入图像及其每个像素的类别索引。由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本需要通过自定义的`filter`函数所移除。此外，我们还对输入图像的RGB三个通道的值分别做标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0284ead-7d80-4fcb-9667-cef75821ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch中方便以后使用\n",
    "class VOCSegDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, is_train, crop_size, voc_dir, colormap2label, max_num=None):\n",
    "        \"\"\"\n",
    "        crop_size: (h, w)\n",
    "        \"\"\"\n",
    "        self.rgb_mean = np.array([0.485, 0.456, 0.406])\n",
    "        self.rgb_std = np.array([0.229, 0.224, 0.225])\n",
    "        self.tsf = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean=self.rgb_mean, \n",
    "                                             std=self.rgb_std)\n",
    "        ])\n",
    "        \n",
    "        self.crop_size = crop_size # (h, w)\n",
    "        features, labels = read_voc_images(root=voc_dir, \n",
    "                                           is_train=is_train, \n",
    "                                           max_num=max_num)\n",
    "        self.features = self.filter(features) # PIL image\n",
    "        self.labels = self.filter(labels)     # PIL image\n",
    "        self.colormap2label = colormap2label\n",
    "        print('read ' + str(len(self.features)) + ' valid examples')\n",
    "\n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if (\n",
    "            img.size[1] >= self.crop_size[0] and\n",
    "            img.size[0] >= self.crop_size[1])]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, label = voc_rand_crop(self.features[idx],\n",
    "                                       self.labels[idx],\n",
    "                                       *self.crop_size)\n",
    "        \n",
    "        # float32 tensor\n",
    "        return (self.tsf(feature), \n",
    "                voc_label_indices(label, self.colormap2label)) # uint8 tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0c998-d3f9-46e3-bcdd-0672bd5a2567",
   "metadata": {},
   "source": [
    "### 9.9.2.3 读取数据集\n",
    "\n",
    "我们通过自定义的`VOCSegDataset`类来分别创建训练集和测试集的实例。假设我们指定随机裁剪的输出图像的形状为$320\\times 480$。下面我们可以查看训练集和测试集所保留的样本个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9565a1-0c3c-4b5c-932e-1803bbcc9df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop_size = (320, 480)\n",
    "max_num = 100\n",
    "\n",
    "\n",
    "\n",
    "voc_train = VOCSegDataset(True, \n",
    "                          crop_size, \n",
    "                          voc_dir, \n",
    "                          colormap2label, \n",
    "                          max_num)\n",
    "\n",
    "\n",
    "voc_test = VOCSegDataset(False,\n",
    "                         crop_size, \n",
    "                         voc_dir, \n",
    "                         colormap2label,\n",
    "                         max_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7850450-1334-4e12-810b-19a4fa652c19",
   "metadata": {},
   "source": [
    "设批量大小为64，分别定义训练集和测试集的迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1274c7-9868-4b73-ae3b-265e24a7a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(voc_train, \n",
    "                                         batch_size, \n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True, \n",
    "                                         num_workers=num_workers\n",
    "                                        )\n",
    "\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(voc_test, \n",
    "                                        batch_size, \n",
    "                                        drop_last=True,\n",
    "                                        num_workers=num_workers\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98f2ba-f3ea-48df-989b-9530c68fa538",
   "metadata": {},
   "source": [
    "打印第一个小批量的类型和形状。不同于图像分类和目标识别，这里的标签是一个三维数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae558f-90c8-474a-ba3e-0da6d220215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in train_iter:\n",
    "    print(X.dtype, X.shape)\n",
    "    print(y.dtype, Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d04e80c-3e07-4ac2-8202-d786938000f8",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 语义分割关注如何将图像分割成属于不同语义类别的区域。\n",
    "* 语义分割的一个重要数据集叫作Pascal VOC2012。\n",
    "* 由于语义分割的输入图像和标签在像素上一一对应，所以将图像随机裁剪成固定尺寸而不是缩放。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca003c-c744-4212-8597-2aacb4fbc143",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "* 回忆9.1节（图像增广）中的内容。哪些在图像分类中使用的图像增广方法难以用于语义分割？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b3c66-7b9f-4c4b-82ba-5efe47317f69",
   "metadata": {},
   "source": [
    "##  参考文献\n",
    "\n",
    "[1] Pascal VOC2012数据集。http://host.robots.ox.ac.uk/pascal/VOC/voc2012/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058eead0-e953-4e64-af0b-70ebfb9244b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
