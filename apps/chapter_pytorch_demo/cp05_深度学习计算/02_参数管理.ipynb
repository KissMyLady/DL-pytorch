{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1ad880-c6eb-41a0-a076-7f07e2ee9088",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "\n",
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。\n",
    "此时，我们的目标是找到使损失函数最小化的模型参数值。\n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们，\n",
    "将模型保存下来，以便它可以在其他软件中执行，\n",
    "或者为了获得科学的理解而进行检查。\n",
    "\n",
    "之前的介绍中，我们只依靠深度学习框架来完成训练的工作，\n",
    "而忽略了操作参数的具体细节。\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化；\n",
    "* 参数初始化；\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先看一下具有单隐藏层的多层感知机。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2aff79-ec83-45e1-a6ec-0e8882889bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa7b970-bd61-41ca-b552-8238cfa987d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(8, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87306ac0-672b-400b-bf05-cd3f7256913a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3931, 0.0608, 0.9878, 0.3589],\n",
       "        [0.1226, 0.7259, 0.2943, 0.2432]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(2, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f92cf3f-4ee8-4a5c-9e11-935752d52017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5027],\n",
       "        [0.4954]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27264ded-8812-4b47-bd72-fa728e99629f",
   "metadata": {},
   "source": [
    "## [**参数访问**]\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de55755e-adae-4dd6-8983-4076b94be6c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3758781-2306-45f6-8024-2399d4f468b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.4352,  0.4118,  0.3511,  0.1613],\n",
       "                      [-0.0701,  0.4662, -0.2258, -0.1911],\n",
       "                      [ 0.1020, -0.3894,  0.2349, -0.2079],\n",
       "                      [ 0.3260, -0.2003, -0.4478, -0.2870],\n",
       "                      [-0.3141, -0.3362, -0.2770,  0.0787],\n",
       "                      [-0.1592,  0.3814, -0.2883, -0.1427],\n",
       "                      [-0.4864, -0.1844,  0.2203, -0.3192],\n",
       "                      [-0.0295, -0.3716,  0.1208, -0.2345]])),\n",
       "             ('0.bias',\n",
       "              tensor([ 0.3338, -0.1598, -0.2369, -0.3018,  0.4479, -0.4996, -0.4940, -0.0399])),\n",
       "             ('2.weight',\n",
       "              tensor([[ 0.2792,  0.2437, -0.2758, -0.2211,  0.1879,  0.0789, -0.2829,  0.1652]])),\n",
       "             ('2.bias', tensor([0.2308]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45d8d4e5-bc77-43c9-8b0e-edc912ea05e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2792,  0.2437, -0.2758, -0.2211,  0.1879,  0.0789, -0.2829,  0.1652]])), ('bias', tensor([0.2308]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d8285-0f7e-4981-89b9-1198fd86d4d4",
   "metadata": {},
   "source": [
    "输出的结果告诉我们一些重要的事情：\n",
    "首先，这个全连接层包含两个参数，分别是该层的权重和偏置。\n",
    "两者都存储为单精度浮点数（float32）。\n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78489e9-f6e6-4354-92e8-b318b0269acc",
   "metadata": {},
   "source": [
    "### [**目标参数**]\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd68d473-4862-43ea-a8d9-7bc15bb9057f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28ea8806-0c7f-4071-9268-34cb8492b2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.2308], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net[2].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a7fc183-2e6a-4070-b6b4-dd1b5b02549a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2308])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2596c0-44f0-4d41-afb8-03033076219b",
   "metadata": {},
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。\n",
    "这就是我们需要显式参数值的原因。\n",
    "除了值之外，我们还可以访问每个参数的梯度。\n",
    "在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fadd81f-47c6-46b1-a398-5dab610ab420",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25356e1d-24e6-46e5-b68c-588c8a176072",
   "metadata": {},
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3398842f-fb0e-407a-91df-53a711b51488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1d49e0d-e73b-4b83-a8b0-67e0a473cb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9249d-d17f-4a82-a2b5-461ab85ec4e5",
   "metadata": {},
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34ccf450-6563-4786-9419-458fc9ba7bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2308])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "386d3500-5865-456d-8a88-17d975edebda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3338, -0.1598, -0.2369, -0.3018,  0.4479, -0.4996, -0.4940, -0.0399])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['0.bias'] # .data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4537b-d1dd-476a-8a1a-79025d306243",
   "metadata": {},
   "source": [
    "### [**从嵌套块收集参数**]\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1348d435-9540-4698-8c0b-a756cee2c847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4, 8), \n",
    "        nn.ReLU(),  \n",
    "        nn.Linear(8, 4), \n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "\n",
    "rgnet = nn.Sequential(\n",
    "    block2(), \n",
    "    nn.Linear(4, 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efdbfc4e-0cb1-4d1c-bcd8-ae2398358bad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rgnet.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5f188de-cc30-4ae6-abce-94d85167175a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (block 0): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 1): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 2): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 3): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10638a46-9b9a-49ac-a711-1a2e6281a76c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1038],\n",
       "        [0.1038]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6f5d3-f377-4c26-852c-a9fe81840082",
   "metadata": {},
   "source": [
    "## 参数初始化\n",
    "\n",
    "知道了如何访问参数后，现在我们看看如何正确地初始化参数。\n",
    "我们在 :numref:`sec_numerical_stability`中讨论了良好初始化的必要性。\n",
    "深度学习框架提供默认随机初始化，\n",
    "也允许我们创建自定义初始化方法，\n",
    "满足我们通过其他规则实现初始化权重。\n",
    "\n",
    "\n",
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，\n",
    "这个范围是根据输入和输出维度计算出的。\n",
    "PyTorch的`nn.init`模块提供了多种预置初始化方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded23c86-18f9-4cb3-b9d9-efdb9d9cb67f",
   "metadata": {},
   "source": [
    "### [**内置初始化**]\n",
    "\n",
    "让我们首先调用内置的初始化器。\n",
    "下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，\n",
    "且将偏置参数设置为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e1ee026-f35e-4d05-b887-5e6dd660d6fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "        \n",
    "net.apply(init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7fc106c-7870-4bca-9aa4-79dc78243770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0203, -0.0020,  0.0043,  0.0178]), tensor(0.))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a0036-e7a0-40de-93c6-00cdfce66295",
   "metadata": {},
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "612b6e40-146e-43db-82ab-30afc7602a50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "net.apply(init_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "403d7ed5-3d50-4f3f-b02e-76860cdaaedb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f03b74-70f2-436b-9135-d3c68f22a586",
   "metadata": {},
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。\n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，\n",
    "然后将第三个神经网络层初始化为常量值42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a14b11da-e6e3-4194-89a7-f36ec0e99ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=1, bias=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14b87a8b-6131-4f6a-92f6-b28813ba5cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4972,  0.1865,  0.0207,  0.3043])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71409994-ad90-492b-9bde-ac27992f40a4",
   "metadata": {},
   "source": [
    "### [**自定义初始化**]\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a112f32f-0935-4e68-bd8a-f83feb088be0",
   "metadata": {},
   "source": [
    "同样，我们实现了一个`my_init`函数来应用到`net`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb1243ff-7a27-4132-a951-3f3eecb46a89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "\n",
    "net.apply(my_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f257ff6e-781d-4636-aebf-ffc42fc026b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.7451, -0.0000, -0.0000, -9.2546],\n",
       "        [ 0.0000,  5.6224,  9.5233,  0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4dd6a-6100-4a6b-afbd-b4ac7a33ab09",
   "metadata": {},
   "source": [
    "注意，我们始终可以直接设置参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce767468-0f7d-4e92-8543-f4dd0eba6164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000,  1.0000, -8.2546])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee342edd-55a4-4dc6-bea0-cd56ea749b33",
   "metadata": {},
   "source": [
    "## [**参数绑定**]\n",
    "\n",
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3845687-51f1-4de6-b0fe-9c19e137dffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1129],\n",
       "        [0.1109]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), nn.ReLU(),\n",
    "    shared, nn.ReLU(),\n",
    "    shared, nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "858b8525-5067-4920-91cf-9d31c6463848",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "\n",
    "net[2].weight.data[0, 0] = 100\n",
    "\n",
    "\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f4e33e-b3fa-40d7-a45d-4e1229a22106",
   "metadata": {},
   "source": [
    "这个例子表明第三个和第五个神经网络层的参数是绑定的。\n",
    "它们不仅值相等，而且由相同的张量表示。\n",
    "因此，如果我们改变其中一个参数，另一个参数也会改变。\n",
    "这里有一个问题：当参数绑定时，梯度会发生什么情况？\n",
    "答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层\n",
    "（即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ddffc-133d-425b-9de3-dc1880e18eb0",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4cb6c-0133-442d-8649-0f3b4a7a8379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
