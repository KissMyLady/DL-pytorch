{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c820bf90-33da-44fd-b635-c8b195b14b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3861568-bfd1-442c-a221-df4613caa2ee",
   "metadata": {},
   "source": [
    "## 3.6.1 获取和读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "112fa044-6172-4653-a956-b257e92eb63c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None, root='~/Datasets/FashionMNIST'):\n",
    "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "        pass\n",
    "\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=root, \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=root, \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    if sys.platform.startswith('win'):\n",
    "        num_workers = 0  # 0表示不用额外的进程来加速读取数据\n",
    "    else:\n",
    "        num_workers = 4\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(\n",
    "        mnist_train, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    test_iter = torch.utils.data.DataLoader(\n",
    "        mnist_test, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "263c265d-3fdc-4caf-9551-62ea89ebcecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1abbf48-3136-4d87-936d-4deea1539dcd",
   "metadata": {},
   "source": [
    "## 3.6.2 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12c74a19-2a02-403b-a1c0-b630664277f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_inputs = 784  # 28 * 28 像素点\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), \n",
    "                 dtype=torch.float\n",
    "                )\n",
    "\n",
    "b = torch.zeros(num_outputs, \n",
    "                dtype=torch.float)\n",
    "\n",
    "\n",
    "W.requires_grad_(requires_grad=True)\n",
    "b.requires_grad_(requires_grad=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd136d86-90f6-4697-87c3-0e2a60732e57",
   "metadata": {},
   "source": [
    "## 3.6.3 实现softmax运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6992fe-aa6d-4c3c-9c13-f875483575b8",
   "metadata": {},
   "source": [
    "在介绍如何定义softmax回归之前，我们先描述一下对如何对多维`Tensor`按维度操作。在下面的例子中，给定一个`Tensor`矩阵`X`。我们可以只对其中同一列（`dim=0`）或同一行（`dim=1`）的元素求和，并在结果中保留行和列这两个维度（`keepdim=True`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ad3b14-e8d6-4584-8ca0-f8ae71ffac70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 9]])\n",
      "tensor([[ 6],\n",
      "        [15]])\n",
      "tensor([[  2.7183,   7.3891,  20.0855],\n",
      "        [ 54.5981, 148.4132, 403.4288]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]]\n",
    "                )\n",
    "\n",
    "print(X.sum(dim=0, keepdim=True))\n",
    "print(X.sum(dim=1, keepdim=True))\n",
    "\n",
    "# 返回 e^x 计算结果\n",
    "print(X.exp())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6704b880-6890-40d6-b2d5-30894716b7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(dim=1, keepdim=True)\n",
    "    return X_exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959347ad-36b6-421b-a43b-bbc23864f8ba",
   "metadata": {},
   "source": [
    "对于随机输入，我们将每个元素变成了非负数，且每一行和为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94c5a55-cff3-4df8-9279-67c091e226b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_prob:  tensor([[0.2421, 0.1958, 0.1768, 0.1116, 0.2737],\n",
      "        [0.1519, 0.2072, 0.2872, 0.2161, 0.1376]])\n",
      "X_prob.sum(dim=1):  tensor([1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((2, 5))\n",
    "X_prob = softmax(X)\n",
    "\n",
    "\n",
    "print(\"X_prob: \", X_prob)\n",
    "\n",
    "print(\"X_prob.sum(dim=1): \", X_prob.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea510a59-4882-499e-b912-794c8aed2b70",
   "metadata": {},
   "source": [
    "## 3.6.4 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccce9ee-c8cb-4b0a-b183-253ab23c8a27",
   "metadata": {},
   "source": [
    "有了softmax运算，我们可以定义上节描述的softmax回归模型了。这里通过`view`函数将每张原始图像改成长度为`num_inputs`的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42589b9a-095e-4b5e-b6ea-1449649bd48d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    # y = W ◆ X + B\n",
    "    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ae538-8556-4d7b-8869-a637f11fcf55",
   "metadata": {},
   "source": [
    "## 3.6.5 定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff164e0-c8e7-4f1e-b03d-26573ca543e2",
   "metadata": {},
   "source": [
    "上一节中，我们介绍了softmax回归使用的交叉熵损失函数。为了得到标签的预测概率，我们可以使用`gather`函数。在下面的例子中，变量`y_hat`是2个样本在3个类别的预测概率，变量`y`是这2个样本的标签类别。通过使用`gather`函数，我们得到了2个样本的标签的预测概率。与3.4节（softmax回归）数学表述中标签类别离散值从1开始逐一递增不同，在代码中，标签类别的离散值是从0开始逐一递增的。\n",
    "\n",
    "> torch.FloatTensor是32位浮点类型数据，torch.LongTensor是64位整型\n",
    "> \n",
    "> torch.tensor是一个类，用于生成一个单精度浮点类型的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c66ae0bb-2c43-451f-a343-22e7ba343d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  tensor([0, 2])\n",
      "损失函数计算为:  tensor([[-2.3026],\n",
      "        [-0.6931]])\n"
     ]
    }
   ],
   "source": [
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "\n",
    "y = torch.LongTensor([0, 2])\n",
    "print(\"y: \", y)\n",
    "\n",
    "res = y_hat.gather(dim=1, index=y.view(-1, 1))\n",
    "\n",
    "res_cress_entropy = torch.log(res)\n",
    "print(\"损失函数计算为: \", res_cress_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "248fdfc3-51c8-4498-bbb8-a0d912fe3602",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [2]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "123d2a08-272a-4fde-84fc-a28f1904f92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 损失函数 y_hat 与 y的损失值计算\n",
    "def cross_entropy(y_hat, y):\n",
    "    \n",
    "    # y标签值, 映射到 y_hat输出值上\n",
    "    res = y_hat.gather(1, y.view(-1, 1))\n",
    "    \n",
    "    # 计算损失\n",
    "    return - torch.log(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81ec40-bf39-4803-ad07-1a388f413c64",
   "metadata": {},
   "source": [
    "## 3.6.6 计算分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "faaad6a7-de01-4fcd-bd76-3f6343b662a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.argmax(dim=1)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f11a5f7-6597-4648-a4c7-4f537e60a536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    res = y_hat.argmax(dim=1) == y\n",
    "    return res.float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3f837963-a343-4c47-97d0-d917dec8e9be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(y_hat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f6201ea1-e1bb-4098-b10e-f1e2d589663a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 该函数将被逐步改进：它的完整实现将在“图像增广”一节中描述\n",
    "\n",
    "# 精度计算\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    \n",
    "    for X, y in data_iter:\n",
    "        \n",
    "        # 网络输出值\n",
    "        network_value_y = net(X)\n",
    "        \n",
    "        # 取 网络输出值 最大, 与 y 比较\n",
    "        res_bool = network_value_y.argmax(dim=1) == y\n",
    "        \n",
    "        # 统计正确数\n",
    "        acc_sum += res_bool.float().sum().item()\n",
    "        \n",
    "        # 长度合计 += 256\n",
    "        n += y.shape[0]\n",
    "        pass\n",
    "    \n",
    "    # 正确百分比\n",
    "    accuracy_percentage = acc_sum / n\n",
    "    return accuracy_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7ffaa0a-3461-4c1e-ad2e-a232470d582d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0603\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_accuracy(test_iter, net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33a01f-d363-4c4f-b2c7-9ed49d6cfce6",
   "metadata": {},
   "source": [
    "## 3.6.7 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9b4e905-c249-4acf-9992-8a2aaf7d35f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 参数更新\n",
    "def sgd(params, lr, batch_size):\n",
    "    # 为了和原书保持一致，这里除以了batch_size，但是应该是不用除的，\n",
    "    # 因为一般用PyTorch计算loss时就默认已经沿batch维求了平均了。\n",
    "    for param in params:\n",
    "        # 注意这里更改param时用的param.data\n",
    "        param.data -= lr * param.grad / batch_size \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3842ad91-8315-4be1-82e2-42c66ac35697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs, lr = 20, 0.1\n",
    "\n",
    "\n",
    "# 本函数已保存在d2lzh包中方便以后使用\n",
    "def train_ch3(net, train_iter, test_iter, loss, \n",
    "              num_epochs, batch_size, \n",
    "              params=None, \n",
    "              lr=None, \n",
    "              optimizer=None):\n",
    "    \n",
    "    # epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        \n",
    "        # 训练数据\n",
    "        for X, y in train_iter:\n",
    "            \n",
    "            # 计算网络输出值 y_hat\n",
    "            y_hat = net(X)\n",
    "            \n",
    "            # 网络损失计算\n",
    "            l = loss(y_hat, y).sum()\n",
    "            \n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            \n",
    "            # 计算反向传播\n",
    "            l.backward()\n",
    "            \n",
    "            if optimizer is None:\n",
    "                # 梯度下降\n",
    "                sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                # 优化器(这里用不到)\n",
    "                optimizer.step()  # “softmax回归的简洁实现”一节将用到\n",
    "\n",
    "            # 损失数合计\n",
    "            train_l_sum += l.item()\n",
    "\n",
    "            # 正确数计算\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            \n",
    "            # 总数n\n",
    "            n += y.shape[0]\n",
    "            pass\n",
    "    \n",
    "    \n",
    "        epoch_index = epoch + 1        # 训练批次\n",
    "        # test_acc = evaluate_accuracy(test_iter, net)  # 测试数据 正确百分比\n",
    "        test_acc = -1.0\n",
    "        loss_point = train_l_sum / n   # 损失 百分比\n",
    "        train_acc = train_acc_sum / n  # 训练 正确百分比\n",
    "        \n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f' \n",
    "              % (epoch_index, loss_point, train_acc, test_acc)\n",
    "             )\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2f97dc48-1e1f-4458-8e97-878441e44519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.4119, train acc 0.859, test acc -1.000\n",
      "epoch 2, loss 0.4112, train acc 0.859, test acc -1.000\n",
      "epoch 3, loss 0.4096, train acc 0.860, test acc -1.000\n",
      "epoch 4, loss 0.4086, train acc 0.860, test acc -1.000\n",
      "epoch 5, loss 0.4081, train acc 0.861, test acc -1.000\n",
      "epoch 6, loss 0.4070, train acc 0.860, test acc -1.000\n",
      "epoch 7, loss 0.4056, train acc 0.861, test acc -1.000\n",
      "epoch 8, loss 0.4051, train acc 0.861, test acc -1.000\n",
      "epoch 9, loss 0.4051, train acc 0.861, test acc -1.000\n",
      "epoch 10, loss 0.4035, train acc 0.862, test acc -1.000\n",
      "epoch 11, loss 0.4032, train acc 0.861, test acc -1.000\n",
      "epoch 12, loss 0.4022, train acc 0.862, test acc -1.000\n",
      "epoch 13, loss 0.4013, train acc 0.862, test acc -1.000\n",
      "epoch 14, loss 0.4006, train acc 0.863, test acc -1.000\n",
      "epoch 15, loss 0.3998, train acc 0.863, test acc -1.000\n",
      "epoch 16, loss 0.3992, train acc 0.863, test acc -1.000\n",
      "epoch 17, loss 0.3993, train acc 0.862, test acc -1.000\n",
      "epoch 18, loss 0.3975, train acc 0.864, test acc -1.000\n",
      "epoch 19, loss 0.3983, train acc 0.862, test acc -1.000\n",
      "epoch 20, loss 0.3974, train acc 0.863, test acc -1.000\n",
      "耗时:  15.849167585372925\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "train_ch3(net, \n",
    "          train_iter,\n",
    "          test_iter, \n",
    "          cross_entropy, \n",
    "          num_epochs, \n",
    "          batch_size, \n",
    "          [W, b], \n",
    "          lr\n",
    "         )\n",
    "\n",
    "print(\"耗时: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca960d2-5345-46f5-9619-81b8de3c6341",
   "metadata": {},
   "source": [
    "## 3.6.8 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fcb7b961-9d46-4620-bf9f-ba8e217c3e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f422f349820>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dadfa340-30f9-4e4a-811a-4972092a89a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6413a1b3-5460-4761-806a-771b9568deff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_num: 8409\n",
      "error_num: 1591\n",
      "正确率: 0.8409\n"
     ]
    }
   ],
   "source": [
    "correct_num = 0\n",
    "error_num = 0\n",
    "\n",
    "for i, value in enumerate(test_iter):\n",
    "    X, y = value[0], value[1]\n",
    "    \n",
    "    true_labels = get_fashion_mnist_labels(y.numpy())\n",
    "    pred_labels = get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())\n",
    "\n",
    "    # titles = [true + '\\n' + pred for true, pred in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # d2l.show_fashion_mnist(X[0:9], titles[0:9])\n",
    "    \n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        if pred == true:\n",
    "            correct_num += 1\n",
    "        else:\n",
    "            error_num += 1\n",
    "        pass\n",
    "\n",
    "    \n",
    "print(\"correct_num: %s\" % correct_num)\n",
    "print(\"error_num: %s\" % error_num)\n",
    "print(\"正确率: %s\" % (correct_num / (correct_num+ error_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c918620-b234-48f3-bde1-deac0b828705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
