{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e15d37-8037-42c0-bf9d-d1ea8bc07706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from d2lzh_pytorch.myUtils import Timer, Accumulator, try_gpu, try_all_gpus\n",
    "from d2lzh_pytorch.myPolt import Animator\n",
    "\n",
    "from d2lzh_pytorch.nlp.model.BERT_model import BERTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131d2246-03e0-4876-8803-ca8946fcdea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len_vocab = 20256\n",
    "\n",
    "net = BERTModel(len_vocab,\n",
    "                num_hiddens=128,\n",
    "                norm_shape=[128],\n",
    "                ffn_num_input=128,\n",
    "                ffn_num_hiddens=256,\n",
    "                num_heads=2,\n",
    "                num_layers=2,\n",
    "                dropout=0.2,\n",
    "                key_size=128,\n",
    "                query_size=128,\n",
    "                value_size=128,\n",
    "                hid_in_features=128,\n",
    "                mlm_in_features=128,\n",
    "                nsp_in_features=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5236d27-366c-4755-8a8c-0b50c6b649ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─BERTEncoder: 1-1                            --\n",
      "|    └─Embedding: 2-1                         2,592,768\n",
      "|    └─Embedding: 2-2                         256\n",
      "|    └─Sequential: 2-3                        --\n",
      "|    |    └─EncoderBlock: 3-1                 132,480\n",
      "|    |    └─EncoderBlock: 3-2                 132,480\n",
      "├─Sequential: 1-2                             --\n",
      "|    └─Linear: 2-4                            16,512\n",
      "|    └─Tanh: 2-5                              --\n",
      "├─MaskLM: 1-3                                 --\n",
      "|    └─Sequential: 2-6                        --\n",
      "|    |    └─Linear: 3-3                       16,512\n",
      "|    |    └─ReLU: 3-4                         --\n",
      "|    |    └─LayerNorm: 3-5                    256\n",
      "|    |    └─Linear: 3-6                       2,613,024\n",
      "├─NextSentencePred: 1-4                       --\n",
      "|    └─Linear: 2-7                            258\n",
      "======================================================================\n",
      "Total params: 5,504,546\n",
      "Trainable params: 5,504,546\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─BERTEncoder: 1-1                            --\n",
       "|    └─Embedding: 2-1                         2,592,768\n",
       "|    └─Embedding: 2-2                         256\n",
       "|    └─Sequential: 2-3                        --\n",
       "|    |    └─EncoderBlock: 3-1                 132,480\n",
       "|    |    └─EncoderBlock: 3-2                 132,480\n",
       "├─Sequential: 1-2                             --\n",
       "|    └─Linear: 2-4                            16,512\n",
       "|    └─Tanh: 2-5                              --\n",
       "├─MaskLM: 1-3                                 --\n",
       "|    └─Sequential: 2-6                        --\n",
       "|    |    └─Linear: 3-3                       16,512\n",
       "|    |    └─ReLU: 3-4                         --\n",
       "|    |    └─LayerNorm: 3-5                    256\n",
       "|    |    └─Linear: 3-6                       2,613,024\n",
       "├─NextSentencePred: 1-4                       --\n",
       "|    └─Linear: 2-7                            258\n",
       "======================================================================\n",
       "Total params: 5,504,546\n",
       "Trainable params: 5,504,546\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "net = net.to(\"cuda\")\n",
    "a8 = (3, 255, 255)\n",
    "summary(net, input_size=a8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66954a1-d4e0-49b1-9c16-bb3f41a0854d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTModel(\n",
       "  (encoder): BERTEncoder(\n",
       "    (token_embedding): Embedding(20256, 128)\n",
       "    (segment_embedding): Embedding(2, 128)\n",
       "    (blks): Sequential(\n",
       "      (0): EncoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (hidden): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (mlm): MaskLM(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=128, out_features=20256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (nsp): NextSentencePred(\n",
       "    (output): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0262b6-b8ff-401e-8c7b-8a3ce02043fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
