{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05aa3c9-b77a-475d-b891-55dd7cfdf3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db6b18a-32fd-4dc5-b490-496300c488fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, split):\n",
    "        # self.dataset = load_dataset('lansinuote/ChnSentiCorp', split=split)\n",
    "        self.dataset = load_from_disk('./data/ChnSentiCorp')['%s' % split]\n",
    "        print(self.dataset['text'][0])\n",
    "        print(self.dataset['label'][0])\n",
    "        # self.dataset = load_from_disk('./data/ChnSentiCorp')\n",
    "        # print(self.dataset)\n",
    "        # self.dataset = datasets.fetch_openml('./data/ChnSentiCorp', split=split)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset['text'][i]\n",
    "        label = self.dataset['label'][i]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f8de82-6e74-491c-b022-5d0e9e836d63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x7f7a6938fd90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = Dataset('train') #['train']\n",
    "\n",
    "# print(dataset['train'][0:10])\n",
    "# print(dataset['train'][1])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1313fb25-3d2f-4ffe-94a9-fa132a05c983",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       " 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9741a49-f59b-4867-bcf3-62025a70c23e",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a539e0d1-57ba-4470-80b3-9afb0904861c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:  BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# 加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "print('token: ', token)\n",
    "\n",
    "\n",
    "# 加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese').to(device)\n",
    "# print('pretrained ', pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2674c-afd3-49dc-8efa-f0a7c43b214c",
   "metadata": {},
   "source": [
    "### 定义 collate_fn 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d7da3f-5202-4d6f-9e89-e98b1e6048d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    # print(type(data))\n",
    "    # print('data数据: %s' % data)\n",
    "    # print('data长度: ', len(data))\n",
    "\n",
    "    \n",
    "    # 取每个评论的第一个字符作为数据, 第二作为标签\n",
    "    # 猫和老鼠的DVD,我在当当网已买过10余次了。除了做为礼物送给亲朋好有的孩子外，...\n",
    "    \n",
    "    sents =  [i[0] for i in data] # 猫 ...  16个\n",
    "    labels = [i[1] for i in data] # 和 ...   16个\n",
    "    \n",
    "    #print('sents: %s sents长度: %s' % (sents, len(sents)))\n",
    "    #print('labels: %s' % labels)    \n",
    "    \n",
    "    # 编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True\n",
    "                                  )\n",
    "    \n",
    "    # dict_keys(['input_ids', 'token_type_ids', 'length', 'attention_mask'])\n",
    "    # print('编码后的date: ', data.keys()) \n",
    "    \n",
    "    #print('input_ids状态: ', data['input_ids'].shape) [16, 500]\n",
    "    #print('token_type_ids状态: ', data['token_type_ids'].shape) [16, 500]\n",
    "    #print('attention_mask状态: ', data['attention_mask'].shape) [16, 500]\n",
    "    \n",
    "    # 打印\n",
    "    # print(data['input_ids'])\n",
    "    \n",
    "    \n",
    "    # input_ids:编码之后的数字\n",
    "    # attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5797c595-bc4f-4ce8-a768-dfdaea9b08b6",
   "metadata": {},
   "source": [
    "## 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24347ac8-ffcf-40a4-ab4e-5bfcf71e4d23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 torch.Size([16, 500]) torch.Size([16, 500]) torch.Size([16, 500]) [1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1]\n",
      "\n",
      "\n",
      "1 torch.Size([16, 500]) torch.Size([16, 500]) torch.Size([16, 500]) [0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "\n",
      "\n",
      "2 torch.Size([16, 500]) torch.Size([16, 500]) torch.Size([16, 500]) [0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0]\n",
      "\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "\n",
    "for i, values in enumerate(loader):\n",
    "    input_ids, attention_mask, token_type_ids, labels = values\n",
    "    \n",
    "    print('')\n",
    "    # print(i, values)\n",
    "    print(i, input_ids.shape, \n",
    "          attention_mask.shape, \n",
    "          token_type_ids.shape, \n",
    "          labels\n",
    "         )\n",
    "    print('')\n",
    "    if i >= 2:\n",
    "        break\n",
    "    \n",
    "\n",
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6fe6a5-fbb4-4622-8859-cc660c82eab3",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b47b45-1ca2-41a4-91b0-7d35e689663c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "    \n",
    "#模型试算\n",
    "#out = pretrained(input_ids=input_ids,\n",
    "#           attention_mask=attention_mask,\n",
    "#           token_type_ids=token_type_ids)\n",
    "\n",
    "#out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eea38d54-2c3d-47b8-b16b-afe02201f9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "            pass\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "        out = out.softmax(dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "#model(input_ids=input_ids,\n",
    "#      attention_mask=attention_mask,\n",
    "#      token_type_ids=token_type_ids\n",
    "#     ).shape\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81844361-4629-4469-97ee-7f54628b12b7",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9312a384-e700-46d7-a5e9-48ccf14098b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mylady/.virtualenvs/dl-pytorch/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要训练 600 次\n",
      "0 0.7100347280502319 0.4375\n",
      "5 0.6571966409683228 0.75\n",
      "10 0.666100025177002 0.625\n",
      "15 0.6181852221488953 0.8125\n",
      "20 0.6298019289970398 0.625\n",
      "25 0.6471292972564697 0.75\n",
      "30 0.5541802048683167 0.75\n",
      "35 0.5615012645721436 0.8125\n",
      "40 0.5748679637908936 0.625\n",
      "45 0.5750176310539246 0.6875\n",
      "50 0.5535979866981506 0.6875\n",
      "55 0.4942813217639923 0.9375\n",
      "60 0.48409798741340637 0.8125\n",
      "65 0.5022131204605103 0.875\n",
      "70 0.5164206624031067 0.8125\n",
      "75 0.4828246533870697 0.875\n",
      "80 0.4307330250740051 1.0\n",
      "85 0.5142767429351807 0.8125\n",
      "90 0.4294148087501526 0.875\n",
      "95 0.47537413239479065 0.875\n",
      "100 0.4885493814945221 0.875\n",
      "105 0.5826014280319214 0.6875\n",
      "110 0.6002951264381409 0.625\n",
      "115 0.472451388835907 0.9375\n",
      "120 0.5165157318115234 0.9375\n",
      "125 0.5182137489318848 0.8125\n",
      "130 0.46300631761550903 0.875\n",
      "135 0.4535527229309082 0.9375\n",
      "140 0.5014957785606384 0.875\n",
      "145 0.49747657775878906 0.8125\n",
      "150 0.4090587794780731 0.9375\n",
      "155 0.5088287591934204 0.8125\n",
      "160 0.4976530969142914 0.875\n",
      "165 0.4855462312698364 0.875\n",
      "170 0.4235306680202484 0.875\n",
      "175 0.38155293464660645 1.0\n",
      "180 0.4765636920928955 0.875\n",
      "185 0.47155165672302246 0.875\n",
      "190 0.445909321308136 0.875\n",
      "195 0.46765023469924927 0.875\n",
      "200 0.5023977756500244 0.9375\n",
      "205 0.45788246393203735 0.8125\n",
      "210 0.3829420804977417 1.0\n",
      "215 0.4707658290863037 0.875\n",
      "220 0.4046308994293213 0.9375\n",
      "225 0.4481063783168793 0.875\n",
      "230 0.47885778546333313 0.875\n",
      "235 0.3917713463306427 1.0\n",
      "240 0.5472802519798279 0.8125\n",
      "245 0.45790597796440125 0.8125\n",
      "250 0.4883003234863281 0.875\n",
      "255 0.4566722512245178 0.875\n",
      "260 0.41664212942123413 0.875\n",
      "265 0.46271082758903503 0.875\n",
      "270 0.5157526731491089 0.8125\n",
      "275 0.42343705892562866 0.9375\n",
      "280 0.4704968333244324 0.8125\n",
      "285 0.40107962489128113 0.9375\n",
      "290 0.5527004599571228 0.6875\n",
      "295 0.5187957882881165 0.8125\n",
      "300 0.47366514801979065 0.875\n",
      "当前i:  300\n",
      "305 0.48141148686408997 0.875\n",
      "310 0.7096007466316223 0.5625\n",
      "315 0.4898744225502014 0.8125\n",
      "320 0.36583784222602844 0.9375\n",
      "325 0.4084402620792389 0.9375\n",
      "330 0.49768057465553284 0.8125\n",
      "335 0.5242894887924194 0.75\n",
      "340 0.4037151038646698 0.9375\n",
      "345 0.37598365545272827 0.9375\n",
      "350 0.49798113107681274 0.75\n",
      "355 0.4176892340183258 0.9375\n",
      "360 0.42365384101867676 0.9375\n",
      "365 0.5169790983200073 0.8125\n",
      "370 0.4413790702819824 0.875\n",
      "375 0.3783842623233795 0.9375\n",
      "380 0.42024144530296326 0.9375\n",
      "385 0.36662009358406067 1.0\n",
      "390 0.5101943612098694 0.75\n",
      "395 0.4181376099586487 0.9375\n",
      "400 0.44144952297210693 0.875\n",
      "405 0.45955929160118103 0.875\n",
      "410 0.518494725227356 0.75\n",
      "415 0.4317741394042969 0.875\n",
      "420 0.39958491921424866 0.9375\n",
      "425 0.4478139281272888 0.875\n",
      "430 0.41849857568740845 0.875\n",
      "435 0.4528256356716156 0.875\n",
      "440 0.5993853807449341 0.75\n",
      "445 0.4792500436306 0.875\n",
      "450 0.5806434154510498 0.6875\n",
      "455 0.4785293936729431 0.875\n",
      "460 0.39883872866630554 0.9375\n",
      "465 0.42069387435913086 0.9375\n",
      "470 0.5263380408287048 0.8125\n",
      "475 0.4951823353767395 0.8125\n",
      "480 0.5407077074050903 0.75\n",
      "485 0.3363840579986572 1.0\n",
      "490 0.5019220113754272 0.875\n",
      "495 0.39699485898017883 0.9375\n",
      "500 0.4382079243659973 0.875\n",
      "505 0.4200308322906494 0.875\n",
      "510 0.5175732970237732 0.8125\n",
      "515 0.3406389355659485 1.0\n",
      "520 0.3910712003707886 0.9375\n",
      "525 0.36396411061286926 1.0\n",
      "530 0.4160196781158447 0.9375\n",
      "535 0.4582490921020508 0.875\n",
      "540 0.4059014320373535 0.9375\n",
      "545 0.4073733687400818 0.9375\n",
      "550 0.40163472294807434 0.875\n",
      "555 0.378678560256958 1.0\n",
      "560 0.3848426043987274 0.9375\n",
      "565 0.498455673456192 0.8125\n",
      "570 0.41229358315467834 0.875\n",
      "575 0.37937161326408386 0.9375\n",
      "580 0.4252006709575653 0.875\n",
      "585 0.33958345651626587 1.0\n",
      "590 0.4276103973388672 0.9375\n",
      "595 0.4967587888240814 0.8125\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "# 训练\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "print('需要训练 %s 次' % len(loader))\n",
    "\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    \n",
    "    # print('input_ids.shape: %s, labels.shape: %s \\n' % (input_ids.shape, labels.shape))\n",
    "    # print('input_ids: %s, labels: %s' % (input_ids, labels))\n",
    "    # print(\"\")\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    token_type_ids = token_type_ids.to(device)\n",
    "    labels = torch.tensor(labels).to(device)\n",
    "        \n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "    # print('')\n",
    "    # print('out状态: ', out, out.shape)\n",
    "    # print('labels状态: ', labels, labels)\n",
    "    \n",
    "    # 梯度下降\n",
    "    l = loss(out, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    if i % 5 == 0:\n",
    "        out = out.cpu()\n",
    "        labels = labels.cpu()\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "        print(i, l.item(), accuracy)\n",
    "\n",
    "    if i == 300:\n",
    "        print('当前i:  %s' % i)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade015f3-7b69-4ba3-8273-f29605376e9c",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c14c9d-afa8-4d2c-bf3c-f2ebbf400295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_res():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):\n",
    "        \n",
    "        if i > 100:\n",
    "            break\n",
    "            \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "            pass\n",
    "        \n",
    "        out = out.cpu()\n",
    "        labels = labels.cpu()\n",
    "        out = out.argmax(dim=1)\n",
    "        \n",
    "        # print('text: %s, 预测值: %s, 真实值: %s' % (input_ids, out, labels))\n",
    "        \n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "        \n",
    "        print('当前批次, acc: %s' % (out.sum() / (out == labels).sum()))\n",
    "\n",
    "    print('acc: ', correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c426871d-1416-4d09-a172-b2eabfc6d1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這間酒店環境和服務態度亦算不錯,但房間空間太小~~不宣容納太大件行李~~且房間格調還可以~~ 中餐廳的廣東點心不太好吃~~要改善之~~~~但算價錢平宜~~可接受~~ 西餐廳格調都很好~~但吃的味道一般且令人等得太耐了~~要改善之~~\n",
      "1\n",
      "当前批次, acc: tensor(0.9600)\n",
      "当前批次, acc: tensor(0.4333)\n",
      "当前批次, acc: tensor(0.4839)\n",
      "当前批次, acc: tensor(0.5714)\n",
      "当前批次, acc: tensor(0.5357)\n",
      "当前批次, acc: tensor(0.5161)\n",
      "当前批次, acc: tensor(0.5667)\n",
      "当前批次, acc: tensor(0.4483)\n",
      "当前批次, acc: tensor(0.7917)\n",
      "当前批次, acc: tensor(0.5517)\n",
      "当前批次, acc: tensor(0.3667)\n",
      "当前批次, acc: tensor(0.7692)\n",
      "当前批次, acc: tensor(0.6800)\n",
      "当前批次, acc: tensor(0.4667)\n",
      "当前批次, acc: tensor(0.6333)\n",
      "当前批次, acc: tensor(0.6897)\n",
      "当前批次, acc: tensor(0.5769)\n",
      "当前批次, acc: tensor(0.6667)\n",
      "当前批次, acc: tensor(0.6333)\n",
      "当前批次, acc: tensor(0.5769)\n",
      "当前批次, acc: tensor(0.8750)\n",
      "当前批次, acc: tensor(0.4815)\n",
      "当前批次, acc: tensor(0.6071)\n",
      "当前批次, acc: tensor(0.4516)\n",
      "当前批次, acc: tensor(0.5172)\n",
      "当前批次, acc: tensor(0.4815)\n",
      "当前批次, acc: tensor(0.4828)\n",
      "当前批次, acc: tensor(0.6000)\n",
      "当前批次, acc: tensor(0.5000)\n",
      "当前批次, acc: tensor(0.7083)\n",
      "当前批次, acc: tensor(0.5806)\n",
      "当前批次, acc: tensor(0.6000)\n",
      "当前批次, acc: tensor(0.5556)\n",
      "当前批次, acc: tensor(0.6897)\n",
      "当前批次, acc: tensor(0.4615)\n",
      "当前批次, acc: tensor(0.3448)\n",
      "当前批次, acc: tensor(0.7308)\n",
      "acc:  0.8733108108108109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 测试结果\n",
    "test_res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05475a9-9ccf-4968-a401-9fea74273ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c21da-e8a3-4fa1-a92a-7f1d12253d05",
   "metadata": {},
   "source": [
    "## 计算后的模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52a4ae6a-faae-4943-aaef-f62231cd4ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "torch.save(model.state_dict(), \n",
    "           'chinese_class_mission_2023_4_20.pt') # 推荐的文件后缀名是pt或pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f88ea207-8690-4be0-be5c-36a8cbbfd407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 封装检测类\n",
    "\n",
    "def str_felling_detect(model, str_sents):\n",
    "    \n",
    "\n",
    "    out = token.encode_plus(str_sents,\n",
    "                            # 当句子长度大于max_length时,截断\n",
    "                            truncation=True,\n",
    "                            # 一律补pad到 max_length长度\n",
    "                            padding='max_length',\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=500,\n",
    "\n",
    "                            #可取值tf,pt,np,默认为返回list\n",
    "                            return_tensors=None,\n",
    "                            #返回token_type_ids\n",
    "                            return_token_type_ids=True,\n",
    "                            #返回attention_mask\n",
    "                            return_attention_mask=True,\n",
    "                            #返回special_tokens_mask 特殊符号标识\n",
    "                            return_special_tokens_mask=True,\n",
    "\n",
    "                            #返回offset_mapping 标识每个词的起止位置,\n",
    "                            # 这个参数只能BertTokenizerFast使用\n",
    "                            #return_offsets_mapping=True,\n",
    "\n",
    "                            #返回length 标识长度\n",
    "                            return_length=True\n",
    "                           )\n",
    "    \n",
    "    input_ids = torch.tensor([out['input_ids']]).to(device)\n",
    "    attention_mask = torch.tensor([out['attention_mask']]).to(device)\n",
    "    token_type_ids = torch.tensor([out['token_type_ids']]).to(device)\n",
    "\n",
    "    out_test = model(input_ids=input_ids,\n",
    "                     attention_mask=attention_mask,                 \n",
    "                     token_type_ids=token_type_ids\n",
    "                    )\n",
    "    print(out_test)\n",
    "    \n",
    "    # print(out_test.argmax(dim=1).cpu())  # tensor([1])\n",
    "    out_res = out_test.argmax(dim=1).cpu().item()\n",
    "    # print(out_res, type(out_res))  # <class 'int'>\n",
    "    \n",
    "    if out_res == 1:\n",
    "        print('1 正面情感输出')\n",
    "    elif out_res == 0:\n",
    "        print('0 负面情感输出')\n",
    "    else:\n",
    "        print('中性句子')\n",
    "\n",
    "    return out_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca2406a-26e0-4df5-bbe6-5b5224cec97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0221, 0.9779]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "1 正面情感输出\n",
      "\n",
      "tensor([[0.9989, 0.0011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "0 负面情感输出\n",
      "\n",
      "tensor([[0.9481, 0.0519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "0 负面情感输出\n",
      "\n",
      "tensor([[0.8016, 0.1984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "0 负面情感输出\n",
      "\n",
      "tensor([[0.0011, 0.9989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "1 正面情感输出\n",
      "\n"
     ]
    }
   ],
   "source": [
    "str_1 = '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁' \\\n",
    "       '泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般'\n",
    "\n",
    "str_2 = \"\"\"1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. 3.机盖的钢琴漆，手不能摸，一摸一个印. 4.硬盘分区不好办.\"\"\"\n",
    "\n",
    "str_3 = '房间太小。其他的都一般。。。。。。。。。'\n",
    "\n",
    "str_4 = \"呵呵，虽然表皮看上去不错很精致，但是我还是能看得出来是盗的。但是里面的内容真的不错，我妈爱看，我自己也学着找一些穴位。\"\n",
    "\n",
    "str_5 =\"地理位置佳，在市中心。酒店服务好、早餐品种丰富。我住的商务数码房电脑宽带速度满意,房间还算干净，离湖南路小吃街近\"\n",
    "\n",
    "\n",
    "for str_sents in [str_1, str_2, str_3, str_4, str_5]:\n",
    "    str_felling_detect(model, str_sents)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fbd2c-2f31-477c-bcc7-13f3406199be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
