{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70df8b90-d791-4738-8ec8-5b38e692cdc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9b4748-e618-49f4-814c-6fdc02512eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, split):\n",
    "        # dataset = load_dataset('lansinuote/ChnSentiCorp', keep_in_memory=True)\n",
    "        dataset = load_from_disk('./data/ChnSentiCorp')\n",
    "        \n",
    "        print(dataset)\n",
    "        def f(data):\n",
    "            return len(data['text']) > 30\n",
    "        self.dataset = dataset.filter(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd4f6103-b189-4e37-91e7-3ca9398d6228",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/validation/cache-cf45964edee402a8.arrow\n",
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/test/cache-c6f7400aef16ddba.arrow\n",
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/train/cache-478819d08c52879a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9600\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('train')\n",
    "len(dataset) # , dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2da91b69-baf4-4dbe-9c19-aceaa45b6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# 加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "print(token)\n",
    "\n",
    "\n",
    "# 加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a68a67a7-66c6-4a2f-81dc-a94e2a5fe56d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \n",
    "    # print(data)\n",
    "    # print('data长度: ', len(data))\n",
    "    # 编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=data,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=30,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    # print('编码后的data打印: ', data.keys())\n",
    "    \n",
    "    # input_ids:编码之后的数字\n",
    "    # attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "\n",
    "    # 把第15个词固定替换为mask\n",
    "    # print('第15个词: %s' % input_ids[:, 15])\n",
    "    \n",
    "    # 这里直接使用了编码后的数据作为真实预测值\n",
    "    labels = input_ids[:, 15].reshape(-1).clone()\n",
    "    input_ids[:, 15] = token.get_vocab()[token.mask_token]\n",
    "\n",
    "    # print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1050958-e408-47c1-bb00-f5c366b9f558",
   "metadata": {},
   "source": [
    "## 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f4f90b-3933-44b3-8114-5426d4af7160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574\n",
      "[CLS] 这 本 书 我 又 反 复 的 读 了 几 遍 ， 作 [MASK] 的 语 句 虽 然 没 有 别 人 太 好 ， 但 [SEP]\n",
      "者\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n",
      "574\n",
      "[CLS] 真 没 想 到 我 每 次 在 当 当 网 上 精 心 [MASK] 儿 子 挑 选 的 图 书, 他 都 非 常 的 [SEP]\n",
      "为\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n",
      "574\n",
      "[CLS] 我 非 常 不 满 意 新 月 阁 客 栈 ， 如 果 [MASK] 零 分 可 以 选 ， 我 宁 可 一 分 也 不 [SEP]\n",
      "有\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n",
      "574\n",
      "[CLS] 很 漂 亮 ， 黑 漆 加 白 灯 很 亮 ， 配 置 [MASK] 经 很 高 了 ， 价 格 能 接 受 ， 一 直 [SEP]\n",
      "已\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n",
      "574\n",
      "[CLS] 国 庆 期 间 住 过 本 酒 店 ， 我 的 评 价 [MASK] 是 \" 差, 实 在 实 际 太 差 \". 第 [SEP]\n",
      "就\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n",
      "574\n",
      "[CLS] 看 完 不 知 道 作 者 到 底 想 说 什 么 ， [MASK] 了 解 一 下 具 体 的 方 法 结 果 一 无 [SEP]\n",
      "想\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset['train'],\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True\n",
    "                                    )\n",
    "\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    print(len(loader))\n",
    "    print(token.decode(input_ids[0]))\n",
    "    print(token.decode(labels[0]))\n",
    "    print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels.shape)\n",
    "    print(\"\")\n",
    "    \n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e280df8b-ee2e-43a5-b520-988f910200e8",
   "metadata": {},
   "source": [
    "## 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc54602-0d1d-4196-afe8-e722bd450e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "\n",
    "def test_1():\n",
    "    # 模型试算\n",
    "    out = pretrained(input_ids=input_ids,\n",
    "                     attention_mask=attention_mask,\n",
    "                     token_type_ids=token_type_ids\n",
    "                    )\n",
    "\n",
    "    out.last_hidden_state.shape\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff667c16-9b46-4c71-9917-c4710b807862",
   "metadata": {},
   "source": [
    "## 定义下游任务模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fba2358-342d-4216-987f-3dc874137baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = torch.nn.Linear(768, token.vocab_size, bias=False)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(token.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        \n",
    "        # pretrained = pretrained.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "            pass\n",
    "        \n",
    "        out = self.decoder(out.last_hidden_state[:, 15])\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "#model(input_ids=input_ids,\n",
    "#      attention_mask=attention_mask,\n",
    "#      token_type_ids=token_type_ids\n",
    "#     ).shape\n",
    "\n",
    "\n",
    "# 模型转移到GPU上\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c93a2c8-e36e-4fbd-a011-5f485658bb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499cc25f-639e-4a0d-8626-3d82c227da68",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0806ae15-3701-4fad-bc89-56e919cdb40a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "训练批次: 0 当前第 0 轮训练 loss: 10.269051551818848 acc: 0.0\n",
      "训练批次: 0 当前第 100 轮训练 loss: 6.521791458129883 acc: 0.125\n",
      "训练批次: 0 当前第 200 轮训练 loss: 5.197349548339844 acc: 0.25\n",
      "训练批次: 0 当前第 300 轮训练 loss: 5.080999851226807 acc: 0.25\n",
      "训练批次: 0 当前第 400 轮训练 loss: 3.2679288387298584 acc: 0.5625\n",
      "训练批次: 0 当前第 500 轮训练 loss: 3.4124574661254883 acc: 0.5\n",
      "训练批次: 1 当前第 0 轮训练 loss: 2.7553489208221436 acc: 0.5\n",
      "训练批次: 1 当前第 100 轮训练 loss: 3.156139612197876 acc: 0.5625\n",
      "训练批次: 1 当前第 200 轮训练 loss: 2.1555659770965576 acc: 0.625\n",
      "训练批次: 1 当前第 300 轮训练 loss: 1.5703136920928955 acc: 0.8125\n",
      "训练批次: 1 当前第 400 轮训练 loss: 1.6401739120483398 acc: 0.75\n",
      "训练批次: 1 当前第 500 轮训练 loss: 2.5194320678710938 acc: 0.625\n",
      "训练批次: 2 当前第 0 轮训练 loss: 1.0090479850769043 acc: 0.9375\n",
      "训练批次: 2 当前第 100 轮训练 loss: 1.579595685005188 acc: 0.625\n",
      "训练批次: 2 当前第 200 轮训练 loss: 0.6561413407325745 acc: 0.875\n",
      "训练批次: 2 当前第 300 轮训练 loss: 1.3835166692733765 acc: 0.8125\n",
      "训练批次: 2 当前第 400 轮训练 loss: 1.1443946361541748 acc: 0.75\n",
      "训练批次: 2 当前第 500 轮训练 loss: 1.8488258123397827 acc: 0.5\n",
      "训练批次: 3 当前第 0 轮训练 loss: 0.46124163269996643 acc: 0.875\n",
      "训练批次: 3 当前第 100 轮训练 loss: 0.5599803924560547 acc: 0.875\n",
      "训练批次: 3 当前第 200 轮训练 loss: 0.4810965657234192 acc: 1.0\n",
      "训练批次: 3 当前第 300 轮训练 loss: 0.6486729979515076 acc: 0.875\n",
      "训练批次: 3 当前第 400 轮训练 loss: 0.5205398201942444 acc: 0.9375\n",
      "训练批次: 3 当前第 500 轮训练 loss: 0.33221232891082764 acc: 0.9375\n",
      "训练批次: 4 当前第 0 轮训练 loss: 0.30675163865089417 acc: 0.9375\n",
      "训练批次: 4 当前第 100 轮训练 loss: 0.5309487581253052 acc: 0.875\n",
      "训练批次: 4 当前第 200 轮训练 loss: 0.4211013615131378 acc: 0.875\n",
      "训练批次: 4 当前第 300 轮训练 loss: 0.4312525689601898 acc: 0.875\n",
      "训练批次: 4 当前第 400 轮训练 loss: 0.2765580713748932 acc: 1.0\n",
      "训练批次: 4 当前第 500 轮训练 loss: 0.10674941539764404 acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "# 训练\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# 损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "print(\"training on \", device)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "        # print('labels', labels)\n",
    "        # labels tensor([2523, 1962, ....,  6817, 1962])\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                   )\n",
    "        l = loss(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            out = out.cpu()\n",
    "            labels = labels.cpu()\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "            print('训练批次: %s 当前第 %s 轮训练 loss: %s acc: %s' % \\\n",
    "                  (epoch, i, l.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea90d73-a897-44f4-86df-560692380a83",
   "metadata": {},
   "source": [
    "## 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "118bc64b-796e-4fe3-ab7a-0b39922ca65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "\n",
    "model_save_path = 'chinese_full_vacant_mission_2023_4_10.pt'\n",
    "# torch.save(model.state_dict(),  model_save_path)  # 推荐的文件后缀名是pt或pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c5e10-b672-4b13-a16b-02e7f85a368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载\n",
    "# 加载保存的模型\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "471367ea-5612-46cf-96d6-6e36c5aadad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "def test_calculate():\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=dataset['test'],\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):\n",
    "        \n",
    "        if i == 15:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "            pass\n",
    "        \n",
    "        out = out.argmax(dim=1).cpu()\n",
    "        labels = labels.cpu()\n",
    "        \n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "        print('序: %s 输入内容: %s' % (i, token.decode(input_ids[0])))\n",
    "        print('解码: ', token.decode(labels[0]))\n",
    "        \n",
    "    print('acc: %.2f' % (correct / total))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87ab28fa-0f54-4897-908f-4aace314550c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序: 0 输入内容: [CLS] 很 满 意 的 一 家 酒 店 ， 装 修 不 错 ， [MASK] 天 住 下 晚 上 房 间 的 网 络 出 了 问 [SEP]\n",
      "解码:  那\n",
      "序: 1 输入内容: [CLS] 酒 店 位 置 很 偏 ， 大 厅 很 漂 亮 ， 可 [MASK] 房 间 里 有 刚 装 修 的 味 道 ， 房 间 [SEP]\n",
      "解码:  是\n",
      "序: 2 输入内容: [CLS] 风 扇 确 实 够 响 的 ， 尤 其 是 到 晚 上 [MASK] 围 安 静 下 来 。 风 扇 频 频 开 启 ， [SEP]\n",
      "解码:  周\n",
      "序: 3 输入内容: [CLS] 没 有 许 多 网 友 评 价 热 度 高 的 问 题 [MASK] 第 一 次 要 手 动 安 装 winxp ， 主 板 [SEP]\n",
      "解码:  ，\n",
      "序: 4 输入内容: [CLS] 先 买 了 《 不 一 样 的 卡 梅 拉 》 的 前 [MASK] 册 给 儿 子 ， 每 天 晚 上 睡 觉 前 都 [SEP]\n",
      "解码:  六\n",
      "序: 5 输入内容: [CLS] 【 荐 书 】 专 家 认 为 目 前 的 由 次 凭 [MASK] 机 引 发 的 金 融 海 啸 只 是 开 始 ， [SEP]\n",
      "解码:  危\n",
      "序: 6 输入内容: [CLS] 当 时 优 惠 了 1300 ， 配 置 高 ， 相 对 于 [MASK] 购 价 来 说 性 价 比 太 高 了 。 不 过 [SEP]\n",
      "解码:  抢\n",
      "序: 7 输入内容: [CLS] 我 的 订 单 是 7 月 17 日 就 发 出 来 了 [MASK] 但 是 现 在 是 8 月 2 日 ， 我 还 没 [SEP]\n",
      "解码:  ，\n",
      "序: 8 输入内容: [CLS] 刚 看 到 第 一 本 的 名 字 《 我 想 去 看 [MASK] 》 就 觉 得 很 可 爱 ， 然 后 看 内 容 [SEP]\n",
      "解码:  海\n",
      "序: 9 输入内容: [CLS] 酒 店 的 位 置 很 好, 距 离 火 车 站 非 [MASK] 近. 总 提 感 觉 酒 店 的 性 价 比 不 [SEP]\n",
      "解码:  常\n",
      "序: 10 输入内容: [CLS] 我 家 小 宝 同 学 特 别 喜 爱 卡 梅 拉 系 [MASK] 故 事, 每 天 晚 上 睡 觉 前 非 讲 不 [SEP]\n",
      "解码:  列\n",
      "序: 11 输入内容: [CLS] 没 有 蓝 牙 、 摄 像 头 ， 连 麦 克 风 都 [MASK] 了 装 [UNK] 有 点 小 麻 烦 ， [UNK] 原 版 没 [SEP]\n",
      "解码:  省\n",
      "序: 12 输入内容: [CLS] 整 机 配 置 均 衡 ， 就 是 内 存 稍 小 ， [MASK] 东 要 是 能 再 送 个 内 存 就 完 美 了 [SEP]\n",
      "解码:  京\n",
      "序: 13 输入内容: [CLS] 住 这 里 很 多 次 了 。 但 是 感 觉 酒 店 [MASK] 服 务 质 量 在 下 降 ， 价 格 反 而 是 [SEP]\n",
      "解码:  的\n",
      "序: 14 输入内容: [CLS] 入 住 11 楼 大 床 房 ， 一 出 电 梯 就 有 [MASK] 显 的 地 毯 散 发 的 霉 味 ； 房 间 的 [SEP]\n",
      "解码:  明\n",
      "acc: 0.68\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "test_calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41571f99-fd1f-4a7c-8191-c14eadaf89ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
