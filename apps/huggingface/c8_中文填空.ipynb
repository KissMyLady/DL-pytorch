{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70df8b90-d791-4738-8ec8-5b38e692cdc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9b4748-e618-49f4-814c-6fdc02512eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/train/cache-477de4adc0b5b333.arrow\n",
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/test/cache-2942213d0d889635.arrow\n",
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/validation/cache-83eb6d135357ad0e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9600\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, split):\n",
    "        # dataset = load_dataset('lansinuote/ChnSentiCorp', keep_in_memory=True)\n",
    "        dataset = load_from_disk('./data/ChnSentiCorp')\n",
    "        \n",
    "        print(dataset)\n",
    "        def f(data):\n",
    "            return len(data['text']) > 30\n",
    "        self.dataset = dataset.filter(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        return text\n",
    "\n",
    "    \n",
    "dataset = Dataset('train')\n",
    "len(dataset) # , dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be49e477-fefc-400b-82aa-5c884627d9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x7fbbc9736b50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset['test']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da91b69-baf4-4dbe-9c19-aceaa45b6c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a68a67a7-66c6-4a2f-81dc-a94e2a5fe56d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \n",
    "    # print(data)\n",
    "    print('data长度: ', len(data))\n",
    "    # 编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=data,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=30,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    print('编码后的data打印: ', data.keys())\n",
    "    \n",
    "    # input_ids:编码之后的数字\n",
    "    # attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "\n",
    "    # 把第15个词固定替换为mask\n",
    "    print('第15个词: %s' % input_ids[:, 15])\n",
    "    \n",
    "    # 这里直接使用了编码后的数据作为真实预测值\n",
    "    labels = input_ids[:, 15].reshape(-1).clone()\n",
    "    input_ids[:, 15] = token.get_vocab()[token.mask_token]\n",
    "\n",
    "    # print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1050958-e408-47c1-bb00-f5c366b9f558",
   "metadata": {},
   "source": [
    "## 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97f4f90b-3933-44b3-8114-5426d4af7160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data长度:  16\n",
      "编码后的data打印:  dict_keys(['input_ids', 'token_type_ids', 'length', 'attention_mask'])\n",
      "第15个词: tensor([6163, 2523, 4500, 6235, 5291, 6963,  749, 3177, 5010, 5450, 4638, 8024,\n",
      "        6843, 2190, 2382, 8111])\n",
      "574\n",
      "[CLS] 整 体 还 可 以 ， 就 是 不 配 系 统 ， 安 [MASK] [UNK] 系 统 不 方 便 ， 换 了 几 个 系 统 [SEP]\n",
      "装\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n",
      "data长度:  16\n",
      "编码后的data打印:  dict_keys(['input_ids', 'token_type_ids', 'length', 'attention_mask'])\n",
      "第15个词: tensor([ 122,  671, 8039, 3949, 3198, 6772,  749,  131, 6981, 4684, 4669, 3189,\n",
      "        4192, 4638,  117, 6821])\n",
      "574\n",
      "[CLS] 二 年 前 给 他 买 了 全 套 的 神 奇 校 车 [MASK] ， 这 套 书 成 了 他 最 喜 欢 看 书 之 [SEP]\n",
      "1\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n",
      "data长度:  16\n",
      "编码后的data打印:  dict_keys(['input_ids', 'token_type_ids', 'length', 'attention_mask'])\n",
      "第15个词: tensor([ 738, 1104, 1501, 1213,  679,  100, 6772, 7028, 1391, 1963, 1380, 4638,\n",
      "        8024, 1057, 8024, 2382])\n",
      "574\n",
      "[CLS] 每 个 人 的 看 法 可 能 都 有 所 不 同 ， [MASK] 许 还 在 围 墙 外 的 我 没 有 用 心 去 [SEP]\n",
      "也\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n",
      "data长度:  16\n",
      "编码后的data打印:  dict_keys(['input_ids', 'token_type_ids', 'length', 'attention_mask'])\n",
      "第15个词: tensor([  743,   674,  1922,  4937,  6956,  4276,  1962,  2382,  2769,  6843,\n",
      "         8024,  8024,  6438,  2533,  8024, 10086])\n",
      "574\n",
      "[CLS] 系 统 要 自 己 装 ， 会 让 你 很 头 痛 ！ [MASK] 回 来 发 现 不 能 上 网 有 线 和 无 线 [SEP]\n",
      "买\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n",
      "data长度:  16\n",
      "编码后的data打印:  dict_keys(['input_ids', 'token_type_ids', 'length', 'attention_mask'])\n",
      "第15个词: tensor([ 809, 3315, 6983, 6943, 1296, 2421, 1315, 1469, 3221, 3215, 8024, 7770,\n",
      "        1369,  741, 1139, 2989])\n",
      "574\n",
      "[CLS] 宾 馆 虽 然 比 较 老 ， 房 间 感 觉 还 可 [MASK] ， 服 务 很 好 ， 早 餐 品 种 少 了 点 [SEP]\n",
      "以\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n",
      "data长度:  16\n",
      "编码后的data打印:  dict_keys(['input_ids', 'token_type_ids', 'length', 'attention_mask'])\n",
      "第15个词: tensor([ 749,  720, 5636, 1045,  678, 3291,  738,  934,  741,  119,  778, 8024,\n",
      "        8024,  511,  511, 1355])\n",
      "574\n",
      "[CLS] 什 么 时 候 能 收 到, 实 在 不 行, 退 [MASK] 我 再 订, 送 货 上 门 好 了. 送 别 [SEP]\n",
      "了\n",
      "torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16, 30]) torch.Size([16])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset['train'],\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True\n",
    "                                    )\n",
    "\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    print(len(loader))\n",
    "    print(token.decode(input_ids[0]))\n",
    "    print(token.decode(labels[0]))\n",
    "    print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels.shape)\n",
    "    print(\"\")\n",
    "    \n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e280df8b-ee2e-43a5-b520-988f910200e8",
   "metadata": {},
   "source": [
    "## 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc54602-0d1d-4196-afe8-e722bd450e01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# 加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese').to(device)\n",
    "\n",
    "\n",
    "# 不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "\n",
    "def test_1():\n",
    "    # 模型试算\n",
    "    out = pretrained(input_ids=input_ids,\n",
    "                     attention_mask=attention_mask,\n",
    "                     token_type_ids=token_type_ids\n",
    "                    )\n",
    "\n",
    "    out.last_hidden_state.shape\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff667c16-9b46-4c71-9917-c4710b807862",
   "metadata": {},
   "source": [
    "## 定义下游任务模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fba2358-342d-4216-987f-3dc874137baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = torch.nn.Linear(768, token.vocab_size, bias=False)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(token.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        \n",
    "        # pretrained = pretrained.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "            pass\n",
    "        \n",
    "        out = self.decoder(out.last_hidden_state[:, 15])\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "#model(input_ids=input_ids,\n",
    "#      attention_mask=attention_mask,\n",
    "#      token_type_ids=token_type_ids\n",
    "#     ).shape\n",
    "\n",
    "\n",
    "# 模型转移到GPU上\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c93a2c8-e36e-4fbd-a011-5f485658bb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499cc25f-639e-4a0d-8626-3d82c227da68",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806ae15-3701-4fad-bc89-56e919cdb40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "# 训练\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# 损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "print(\"training on \", device)\n",
    "for epoch in range(5):\n",
    "    \n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "        # print('labels', labels)\n",
    "        # labels tensor([2523, 1962, ....,  6817, 1962])\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # print(input_ids)\n",
    "        \n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                   )\n",
    "        \n",
    "        # print('out: ', out, out.shape)  torch.Size([16, 21128])\n",
    "        # print('labels: ', labels, labels.shape)  torch.Size([16])\n",
    "        # print(out.cpu())\n",
    "\n",
    "        l = loss(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            out = out.cpu()\n",
    "            labels = labels.cpu()\n",
    "            \n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "            print('训练批次: %s 当前第 %s 轮训练 loss: %s acc: %s' % (\n",
    "                        epoch, i, l.item(), accuracy)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "471367ea-5612-46cf-96d6-6e36c5aadad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "def test_calculate():\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=dataset['test'],\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):\n",
    "        \n",
    "        if i == 15:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "            pass\n",
    "        \n",
    "        out = out.argmax(dim=1).cpu()\n",
    "        labels = labels.cpu()\n",
    "        \n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "        print('序: %s 输入内容: %s' % (i, token.decode(input_ids[0])))\n",
    "        print('解码: ', token.decode(labels[0]), token.decode(labels[0]))\n",
    "        \n",
    "    print('acc: %.2f' % (correct / total))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87ab28fa-0f54-4897-908f-4aace314550c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序: 0 输入内容: [CLS] 跟 很 多 买 家 一 样 遇 到 了 表 面 金 属 [MASK] 牌 划 痕 问 题 ， 但 知 道 没 法 退 货 [SEP]\n",
      "解码:  标 标\n",
      "序: 1 输入内容: [CLS] 外 观 跟 我 想 象 的 不 一 样 ， 是 康 博 [MASK] 电 脑 ， 一 查 原 来 也 属 于 [UNK] 。 没 [SEP]\n",
      "解码:  的 的\n",
      "序: 2 输入内容: [CLS] 房 间 很 大 味 道 ， 服 务 差 ， 态 度 差 [MASK] 房 单 设 施 差 ~ ~ 永 远 不 会 再 去 [SEP]\n",
      "解码:  ， ，\n",
      "序: 3 输入内容: [CLS] 1. 电 池 时 间 长 ， 普 通 使 用 5 - [MASK] 小 时 没 问 题 ， 省 点 用 可 以 到 近 [SEP]\n",
      "解码:  6 6\n",
      "序: 4 输入内容: [CLS] 1 、 打 车 到 宾 馆 和 离 开 ， 送 上 登 [MASK] 出 租 车 牌 号 的 卡 片 ， 如 有 物 品 [SEP]\n",
      "解码:  记 记\n",
      "序: 5 输入内容: [CLS] 书 送 来 的 时 候 刚 好 是 六 一 儿 童 节 [MASK] 天 的 中 午 ， 儿 子 见 到 书 之 后 迅 [SEP]\n",
      "解码:  那 那\n",
      "序: 6 输入内容: [CLS] 太 重 ， 装 系 统 只 能 装 [UNK] 的 ， 我 的 [MASK] 版 xp3 都 不 能 装 ， 搞 了 我 半 天 [SEP]\n",
      "解码:  正 正\n",
      "序: 7 输入内容: [CLS] 不 能 装 锁 、 光 驱 读 盘 能 力 不 算 太 [MASK] 、 圆 通 快 递 太 离 谱 ， 到 货 无 通 [SEP]\n",
      "解码:  好 好\n",
      "序: 8 输入内容: [CLS] flash 分 编 程 和 动 画 两 部 分 ， 这 本 书 [MASK] 的 是 flash [UNK] ， 可 是 脚 本 的 运 用 任 [SEP]\n",
      "解码:  说 说\n",
      "序: 9 输入内容: [CLS] 住 这 个 酒 店 实 在 是 太 享 受 了, 不 [MASK] 可 以 使 用 五 彩 缤 纷 的 白 毛 巾, [SEP]\n",
      "解码:  仅 仅\n",
      "序: 10 输入内容: [CLS] 性 价 比 高 ， led 屏 幕 ， 西 数 的 硬 盘 [MASK] ddr3 的 内 存 ， 装 系 统 的 时 候 也 [SEP]\n",
      "解码:  ， ，\n",
      "序: 11 输入内容: [CLS] 硬 盘 分 区 很 让 我 崩 溃 ， [UNK] 盘 [UNK] ， [MASK] 共 也 就 [UNK] 的 盘 ， 也 不 知 道 当 时 [SEP]\n",
      "解码:  一 一\n",
      "序: 12 输入内容: [CLS] 外 观 配 置 令 我 满 意 。 包 装 很 好 ， [MASK] 到 的 是 5. 12 的 机 子 ， 经 测 试 [SEP]\n",
      "解码:  买 买\n",
      "序: 13 输入内容: [CLS] 300 多 的 价 格 入 住 这 家 佛 山 历 史 最 [MASK] 久 的 酒 店 也 真 是 值 得 ， 起 码 起 [SEP]\n",
      "解码:  悠 悠\n",
      "序: 14 输入内容: [CLS] 有 摄 像 头 。 也 不 算 很 重 。 总 体 满 [MASK] ， 驱 动 都 没 有 装 ， ghost xp 雨 林 木 [SEP]\n",
      "解码:  意 意\n",
      "acc: 0.70\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "test_calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41571f99-fd1f-4a7c-8191-c14eadaf89ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
