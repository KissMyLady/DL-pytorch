{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f39e611-dad8-4ab7-85f3-99e8e9661709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dda3bd7-bf21-498e-879c-22b9a66a2faa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /home/mylady/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/e02ba894cf18f3fd9b2526c795f983683c4ec732/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /home/mylady/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/e02ba894cf18f3fd9b2526c795f983683c4ec732/quantization_kernels_parallel.c -shared -o /home/mylady/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/e02ba894cf18f3fd9b2526c795f983683c4ec732/quantization_kernels_parallel.so\n",
      "Load kernel : /home/mylady/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/e02ba894cf18f3fd9b2526c795f983683c4ec732/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 6\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n",
      "åŠ è½½å®Œæ¯•..\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", \n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "\n",
    "chat_model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", \n",
    "                                       trust_remote_code=True).half().cuda()\n",
    "\n",
    "print('åŠ è½½å®Œæ¯•..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557ddb5b-a953-4a59-b96a-e85e87cfa16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = chat_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2975a3f-da8b-4ff8-885d-00d373b2853a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a7192bf-0e3d-4882-b8ed-5b307e73cdc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e30210dc-3e6f-4593-9cd1-3ba47076f088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4441f747-fda5-4fa6-b835-50380eb62ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä»¥ä¸‹æ—¶æ˜¯ä¸€äº›æœ‰ç”¨çš„æŠ€å·§ï¼Œå¯ä»¥å¸®åŠ©åœ¨æ™šä¸Šå…¥ç¡ï¼š\\n\\n1. ä¿æŒå®‰é™ï¼šå°½å¯èƒ½å‡å°‘å™ªéŸ³å’Œåˆºæ¿€ï¼Œä¾‹å¦‚æ‰‹æœºå’Œç”µè§†ã€‚\\n\\n2. æ”¾æ¾èº«ä½“ï¼šå°è¯•è¿›è¡Œæ·±å‘¼å¸æˆ–æ¸è¿›æ€§è‚Œè‚‰æ¾å¼›æ¥æ”¾æ¾èº«ä½“ã€‚\\n\\n3. é¿å…ä½¿ç”¨ç”µå­è®¾å¤‡ï¼šæ‰‹æœºå’Œç”µè„‘çš„è“å…‰å¯èƒ½ä¼šå¹²æ‰°ç¡çœ ã€‚å»ºè®®åœ¨ç¡å‰ 1-2 å°æ—¶åœæ­¢ä½¿ç”¨è¿™äº›è®¾å¤‡ã€‚\\n\\n4. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒï¼šä½¿ç”¨èˆ’é€‚çš„åºŠå«å’Œæ•å¤´ï¼Œä¿æŒæˆ¿é—´çš„æ¸©åº¦å’Œæ¹¿åº¦é€‚å®œã€‚\\n\\n5. é¿å…é¥®é£Ÿå’Œé¥®æ–™çš„åˆºæ¿€ï¼šé¿å…é¥®ç”¨å’–å•¡ã€èŒ¶å’Œå¯ä¹ç­‰åˆºæ¿€æ€§é¥®æ–™ï¼Œä»¥åŠåƒè¾›è¾£æˆ–æ²¹è…»çš„é£Ÿç‰©ã€‚\\n\\n6. ç¡å‰æ”¾æ¾ï¼šåœ¨ç¡å‰ 1-2 å°æ—¶ï¼Œè¿›è¡Œä¸€äº›æ”¾æ¾çš„æ´»åŠ¨ï¼Œä¾‹å¦‚é˜…è¯»æˆ–æ´—æ¾¡ï¼Œå¸®åŠ©æ”¾æ¾èº«å¿ƒã€‚\\n\\nå¦‚æœä»¥ä¸Šæ–¹æ³•ä»ç„¶æ— æ³•å…¥ç¡ï¼Œå»ºè®®å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶ï¼Œè·å–æ›´ä¸“ä¸šçš„å»ºè®®å’Œå¸®åŠ©ã€‚'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d80ae5-87e0-4a86-9d74-7f06a89dcf34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
