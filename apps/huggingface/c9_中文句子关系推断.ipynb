{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d02d7b-061a-4a97-ab11-0091930f86b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "import random, math\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/mylady/code/python/DL-pytorch/apps/chapter_pytorch_demo\")\n",
    "import d2lzh_pytorch.torch as d2l\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9be66b-0c91-4a00-b9a1-11452a033563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日志级别\n",
    "import transformers\n",
    "\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d71f014a-fd28-4f0c-b70e-a138b6f83ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/train/cache-2c893bcab2dc48fd.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载完毕..\n"
     ]
    }
   ],
   "source": [
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, split):\n",
    "        # dataset = load_dataset(path='lansinuote/ChnSentiCorp', split=split)\n",
    "        dataset = load_from_disk('./data/ChnSentiCorp')['%s' % split]\n",
    "        def f(data):\n",
    "            return len(data['text']) > 20\n",
    "        self.dataset = dataset.filter(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset['text'][i]\n",
    "        \n",
    "        # print('原始数据: ', text)\n",
    "        # label = self.dataset['label'][i]\n",
    "        #切分一句话为前半句和后半句\n",
    "        #sentence1 = text[:20]\n",
    "        #sentence2 = text[20:]\n",
    "        \n",
    "        #切分一句话为前半句和后半句\n",
    "        s_len = len(text)\n",
    "        half_len = math.floor(s_len / 2)  # 切分句子为两段\n",
    "        \n",
    "        sentence1 = text[:half_len]\n",
    "        sentence2 = text[half_len: ]\n",
    "        label = 0\n",
    "\n",
    "        # 有一半的概率把后半句替换为一句无关的话\n",
    "        if random.randint(0, 1) == 0:\n",
    "            j = random.randint(0, len(self.dataset) - 1)\n",
    "            # sentence2 = self.dataset[j]['text'][20:40]\n",
    "            sentence2 = self.dataset['text'][j][half_len:]\n",
    "            label = 1\n",
    "\n",
    "        return sentence1, sentence2, label\n",
    "\n",
    "# 加载\n",
    "dataset = Dataset('train')\n",
    "print(\"数据加载完毕..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85d5d72-f53d-4761-a6f6-f11bdca2f3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般', '，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 0)\n",
      "\n",
      "('15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜', '送货要批评一下，星期六下的单，星期三才到。 价格贵了点！', 1)\n",
      "\n",
      "('1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来.', ' 3.机盖的钢琴漆，手不能摸，一摸一个印. 4.硬盘分区不好办.', 0)\n",
      "\n",
      "('今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当', '一下，我想许多入门玩家会着急一下下。。。', 1)\n",
      "\n",
      "('机器背面似乎被撕了张什么标签，残胶还在。但', '！感觉上有种内容不咋的，包装挺豪华的感觉！卖的不是内容而是包装！', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(dataset[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a66e21-53b3-456a-ad48-7475eecf9aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9552 选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般 \t 通还算方便,离地铁不行10分钟内,但你必须知道哪条是正确的路,如果入住,必须要有心理准备,一个字\"小\"! 1\n",
      "9552 15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜 \t 欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错 0\n",
      "9552 1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. \t  1\n",
      "9552 今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当 \t 网是不是该跟出版社商量商量,单独出个第6卷,让我们的孩子不会有所遗憾。 0\n",
      "9552 机器背面似乎被撕了张什么标签，残胶还在。但 \t 就是作者的一些美容方子有些东西我找不到，呵呵！继续看！ 1\n"
     ]
    }
   ],
   "source": [
    "# 测试拼接的数据\n",
    "for t_item in range(5):\n",
    "    sentence1, sentence2, label = dataset[t_item]\n",
    "    print(len(dataset), sentence1, '\\t',sentence2, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0290920b-732c-4c07-908e-29060ffc94ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "模型加载完毕..\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# 加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "print(token)\n",
    "\n",
    "\n",
    "# 加载预训练模型\n",
    "pretrained = BertModel.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    "    force_download=False\n",
    ").to(device)\n",
    "\n",
    "print('模型加载完毕..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5eda51dd-5d3b-4910-b1f4-fe873c0edf62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[:2] for i in data]\n",
    "    labels = [i[2] for i in data]\n",
    "    \n",
    "    # print(\"sents: \", sents)\n",
    "\n",
    "    # 编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=200,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True,\n",
    "                                   add_special_tokens=True)\n",
    "\n",
    "    # input_ids: 编码之后的数字\n",
    "    # attention_mask: 是补零的位置是0,其他位置是1\n",
    "    # token_type_ids: 第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e679ef-c4fa-46f3-9414-e71871116081",
   "metadata": {},
   "source": [
    "## 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98b5b75d-772b-4c38-ac72-40b15d23365e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194\n",
      "[CLS] 虽 然 很 喜 欢 张 小 娴 的 文 字 ， 面 包 树 系 列 也 很 早 就 看 过 了 ， 但 是 还 是 买 了 她 所 有 自 己 喜 欢 的 书 回 来 珍 藏 。 面 包 树 系 列 故 事 讨 厌 女 主 角 的 个 性 也 不 喜 欢 结 尾 为 了 一 个 自 己 所 谓 的 完 美 爱 情 ， 放 弃 了 那 么 多 [SEP] 可 能 会 很 适 合 自 己 的 优 秀 男 人 ， 不 值 但 是 这 样 对 爱 情 的 执 着 也 许 会 打 动 很 多 人 吧 我 确 实 也 为 书 里 伤 感 的 情 节 而 落 过 泪 感 叹 唏 嘘 但 是 这 不 是 我 要 的 每 个 人 追 求 的 爱 情 不 一 样 ， 有 些 感 受 却 是 可 以 一 样 的 吧 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 200]),\n",
       " torch.Size([8, 200]),\n",
       " torch.Size([8, 200]),\n",
       " tensor([0, 1, 0, 0, 0, 1, 1, 0]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=8,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    print(len(loader))\n",
    "    print(token.decode(input_ids[0]))\n",
    "    break\n",
    "\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "647ad5de-9bfb-440b-9f46-8a27d160cd87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "446cfa9b-6444-4dd4-b774-c15979305ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8045ba75-ce34-4af7-8b27-a207c2a39b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "        out = out.softmax(dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f622b7d-cf74-4a6e-b94d-e3554267c417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "# 模型转移到GPU上\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7368d5e6-57d9-4c26-9484-49918e1500f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, idx: 0, loss: 0.382328599691391, acc: 0.9375, time:   0.0000\n",
      "epoch: 0, idx: 100, loss: 0.3338753581047058, acc: 1.0, time:  58.3937\n",
      "epoch: 0, idx: 200, loss: 0.4323582053184509, acc: 0.9375, time: 117.6528\n",
      "epoch: 0, idx: 300, loss: 0.401727557182312, acc: 0.875, time: 176.1585\n",
      "epoch: 0, idx: 400, loss: 0.5155487060546875, acc: 0.8125, time: 235.0740\n",
      "epoch: 0, idx: 500, loss: 0.39933276176452637, acc: 0.9375, time: 294.1627\n",
      "epoch: 1, idx: 0, loss: 0.4222555160522461, acc: 0.875, time: 350.9893\n",
      "epoch: 1, idx: 100, loss: 0.48862069845199585, acc: 0.8125, time: 409.2094\n",
      "epoch: 1, idx: 200, loss: 0.4204719066619873, acc: 0.875, time: 468.3088\n",
      "epoch: 1, idx: 300, loss: 0.5073793530464172, acc: 0.75, time: 527.0634\n",
      "epoch: 1, idx: 400, loss: 0.3615754246711731, acc: 0.9375, time: 585.7825\n",
      "epoch: 1, idx: 500, loss: 0.33340275287628174, acc: 1.0, time: 644.6685\n",
      "epoch: 2, idx: 0, loss: 0.5118041038513184, acc: 0.75, time: 702.2481\n",
      "epoch: 2, idx: 100, loss: 0.3177926242351532, acc: 1.0, time: 761.3043\n",
      "epoch: 2, idx: 200, loss: 0.4221370816230774, acc: 0.875, time: 820.6707\n",
      "epoch: 2, idx: 300, loss: 0.3722548186779022, acc: 0.9375, time: 879.6043\n",
      "epoch: 2, idx: 400, loss: 0.3615317642688751, acc: 0.9375, time: 939.3713\n",
      "epoch: 2, idx: 500, loss: 0.39452308416366577, acc: 0.875, time: 997.7779\n",
      "epoch: 3, idx: 0, loss: 0.3628849983215332, acc: 0.9375, time: 1055.9985\n",
      "epoch: 3, idx: 100, loss: 0.3925946056842804, acc: 0.9375, time: 1115.1974\n",
      "epoch: 3, idx: 200, loss: 0.42900100350379944, acc: 0.875, time: 1174.5679\n",
      "epoch: 3, idx: 300, loss: 0.3904726803302765, acc: 0.875, time: 1233.6492\n",
      "epoch: 3, idx: 400, loss: 0.43785297870635986, acc: 0.875, time: 1293.0845\n",
      "epoch: 3, idx: 500, loss: 0.337925523519516, acc: 1.0, time: 1352.1654\n",
      "epoch: 4, idx: 0, loss: 0.37306392192840576, acc: 0.9375, time: 1408.5854\n",
      "epoch: 4, idx: 100, loss: 0.4595072269439697, acc: 0.8125, time: 1467.2761\n",
      "epoch: 4, idx: 200, loss: 0.43567851185798645, acc: 0.875, time: 1526.5403\n",
      "epoch: 4, idx: 300, loss: 0.4150247573852539, acc: 0.875, time: 1585.3282\n",
      "epoch: 4, idx: 400, loss: 0.3205079436302185, acc: 1.0, time: 1645.0097\n",
      "epoch: 4, idx: 500, loss: 0.3742443323135376, acc: 0.9375, time: 1703.0965\n",
      "总计耗时:  2626318.042231798\n",
      "计算耗时时间段:  ['0.8011', '0.5791', '0.6064', '0.5599', '0.5176', '0.5393', '0.5608', '0.5809', '0.6197', '0.6065', '0.5801', '0.6439', '0.5791', '0.5811', '0.6635', '0.5856', '0.5849', '0.5170', '0.6455', '0.5354', '0.6250', '0.6181', '0.6210', '0.5419', '0.5779', '0.6468', '0.4971', '0.6056', '0.6672', '0.5609', '0.5036', '0.6070', '0.5407', '0.6663', '0.5820', '0.6864', '0.5799', '0.5639', '0.5387', '0.6013', '0.6221', '0.5391', '0.6618', '0.6016', '0.5798', '0.5336', '0.5613', '0.5818', '0.5608', '0.5616', '0.5200', '0.6458', '0.4349', '0.6222', '0.5611', '0.6026', '0.5388', '0.5630', '0.5576', '0.5812', '0.5549', '0.5604', '0.5351', '0.6868', '0.5831', '0.5160', '0.5363', '0.5813', '0.5876', '0.5393', '0.6316', '0.6452', '0.5817', '0.5623', '0.6658', '0.4736', '0.5567', '0.6002', '0.5578', '0.6213', '0.5585', '0.6194', '0.5798', '0.5166', '0.4959', '0.6229', '0.5892', '0.5844', '0.7034', '0.5587', '0.4932', '0.5578', '0.6200', '0.5800', '0.6605', '0.5777', '0.6036', '0.5801', '0.5434', '0.6226', '0.6496', '0.6416', '0.5402', '0.6049', '0.5370', '0.5815', '0.6013', '0.6025', '0.5601', '0.6656', '0.5658', '0.5402', '0.6256', '0.4950', '0.6689', '0.5572', '0.5575', '0.6384', '0.5819', '0.5807', '0.6429', '0.5558', '0.5589', '0.7241', '0.6034', '0.5387', '0.6028', '0.5572', '0.5552', '0.7024', '0.5782', '0.5597', '0.6192', '0.5621', '0.6025', '0.5150', '0.5400', '0.4722', '0.7030', '0.5363', '0.6015', '0.5366', '0.5828', '0.5790', '0.5399', '0.5622', '0.7036', '0.6654', '0.5570', '0.5785', '0.5594', '0.5397', '0.6630', '0.4334', '0.7690', '0.5792', '0.5594', '0.6024', '0.6572', '0.6022', '0.6448', '0.6447', '0.6626', '0.4578', '0.6435', '0.5975', '0.6222', '0.6007', '0.6232', '0.6856', '0.5578', '0.5381', '0.6204', '0.5996', '0.5387', '0.6801', '0.5800', '0.5584', '0.6008', '0.6029', '0.7236', '0.5168', '0.5572', '0.5594', '0.5398', '0.5362', '0.5800', '0.5843', '0.5792', '0.5797', '0.5961', '0.6224', '0.6035', '0.6438', '0.6236', '0.6189', '0.6201', '0.5371', '0.5807', '0.6005', '0.6639', '0.7048', '0.5175', '0.5139', '0.5371', '0.5980', '0.5384', '0.5798', '0.6020', '0.5383', '0.5192', '0.5801', '0.6368', '0.6226', '0.6233', '0.5795', '0.6054', '0.5338', '0.6855', '0.5757', '0.5614', '0.5363', '0.6459', '0.5560', '0.6838', '0.5596', '0.4922', '0.5378', '0.5782', '0.5790', '0.5594', '0.6438', '0.6822', '0.5367', '0.6062', '0.5408', '0.5630', '0.5398', '0.5204', '0.5803', '0.4932', '0.6052', '0.6197', '0.6843', '0.6431', '0.6440', '0.5595', '0.5159', '0.5200', '0.7008', '0.6220', '0.5585', '0.6218', '0.5827', '0.5162', '0.6441', '0.5785', '0.5584', '0.6419', '0.4956', '0.4951', '0.6211', '0.5590', '0.5578', '0.4946', '0.5122', '0.7043', '0.5828', '0.6017', '0.5360', '0.5376', '0.4963', '0.6004', '0.5579', '0.5365', '0.6232', '0.5571', '0.5797', '0.6002', '0.5788', '0.6238', '0.5770', '0.6402', '0.6221', '0.5172', '0.6007', '0.6195', '0.7669', '0.6013', '0.5171', '0.6217', '0.6631', '0.5376', '0.6584', '0.5814', '0.6038', '0.5580', '0.5597', '0.5575', '0.6829', '0.6614', '0.6269', '0.6247', '0.5799', '0.5382', '0.7679', '0.5380', '0.5573', '0.5752', '0.6026', '0.5788', '0.6845', '0.6024', '0.5586', '0.6242', '0.5350', '0.5372', '0.7005', '0.5841', '0.5602', '0.4538', '0.5806', '0.5834', '0.5382', '0.5561', '0.6016', '0.5557', '0.6653', '0.6013', '0.6226', '0.5386', '0.6018', '0.6003', '0.6591', '0.6236', '0.5838', '0.5804', '0.6051', '0.6632', '0.5822', '0.5594', '0.5581', '0.6222', '0.5598', '0.5807', '0.5392', '0.5614', '0.6142', '0.5176', '0.4105', '0.6208', '0.5355', '0.5376', '0.5590', '0.6465', '0.5832', '0.5805', '0.6875', '0.6241', '0.6028', '0.6045', '0.5781', '0.5388', '0.4948', '0.5594', '0.6140', '0.6286', '0.6227', '0.5988', '0.5999', '0.4730', '0.5829', '0.6191', '0.5362', '0.4356', '0.6423', '0.6865', '0.6425', '0.6621', '0.6413', '0.6879', '0.6269', '0.6184', '0.5847', '0.5795', '0.6216', '0.5383', '0.5141', '0.6014', '0.4707', '0.7118', '0.6402', '0.5624', '0.5364', '0.5784', '0.5795', '0.5153', '0.5845', '0.6639', '0.6034', '0.6223', '0.5814', '0.5614', '0.4943', '0.6049', '0.6220', '0.6390', '0.5177', '0.6432', '0.5997', '0.6612', '0.6684', '0.5412', '0.5815', '0.5396', '0.5614', '0.6230', '0.6458', '0.5612', '0.6817', '0.5361', '0.6006', '0.5829', '0.5604', '0.6053', '0.5142', '0.5374', '0.6607', '0.5626', '0.5828', '0.5805', '0.5401', '0.5658', '0.5371', '0.6050', '0.6384', '0.6247', '0.5793', '0.5837', '0.5998', '0.6829', '0.6238', '0.6454', '0.5361', '0.5177', '0.5557', '0.5608', '0.6183', '0.6856', '0.6026', '0.5805', '0.6021', '0.6204', '0.6636', '0.6222', '0.5580', '0.5167', '0.5993', '0.5619', '0.4956', '0.5433', '0.5352', '0.5633', '0.6800', '0.5765', '0.5813', '0.5974', '0.6252', '0.5374', '0.6026', '0.6651', '0.5997', '0.5622', '0.5813', '0.5790', '0.6194', '0.5591', '0.5367', '0.6000', '0.6048', '0.5789', '0.5015', '0.5795', '0.5843', '0.6885', '0.5826', '0.5631', '0.5581', '0.5624', '0.5982', '0.6863', '0.6456', '0.6041', '0.5369', '0.6219', '0.5393', '0.7267', '0.5565', '0.6472', '0.5800', '0.6421', '0.6664', '0.6204', '0.6462', '0.5375', '0.5605', '0.6014', '0.5399', '0.7262', '0.5140', '0.5412', '0.5774', '0.6007', '0.5995', '0.5850', '0.7250', '0.5767', '0.6032', '0.6221', '0.6441', '0.6016', '0.5579', '0.5814', '0.5370', '0.5830', '0.6435', '0.5178', '0.6176', '0.5596', '0.5184', '0.6002', '0.5586', '0.5982', '0.6017', '0.6641', '0.5814', '0.6022', '0.4927', '0.6256', '0.5611', '0.6026', '0.7036', '0.5599', '0.6009', '0.6226', '0.5179', '0.5140', '0.5819', '0.6231', '0.5130', '0.5765', '0.5572', '0.5202', '0.5569', '0.5829', '0.5580', '0.5384', '0.5571', '0.6031', '0.5977', '0.6395', '0.6018', '0.5775', '0.5818', '0.5767', '0.5168', '0.6245', '0.6166', '0.6027', '0.5349', '0.5167', '0.6236', '0.5381', '0.5978', '0.5836', '0.6845', '0.6407', '0.5805', '0.5539', '0.5192', '0.5799', '0.5605', '0.6442', '0.6018', '0.5429', '0.5785', '0.4953', '0.5756', '0.6025', '0.5772', '0.6410', '0.6007', '0.5362', '0.6021', '0.5984', '0.5394', '0.6185', '0.7466', '0.5603', '0.5981', '0.5784', '0.5158', '0.5357', '0.5601', '0.5360', '0.4719', '0.5783', '0.5397', '0.4920', '0.5617', '0.5158', '0.7098', '0.5197', '0.5826', '0.5812', '0.6003', '0.5806', '0.6606', '0.5584', '0.6008', '0.6683', '0.6022', '0.5770', '0.6382', '0.5592', '0.6227', '0.5768', '0.5174', '0.5583', '0.5619', '0.5810', '0.5639', '0.5377', '0.6611', '0.5394', '0.5359', '0.5367', '0.5344', '0.5584', '0.5794', '0.6214', '0.6189', '0.5351', '0.5579', '0.5559', '0.5376', '0.5568', '0.6035', '0.5145', '0.5166', '0.5364', '0.5365', '0.5164', '0.6217', '0.6810', '0.5583', '0.5818', '0.6845', '0.7058', '0.4571', '0.7226', '0.6261', '0.6245', '0.6600', '0.4554', '0.6364', '0.5366', '0.6834', '0.6222', '0.7236', '0.5998', '0.5366', '0.6193', '0.5989', '0.5796', '0.6581', '0.5784', '0.6202', '0.5580', '0.5563', '0.5394', '0.5576', '0.5146', '0.6392', '0.5567', '0.6220', '0.5127', '0.6636', '0.6013', '0.6596', '0.5591', '0.5976', '0.5784', '0.5572', '0.5590', '0.5997', '0.5142', '0.5779', '0.5815', '0.6191', '0.6195', '0.6213', '0.5351', '0.6686', '0.4937', '0.6158', '0.5582', '0.6185', '0.5163', '0.5777', '0.6638', '0.6022', '0.6589', '0.6676', '0.4981', '0.5386', '0.5582', '0.5784', '0.5810', '0.7050', '0.5162', '0.6430', '0.6221', '0.5610', '0.5780', '0.6417', '0.5369', '0.5164', '0.4504', '0.5384', '0.5589', '0.5837', '0.5571', '0.6042', '0.5578', '0.6263', '0.7509', '0.6664', '0.7332', '0.5814', '0.5589', '0.6031', '0.5815', '0.6262', '0.5827', '0.7010', '0.5635', '0.5569', '0.6213', '0.6057', '0.7234', '0.5790', '0.5792', '0.5371', '0.6671', '0.5360', '0.6688', '0.6018', '0.6021', '0.5582', '0.5802', '0.6027', '0.6378', '0.5583', '0.5379', '0.4956', '0.5369', '0.6007', '0.5391', '0.6222', '0.6446', '0.5566', '0.5771', '0.6201', '0.5804', '0.6216', '0.6448', '0.6427', '0.5563', '0.6435', '0.5362', '0.6220', '0.5185', '0.5162', '0.6041', '0.6184', '0.5165', '0.5983', '0.6013', '0.5986', '0.5791', '0.5196', '0.5984', '0.6218', '0.6234', '0.5585', '0.5368', '0.6476', '0.5381', '0.5772', '0.4763', '0.6392', '0.5402', '0.6180', '0.5393', '0.5594', '0.6005', '0.5792', '0.5132', '0.5386', '0.6416', '0.5368', '0.6207', '0.5797', '0.5552', '0.5811', '0.5604', '0.6235', '0.5404', '0.4931', '0.6854', '0.5781', '0.6229', '0.6235', '0.5804', '0.7042', '0.5364', '0.5413', '0.6294', '0.6032', '0.6704', '0.7050', '0.6208', '0.5633', '0.6414', '0.5366', '0.7648', '0.6461', '0.6445', '0.5846', '0.6878', '0.5579', '0.5373', '0.5571', '0.5823', '0.5786', '0.5802', '0.5780', '0.6423', '0.5621', '0.5549', '0.5597', '0.4923', '0.5814', '0.5591', '0.5370', '0.7023', '0.5779', '0.5649', '0.5171', '0.6484', '0.5837', '0.5874', '0.5604', '0.7030', '0.5379', '0.5809', '0.5844', '0.5794', '0.5615', '0.5367', '0.6217', '0.6420', '0.5564', '0.6231', '0.5785', '0.5563', '0.4979', '0.5794', '0.5588', '0.4769', '0.5832', '0.7017', '0.5814', '0.5162', '0.5804', '0.6009', '0.7498', '0.6225', '0.5836', '0.5370', '0.5366', '0.5783', '0.5152', '0.5578', '0.6048', '0.6584', '0.6619', '0.5374', '0.5141', '0.6028', '0.6015', '0.5810', '0.6799', '0.5799', '0.5601', '0.4915', '0.5573', '0.5972', '0.5375', '0.5973', '0.5363', '0.6800', '0.6009', '0.5157', '0.4927', '0.5384', '0.5135', '0.5378', '0.5362', '0.5799', '0.6198', '0.5801', '0.6817', '0.4943', '0.5148', '0.5797', '0.6235', '0.5977', '0.5597', '0.6578', '0.6639', '0.5797', '0.6422', '0.6595', '0.5392', '0.6459', '0.6016', '0.5135', '0.5595', '0.4940', '0.5393', '0.5975', '0.7246', '0.5593', '0.5815', '0.5582', '0.6415', '0.6030', '0.6369', '0.6436', '0.5583', '0.5594', '0.6236', '0.5378', '0.6619', '0.5581', '0.5401', '0.5793', '0.7086', '0.6828', '0.6233', '0.5408', '0.6038', '0.5801', '0.5583', '0.5783', '0.6615', '0.6211', '0.6240', '0.5572', '0.5384', '0.5582', '0.5790', '0.6404', '0.5128', '0.5791', '0.6017', '0.5541', '0.5377', '0.5362', '0.6019', '0.5561', '0.6415', '0.6434', '0.6032', '0.6287', '0.5782', '0.6671', '0.6224', '0.6218', '0.5202', '0.6024', '0.5994', '0.5341', '0.5369', '0.6413', '0.6222', '0.6452', '0.5555', '0.6235', '0.6576', '0.5158', '0.5816', '0.6211', '0.5813', '0.5775', '0.6026', '0.5564', '0.5996', '0.5584', '0.5802', '0.6226', '0.6847', '0.4740', '0.5593', '0.5378', '0.6621', '0.5155', '0.5140', '0.5811', '0.5384', '0.5783', '0.5576', '0.5559', '0.6055', '0.6615', '0.5359', '0.6238', '0.6619', '0.7510', '0.6222', '0.6406', '0.5798', '0.5559', '0.5593', '0.7007', '0.5583', '0.5829', '0.5758', '0.5382', '0.5972', '0.5593', '0.6632', '0.5590', '0.5402', '0.5987', '0.5192', '0.6422', '0.5593', '0.7016', '0.5826', '0.6038', '0.6412', '0.4786', '0.6187', '0.6638', '0.6015', '0.5852', '0.5827', '0.5982', '0.5883', '0.5366', '0.5173', '0.6603', '0.5791', '0.5840', '0.5550', '0.5801', '0.5593', '0.5618', '0.7037', '0.5800', '0.5609', '0.5994', '0.5845', '0.5575', '0.5830', '0.6381', '0.5843', '0.5621', '0.6201', '0.6232', '0.5585', '0.5788', '0.6639', '0.5150', '0.5617', '0.5818', '0.5393', '0.6044', '0.5994', '0.5183', '0.6200', '0.5571', '0.5798', '0.5800', '0.6652', '0.6254', '0.5815', '0.6622', '0.5592', '0.5995', '0.5188', '0.6655', '0.6022', '0.6652', '0.5584', '0.6053', '0.6410', '0.5604', '0.6655', '0.6082', '0.5841', '0.5597', '0.5626', '0.5381', '0.5624', '0.6220', '0.6872', '0.6215', '0.6022', '0.6436', '0.6021', '0.6205', '0.5369', '0.5561', '0.5197', '0.5560', '0.6443', '0.5151', '0.5589', '0.7083', '0.5339', '0.5398', '0.5774', '0.4948', '0.4719', '0.6454', '0.6420', '0.5859', '0.5387', '0.5845', '0.7046', '0.6004', '0.6703', '0.5779', '0.6863', '0.6031', '0.6191', '0.6263', '0.6609', '0.5152', '0.6481', '0.5136', '0.5848', '0.5800', '0.5818', '0.7054', '0.5974', '0.5609', '0.5759', '0.6061', '0.5816', '0.5846', '0.6405', '0.5779', '0.6446', '0.5132', '0.6272', '0.5767', '0.6845', '0.5459', '0.5139', '0.6222', '0.6848', '0.5367', '0.5626', '0.6396', '0.5612', '0.6232', '0.5798', '0.5168', '0.5591', '0.5160', '0.5173', '0.6018', '0.6407', '0.6050', '0.5588', '0.6024', '0.5801', '0.5596', '0.6043', '0.6215', '0.6419', '0.5159', '0.5796', '0.5829', '0.5214', '0.5356', '0.6470', '0.5798', '0.7044', '0.6457', '0.5577', '0.5215', '0.4962', '0.6027', '0.6064', '0.6807', '0.5798', '0.5566', '0.5798', '0.5995', '0.5686', '0.5171', '0.5795', '0.5371', '0.6398', '0.6413', '0.5152', '0.5595', '0.6270', '0.6600', '0.6003', '0.5786', '0.5818', '0.6019', '0.6208', '0.6876', '0.5380', '0.6039', '0.5359', '0.6032', '0.5577', '0.6045', '0.6427', '0.6045', '0.5658', '0.5831', '0.5442', '0.6267', '0.5406', '0.6030', '0.6606', '0.5834', '0.5569', '0.6449', '0.6031', '0.6176', '0.5860', '0.5786', '0.4954', '0.5398', '0.6233', '0.4945', '0.6034', '0.6279', '0.6824', '0.5152', '0.6200', '0.5571', '0.5796', '0.5995', '0.5618', '0.6382', '0.5177', '0.5354', '0.6426', '0.5382', '0.6239', '0.5393', '0.5367', '0.5834', '0.7033', '0.5381', '0.5813', '0.6467', '0.6232', '0.7042', '0.6214', '0.5155', '0.5151', '0.6235', '0.6201', '0.6626', '0.5582', '0.6024', '0.5391', '0.6230', '0.5834', '0.6584', '0.6243', '0.5348', '0.5380', '0.5130', '0.5829', '0.6003', '0.5780', '0.5588', '0.5570', '0.5613', '0.6172', '0.6005', '0.5779', '0.6011', '0.4511', '0.6239', '0.5610', '0.6206', '0.7078', '0.6665', '0.5604', '0.5582', '0.4968', '0.6848', '0.5979', '0.5586', '0.5777', '0.5607', '0.5587', '0.5353', '0.5622', '0.5558', '0.5629', '0.6820', '0.6030', '0.5563', '0.6236', '0.5581', '0.5988', '0.5983', '0.5586', '0.6221', '0.5391', '0.6682', '0.5860', '0.5568', '0.6851', '0.6001', '0.6054', '0.5660', '0.6022', '0.7041', '0.5612', '0.5616', '0.5173', '0.5809', '0.6242', '0.5416', '0.5386', '0.6018', '0.6836', '0.5150', '0.5387', '0.6459', '0.6013', '0.6615', '0.6021', '0.5797', '0.6259', '0.6450', '0.6684', '0.5613', '0.6716', '0.5817', '0.6279', '0.5443', '0.6611', '0.6008', '0.6459', '0.5358', '0.6255', '0.5357', '0.5796', '0.6387', '0.6456', '0.5820', '0.5378', '0.5580', '0.5594', '0.6832', '0.6205', '0.5808', '0.6462', '0.4960', '0.6437', '0.7227', '0.5812', '0.5592', '0.6462', '0.6644', '0.5999', '0.6426', '0.5384', '0.6416', '0.5833', '0.5786', '0.7298', '0.5805', '0.5591', '0.4961', '0.5154', '0.6017', '0.5812', '0.5382', '0.6862', '0.5166', '0.6698', '0.5972', '0.5204', '0.6209', '0.6423', '0.5582', '0.5781', '0.5608', '0.6010', '0.5191', '0.5374', '0.5848', '0.4946', '0.6059', '0.5342', '0.6221', '0.5397', '0.5606', '0.6042', '0.5803', '0.5421', '0.5810', '0.5413', '0.6203', '0.6869', '0.6672', '0.5773', '0.6044', '0.7060', '0.5390', '0.5388', '0.5801', '0.5376', '0.5763', '0.6081', '0.6599', '0.5807', '0.6443', '0.6439', '0.5819', '0.6648', '0.6424', '0.5399', '0.5373', '0.6215', '0.6227', '0.6402', '0.5853', '0.5395', '0.6005', '0.5982', '0.5620', '0.4717', '0.6026', '0.5376', '0.6822', '0.6057', '0.5624', '0.6042', '0.5579', '0.5583', '0.5788', '0.5629', '0.6196', '0.6028', '0.5831', '0.5351', '0.5658', '0.5181', '0.6472', '0.7272', '0.6034', '0.6021', '0.6630', '0.6655', '0.5614', '0.5805', '0.6010', '0.4947', '0.6062', '0.5823', '0.5658', '0.5418', '0.6274', '0.6232', '0.7090', '0.5818', '0.6235', '0.7251', '0.5387', '0.5360', '0.6037', '0.6006', '0.7510', '0.6026', '0.6218', '0.5636', '0.5767', '0.5371', '0.4744', '0.5830', '0.6445', '0.5807', '0.5393', '0.5369', '0.5386', '0.5573', '0.5597', '0.6227', '0.5590', '0.6833', '0.5371', '0.5379', '0.5795', '0.5586', '0.6424', '0.5178', '0.5988', '0.5568', '0.6621', '0.5994', '0.6047', '0.6658', '0.5404', '0.7284', '0.5574', '0.6032', '0.5342', '0.6037', '0.6855', '0.6441', '0.5196', '0.5983', '0.5602', '0.5602', '0.6047', '0.5161', '0.6658', '0.5591', '0.5142', '0.5172', '0.5984', '0.6040', '0.5556', '0.6055', '0.5604', '0.6168', '0.5810', '0.6017', '0.5819', '0.5597', '0.6025', '0.5873', '0.6856', '0.6257', '0.5807', '0.6238', '0.5792', '0.7050', '0.5617', '0.5996', '0.6022', '0.4934', '0.6038', '0.6852', '0.6659', '0.6230', '0.6427', '0.6213', '0.6224', '0.5574', '0.6658', '0.7468', '0.6668', '0.7276', '0.4976', '0.5573', '0.6392', '0.6242', '0.5813', '0.5568', '0.6007', '0.5620', '0.5393', '0.6445', '0.6397', '0.5610', '0.6242', '0.5425', '0.5612', '0.4969', '0.5593', '0.6049', '0.6631', '0.4933', '0.5600', '0.6023', '0.5800', '0.5555', '0.6229', '0.5407', '0.5134', '0.5651', '0.6367', '0.6262', '0.5811', '0.5625', '0.5853', '0.4994', '0.5839', '0.5836', '0.5383', '0.6374', '0.5605', '0.6230', '0.5820', '0.5169', '0.6420', '0.6452', '0.5582', '0.4963', '0.5789', '0.5795', '0.5575', '0.5802', '0.6438', '0.5350', '0.6856', '0.5404', '0.5829', '0.5801', '0.5611', '0.6240', '0.5359', '0.6647', '0.5788', '0.5395', '0.6228', '0.5804', '0.6245', '0.5569', '0.6618', '0.5791', '0.6049', '0.5619', '0.5365', '0.5805', '0.6626', '0.6061', '0.6459', '0.6225', '0.6221', '0.6163', '0.5603', '0.6000', '0.5159', '0.5575', '0.5183', '0.5381', '0.5782', '0.5632', '0.5174', '0.5802', '0.6630', '0.6240', '0.5619', '0.6448', '0.5378', '0.5384', '0.5407', '0.6215', '0.6625', '0.5784', '0.5606', '0.6031', '0.6006', '0.6448', '0.5773', '0.5815', '0.5366', '0.6023', '0.6253', '0.5994', '0.5582', '0.6179', '0.7046', '0.5816', '0.6623', '0.5799', '0.6808', '0.5171', '0.5846', '0.6415', '0.6024', '0.6436', '0.6042', '0.6035', '0.5581', '0.5810', '0.6446', '0.5157', '0.6789', '0.5802', '0.5825', '0.5786', '0.6440', '0.4536', '0.5594', '0.7245', '0.5825', '0.5390', '0.6010', '0.5841', '0.5786', '0.7300', '0.5819', '0.5361', '0.5808', '0.5993', '0.5424', '0.5980', '0.6237', '0.6430', '0.5376', '0.6028', '0.5381', '0.5846', '0.6447', '0.6838', '0.6456', '0.5388', '0.6697', '0.6464', '0.6057', '0.5870', '0.5841', '0.5183', '0.5403', '0.5834', '0.5621', '0.6024', '0.5768', '0.6262', '0.5631', '0.6214', '0.6485', '0.6841', '0.6001', '0.5588', '0.6427', '0.6697', '0.6434', '0.6414', '0.5799', '0.5398', '0.6050', '0.6609', '0.5597', '0.6243', '0.5789', '0.6238', '0.5794', '0.5395', '0.6607', '0.6002', '0.6025', '0.5364', '0.5806', '0.5349', '0.5194', '0.5763', '0.6690', '0.7330', '0.6003', '0.6067', '0.6389', '0.5798', '0.6039', '0.6464', '0.6033', '0.5377', '0.5207', '0.6585', '0.5577', '0.5600', '0.5144', '0.5375', '0.5142', '0.5413', '0.4954', '0.5830', '0.6839', '0.5401', '0.7056', '0.5593', '0.5378', '0.6003', '0.4965', '0.6416', '0.5411', '0.5369', '0.7065', '0.6456', '0.6221', '0.6494', '0.5582', '0.6852', '0.5610', '0.5813', '0.5402', '0.6215', '0.5615', '0.6845', '0.4327', '0.6835', '0.5367', '0.6059', '0.5829', '0.6438', '0.7258', '0.5570', '0.6039', '0.5570', '0.6001', '0.6862', '0.6001', '0.5804', '0.5580', '0.6215', '0.5989', '0.6831', '0.6868', '0.6013', '0.5599', '0.5575', '0.6619', '0.5384', '0.5400', '0.5631', '0.5815', '0.5595', '0.5806', '0.5171', '0.6454', '0.5161', '0.6883', '0.5840', '0.5613', '0.5369', '0.5849', '0.5808', '0.6062', '0.7047', '0.6200', '0.6034', '0.5556', '0.5852', '0.5813', '0.6638', '0.5839', '0.5601', '0.5226', '0.5603', '0.5667', '0.6287', '0.6656', '0.6030', '0.5786', '0.6234', '0.5795', '0.5643', '0.6858', '0.6207', '0.5393', '0.5769', '0.5841', '0.6046', '0.6401', '0.5431', '0.5574', '0.6269', '0.5383', '0.6442', '0.5828', '0.7276', '0.5384', '0.5797', '0.5197', '0.5789', '0.6215', '0.5847', '0.5783', '0.5197', '0.6383', '0.6227', '0.4945', '0.6228', '0.5397', '0.5373', '0.6051', '0.6828', '0.6029', '0.6237', '0.6434', '0.4963', '0.5560', '0.5192', '0.5769', '0.6431', '0.6264', '0.4962', '0.5614', '0.6228', '0.6067', '0.5591', '0.5627', '0.6814', '0.6231', '0.6036', '0.5652', '0.5834', '0.6193', '0.5382', '0.6227', '0.5189', '0.6438', '0.6014', '0.6023', '0.5202', '0.6408', '0.6420', '0.6033', '0.6232', '0.5377', '0.6631', '0.5813', '0.6051', '0.5581', '0.6041', '0.5579', '0.6026', '0.5614', '0.6638', '0.5842', '0.5598', '0.5172', '0.6644', '0.5840', '0.6815', '0.6437', '0.5623', '0.6246', '0.5596', '0.6625', '0.5573', '0.5403', '0.5573', '0.6053', '0.5819', '0.5788', '0.6842', '0.4323', '0.7077', '0.6524', '0.5193', '0.6488', '0.4943', '0.5630', '0.6176', '0.5611', '0.5798', '0.5796', '0.5823', '0.7048', '0.5410', '0.7259', '0.6001', '0.5638', '0.5556', '0.6262', '0.6597', '0.6231', '0.6656', '0.6267', '0.6042', '0.6191', '0.6077', '0.5847', '0.6021', '0.5435', '0.6009', '0.6669', '0.5812', '0.6018', '0.6033', '0.5800', '0.5843', '0.5572', '0.5406', '0.6180', '0.5602', '0.6028', '0.5166', '0.6263', '0.5998', '0.4373', '0.5576', '0.6251', '0.7025', '0.5366', '0.6031', '0.5583', '0.6045', '0.5796', '0.7073', '0.5825', '0.6012', '0.6265', '0.5373', '0.6062', '0.7072', '0.6448', '0.5215', '0.5788', '0.6046', '0.6629', '0.5155', '0.5616', '0.4723', '0.5804', '0.6646', '0.6250', '0.5153', '0.5239', '0.6842', '0.5391', '0.6014', '0.5992', '0.5182', '0.5817', '0.6248', '0.6645', '0.5790', '0.5417', '0.6417', '0.5832', '0.5388', '0.5585', '0.5606', '0.6871', '0.5409', '0.4930', '0.5393', '0.6205', '0.5618', '0.6212', '0.5605', '0.6682', '0.6008', '0.4964', '0.5980', '0.5618', '0.5819', '0.5817', '0.5569', '0.7070', '0.5184', '0.5600', '0.6281', '0.6440', '0.7088', '0.5810', '0.6416', '0.5366', '0.6224', '0.5200', '0.5996', '0.5394', '0.6664', '0.5813', '0.6016', '0.6012', '0.7329', '0.7265', '0.4941', '0.6035', '0.5630', '0.5873', '0.5838', '0.5171', '0.5441', '0.4728', '0.7274', '0.5162', '0.6044', '0.5172', '0.5809', '0.5388', '0.5812', '0.5348', '0.7042', '0.6261', '0.5809', '0.5820', '0.5400', '0.6019', '0.5624', '0.6821', '0.5192', '0.6239', '0.6717', '0.6743', '0.5806', '0.6035', '0.5374', '0.6045', '0.6445', '0.6832', '0.5623', '0.6011', '0.6055', '0.5813', '0.5401', '0.4945', '0.6032', '0.6847', '0.5794', '0.5397', '0.6000', '0.5402', '0.5823', '0.5804', '0.5392', '0.5364', '0.6676', '0.5574', '0.6265', '0.5803', '0.5809', '0.6047', '0.7019', '0.6031', '0.5802', '0.5408', '0.5352', '0.6247', '0.5606', '0.6816', '0.5810', '0.5160', '0.6026', '0.6275', '0.5365', '0.7478', '0.5372', '0.6245', '0.5364', '0.4988', '0.5826', '0.5381', '0.6243', '0.6804', '0.5806', '0.6858', '0.6229', '0.6619', '0.5583', '0.5598', '0.4962', '0.6461', '0.5578', '0.6897', '0.4952', '0.5567', '0.6628', '0.6834', '0.5844', '0.6242', '0.5361', '0.6668', '0.6228', '0.6459', '0.6881', '0.5371', '0.6241', '0.5586', '0.6035', '0.5613', '0.6247', '0.5401', '0.5800', '0.5629', '0.6379', '0.5604', '0.5397', '0.5382', '0.5815', '0.6006', '0.4972', '0.5800', '0.5828', '0.6401', '0.5798', '0.5833', '0.5597', '0.6468', '0.5382', '0.5843', '0.5769', '0.6209', '0.6445', '0.6009', '0.5827', '0.5819', '0.6449', '0.5993', '0.5596', '0.5798', '0.4950', '0.5605', '0.6220', '0.5843', '0.6427', '0.6011', '0.5803', '0.5792', '0.6046', '0.4937', '0.6053', '0.5368', '0.6251', '0.5771', '0.5405', '0.5800', '0.6660', '0.5842', '0.4974', '0.6304', '0.6664', '0.5630', '0.5391', '0.5808', '0.5838', '0.5348', '0.5624', '0.6230', '0.6228', '0.5599', '0.5593', '0.5575', '0.6236', '0.6436', '0.6670', '0.6676', '0.5816', '0.6434', '0.6625', '0.6030', '0.5400', '0.5599', '0.6037', '0.5588', '0.6647', '0.6195', '0.5805', '0.6489', '0.5593', '0.5176', '0.5810', '0.5814', '0.6665', '0.6450', '0.6016', '0.5612', '0.5830', '0.6006', '0.7047', '0.6446', '0.5808', '0.5837', '0.6834', '0.4375', '0.6624', '0.5599', '0.5157', '0.6026', '0.6013', '0.6056', '0.5552', '0.5853', '0.6412', '0.4962', '0.5190', '0.5783', '0.5406', '0.5791', '0.6018', '0.5989', '0.4787', '0.5614', '0.6243', '0.6011', '0.5563', '0.6020', '0.6204', '0.6037', '0.7483', '0.5576', '0.6043', '0.4975', '0.5596', '0.6448', '0.5385', '0.6876', '0.6207', '0.6026', '0.5388', '0.6036', '0.5371', '0.5833', '0.6202', '0.6002', '0.5601', '0.5145', '0.5832', '0.5996', '0.6488', '0.6007', '0.4532', '0.6626', '0.5150', '0.6049', '0.5581', '0.6264', '0.6196', '0.6192', '0.4337', '0.6211', '0.6461', '0.6483', '0.6039', '0.6676', '0.5355', '0.5412', '0.5781', '0.5161', '0.5156', '0.5835', '0.5774', '0.6049', '0.5569', '0.5413', '0.5355', '0.6423', '0.5820', '0.4524', '0.6032', '0.6216', '0.6248', '0.5994', '0.6198', '0.6069', '0.5205', '0.5007', '0.6038', '0.6280', '0.5826', '0.5857', '0.6866', '0.5781', '0.5380', '0.5366', '0.5617', '0.5611', '0.5168', '0.6025', '0.5176', '0.6006', '0.6386', '0.6021', '0.6009', '0.6433', '0.6018', '0.5571', '0.5571', '0.6367', '0.5990', '0.5803', '0.5993', '0.5597', '0.5980', '0.6205', '0.5550', '0.5580', '0.5554', '0.6642', '0.6211', '0.6807', '0.5146', '0.5777', '0.5564', '0.5582', '0.6441', '0.6000', '0.6618', '0.5784', '0.5578', '0.5569', '0.6005', '0.5991', '0.5817', '0.6571', '0.5364', '0.6208', '0.6424', '0.5811', '0.5578', '0.6426', '0.5985', '0.4963', '0.6201', '0.6238', '0.5574', '0.5826', '0.6166', '0.5340', '0.4965', '0.6171', '0.5801', '0.5767', '0.4945', '0.5150', '0.6420', '0.6420', '0.5775', '0.6019', '0.5748', '0.6662', '0.5366', '0.5368', '0.7274', '0.5556', '0.5799', '0.5351', '0.5802', '0.6408', '0.6608', '0.5793', '0.5540', '0.5599', '0.6205', '0.5565', '0.5372', '0.5565', '0.5959', '0.4565', '0.5375', '0.6150', '0.6210', '0.5755', '0.6010', '0.7643', '0.5776', '0.5153', '0.6021', '0.5603', '0.5988', '0.5372', '0.5814', '0.6379', '0.5165', '0.5135', '0.6220', '0.6007', '0.6215', '0.5570', '0.6615', '0.4110', '0.6633', '0.6435', '0.6004', '0.5773', '0.5984', '0.5791', '0.6208', '0.6601', '0.5162', '0.5359', '0.6224', '0.5988', '0.5606', '0.6230', '0.5575', '0.6705', '0.5562', '0.6464', '0.5346', '0.5374', '0.5561', '0.6192', '0.5790', '0.6616', '0.6440', '0.5813', '0.6804', '0.5596', '0.4933', '0.5811', '0.6197', '0.5842', '0.5792', '0.6431', '0.5600', '0.5568', '0.6208', '0.6206', '0.5577', '0.6190', '0.5342', '0.5776', '0.5776', '0.5177', '0.6433', '0.6028', '0.6035', '0.6798', '0.6028', '0.5985', '0.4969', '0.6205', '0.5850', '0.5390', '0.6815', '0.6453', '0.6194', '0.6842', '0.5597', '0.5558', '0.6654', '0.5558', '0.5797', '0.6005', '0.6388', '0.6636', '0.5354', '0.5363', '0.6602', '0.7231', '0.6212', '0.6015', '0.5782', '0.5586', '0.5771', '0.6210', '0.5339', '0.5560', '0.6173', '0.6182', '0.6012', '0.6413', '0.5808', '0.5566', '0.5157', '0.5980', '0.5998', '0.5612', '0.5584', '0.7041', '0.5584', '0.5838', '0.5980', '0.5612', '0.5554', '0.4521', '0.5791', '0.5376', '0.5354', '0.7056', '0.5579', '0.5974', '0.5586', '0.5348', '0.5359', '0.5580', '0.6047', '0.6005', '0.6796', '0.6006', '0.5757', '0.5987', '0.5334', '0.5780', '0.6623', '0.5375', '0.6427', '0.5539', '0.5800', '0.5328', '0.6256', '0.6612', '0.5549', '0.5587', '0.5568', '0.5382', '0.6182', '0.6032', '0.6375', '0.5979', '0.4948', '0.6203', '0.5585', '0.6190', '0.5174', '0.5778', '0.7050', '0.5822', '0.5610', '0.5830', '0.6239', '0.5176', '0.5394', '0.6030', '0.6857', '0.5969', '0.5587', '0.5954', '0.6433', '0.6387', '0.5548', '0.5786', '0.5759', '0.5416', '0.6195', '0.5382', '0.7021', '0.6201', '0.5609', '0.6173', '0.7053', '0.5584', '0.5992', '0.5378', '0.5980', '0.5152', '0.6022', '0.5604', '0.5762', '0.6613', '0.5607', '0.6022', '0.6268', '0.5551', '0.5600', '0.6607', '0.5123', '0.5606', '0.4986', '0.5577', '0.5585', '0.5367', '0.5148', '0.5800', '0.5792', '0.5782', '0.5756', '0.6808', '0.6017', '0.5768', '0.5800', '0.6211', '0.6398', '0.6222', '0.5989', '0.6066', '0.6232', '0.5387', '0.5339', '0.6833', '0.6016', '0.6182', '0.5824', '0.5845', '0.6846', '0.5596', '0.6224', '0.5613', '0.6220', '0.5408', '0.6049', '0.6817', '0.6236', '0.5151', '0.5849', '0.5794', '0.5174', '0.5370', '0.7284', '0.6060', '0.5367', '0.6027', '0.5609', '0.5195', '0.6427', '0.6642', '0.5595', '0.5572', '0.5828', '0.6225', '0.5613', '0.5813', '0.5603', '0.6205', '0.5993', '0.6036', '0.6637', '0.5854', '0.7074', '0.5874', '0.5433', '0.6428', '0.5827', '0.6848', '0.5159', '0.5800', '0.5987', '0.5620', '0.5359', '0.6023', '0.5799', '0.7118', '0.5824', '0.4925', '0.5399', '0.5616', '0.6664', '0.7096', '0.6212', '0.6474', '0.6875', '0.6211', '0.6293', '0.5619', '0.5860', '0.5845', '0.5606', '0.6076', '0.5601', '0.6453', '0.5365', '0.6477', '0.5795', '0.5827', '0.5373', '0.7075', '0.5829', '0.6638', '0.5863', '0.5384', '0.5389', '0.6045', '0.5559', '0.6471', '0.5365', '0.6055', '0.5801', '0.6053', '0.5803', '0.6620', '0.6478', '0.6254', '0.6031', '0.5380', '0.4965', '0.6680', '0.6203', '0.5827', '0.6641', '0.5156', '0.5633', '0.7036', '0.5383', '0.6223', '0.4942', '0.5997', '0.5786', '0.5141', '0.5761', '0.5775', '0.6589', '0.5821', '0.5121', '0.6001', '0.5790', '0.5113', '0.6003', '0.5562', '0.5863', '0.6595', '0.5577', '0.5994', '0.5809', '0.5802', '0.6033', '0.4952', '0.7861', '0.5358', '0.5143', '0.5782', '0.6431', '0.5550', '0.6178', '0.6213', '0.6220', '0.5809', '0.6181', '0.5365', '0.6361', '0.5384', '0.5165', '0.6423', '0.5566', '0.4973', '0.5779', '0.5979', '0.5370', '0.5971', '0.6615', '0.5374', '0.6018', '0.5342', '0.5365', '0.5766', '0.5599', '0.5773', '0.6384', '0.5172', '0.5332', '0.5816', '0.5980', '0.4987', '0.6005', '0.6434', '0.6645', '0.5379', '0.6668', '0.5776', '0.6033', '0.5176', '0.5378', '0.5379', '0.6396', '0.5585', '0.5364', '0.6468', '0.5133', '0.5804', '0.5795', '0.6026', '0.6196', '0.6195', '0.6018', '0.6864', '0.5361', '0.4941', '0.6841', '0.6443', '0.5378', '0.5808', '0.5403', '0.6223', '0.5203', '0.5369', '0.7494', '0.5775', '0.5589', '0.5602', '0.4937', '0.5595', '0.4939', '0.6028', '0.6006', '0.7263', '0.6641', '0.6195', '0.5804', '0.6397', '0.5569', '0.5365', '0.6649', '0.5357', '0.5789', '0.5141', '0.7233', '0.5160', '0.5756', '0.5768', '0.5537', '0.5585', '0.4724', '0.5778', '0.6030', '0.5334', '0.5573', '0.6390', '0.6000', '0.5797', '0.5574', '0.5353', '0.6207', '0.5785', '0.6600', '0.6058', '0.6186', '0.5388', '0.5361', '0.7037', '0.5790', '0.5575', '0.6644', '0.4908', '0.6200', '0.6446', '0.6785', '0.5375', '0.5991', '0.5177', '0.6197', '0.6185', '0.5623', '0.5936', '0.6012', '0.6406', '0.5599', '0.6190', '0.6391', '0.6005', '0.5785', '0.5997', '0.5780', '0.5776', '0.6857', '0.6218', '0.5797', '0.5343', '0.5600', '0.6207', '0.5354', '0.5586', '0.6568', '0.5397', '0.6404', '0.4944', '0.5768', '0.5166', '0.5565', '0.5172', '0.5578', '0.5769', '0.7496', '0.6005', '0.5823', '0.6214', '0.5125', '0.6394', '0.6447', '0.5382', '0.6417', '0.6633', '0.6190', '0.5811', '0.6224', '0.5774', '0.6456', '0.6605', '0.5791', '0.5828']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "# 训练\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "num_epochs = 5 \n",
    "\n",
    "timer = d2l.Timer()\n",
    "timer.stop()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                         batch_size=16,\n",
    "                                         collate_fn=collate_fn,\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True)\n",
    "    \n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            # labels = labels  #.cpu()\n",
    "            out = out.argmax(dim=1)  #.cpu()\n",
    "            res_acc = (out == labels).sum().item() # .cpu()\n",
    "            accuracy = res_acc / len(labels.cpu())\n",
    "            print('epoch: %s, idx: %s, loss: %s, acc: %s, time: %s' % \\\n",
    "                  (epoch, i, loss.item(), accuracy, timer.final_time()))\n",
    "        \n",
    "        #if i % 50 == 0:\n",
    "        #    #print('epoch: %s, idx: %s, time: %s' % (epoch, i, timer.final_time()))\n",
    "        \n",
    "        timer.stop()\n",
    "    pass\n",
    "                 \n",
    "    \n",
    "# 训练结束\n",
    "print('总计耗时: ', timer.sum())\n",
    "print('计算耗时时间段: ', timer.interval_consume())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfb94aa6-23a9-42ab-ace4-72653ff62f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "    \n",
    "    print('测试数据长度: ', len(loader_test))\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):\n",
    "        \n",
    "        #if i == 5:\n",
    "        #    break\n",
    "            \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "            pass\n",
    "        pred = out.argmax(dim=1)\n",
    "        \n",
    "        # 转移到cpu上\n",
    "        pred = pred.cpu()\n",
    "        labels = labels.cpu()\n",
    "        \n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += len(labels)\n",
    "        \n",
    "        if i // 200 == 0:\n",
    "            print(i, 'acc: ', correct / total)\n",
    "            pass\n",
    "\n",
    "    # print('epoch: %s acc: %s' % ( correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "879c8a83-3cce-4510-88ce-cff1202cefa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据长度:  37\n",
      "0 acc:  0.96875\n",
      "1 acc:  0.953125\n",
      "2 acc:  0.9270833333333334\n",
      "3 acc:  0.9296875\n",
      "4 acc:  0.925\n",
      "5 acc:  0.921875\n",
      "6 acc:  0.9241071428571429\n",
      "7 acc:  0.9296875\n",
      "8 acc:  0.9201388888888888\n",
      "9 acc:  0.925\n",
      "10 acc:  0.9147727272727273\n",
      "11 acc:  0.921875\n",
      "12 acc:  0.9206730769230769\n",
      "13 acc:  0.921875\n",
      "14 acc:  0.9229166666666667\n",
      "15 acc:  0.923828125\n",
      "16 acc:  0.9227941176470589\n",
      "17 acc:  0.9253472222222222\n",
      "18 acc:  0.9259868421052632\n",
      "19 acc:  0.9265625\n",
      "20 acc:  0.9285714285714286\n",
      "21 acc:  0.9289772727272727\n",
      "22 acc:  0.9266304347826086\n",
      "23 acc:  0.9270833333333334\n",
      "24 acc:  0.9275\n",
      "25 acc:  0.9290865384615384\n",
      "26 acc:  0.9293981481481481\n",
      "27 acc:  0.9308035714285714\n",
      "28 acc:  0.9310344827586207\n",
      "29 acc:  0.9302083333333333\n",
      "30 acc:  0.9324596774193549\n",
      "31 acc:  0.931640625\n",
      "32 acc:  0.9299242424242424\n",
      "33 acc:  0.9292279411764706\n",
      "34 acc:  0.9303571428571429\n",
      "35 acc:  0.9296875\n",
      "36 acc:  0.9307432432432432\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 42\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc: \u001b[39m\u001b[38;5;124m'\u001b[39m, correct \u001b[38;5;241m/\u001b[39m total)\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m acc: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed2ba30f-56f4-432f-bf72-2f6556f1df01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "\n",
    "# 保存\n",
    "\n",
    "model_save_path = 'chinese_infer_mission_2023_4_20_v4.pt'\n",
    "torch.save(model.state_dict(),  model_save_path)  # 推荐的文件后缀名是pt或pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc6105-a217-4444-99b1-7d7fdb5165a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
