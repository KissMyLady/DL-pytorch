{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d02d7b-061a-4a97-ab11-0091930f86b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "import random, math\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71f014a-fd28-4f0c-b70e-a138b6f83ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/train/cache-2c893bcab2dc48fd.arrow\n"
     ]
    }
   ],
   "source": [
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, split):\n",
    "        # dataset = load_dataset(path='lansinuote/ChnSentiCorp', split=split)\n",
    "        dataset = load_from_disk('./data/ChnSentiCorp')['%s' % split]\n",
    "        \n",
    "        def f(data):\n",
    "            return len(data['text']) > 20\n",
    "        self.dataset = dataset.filter(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset['text'][i]\n",
    "        \n",
    "        # print('原始数据: ', text)\n",
    "        # label = self.dataset['label'][i]\n",
    "        #切分一句话为前半句和后半句\n",
    "        s_len = len(text)\n",
    "        half_len = math.floor(s_len / 2)  # 切分句子为两段\n",
    "        \n",
    "        sentence1 = text[:half_len]\n",
    "        sentence2 = text[half_len: ]\n",
    "        label = 0\n",
    "\n",
    "        # 有一半的概率把后半句替换为一句无关的话\n",
    "        if random.randint(0, 1) == 0:\n",
    "            j = random.randint(0, len(self.dataset) - 1)\n",
    "            # sentence2 = self.dataset[j]['text'][20:40]\n",
    "            sentence2 = self.dataset['text'][j][half_len:]\n",
    "            label = 1\n",
    "\n",
    "        return sentence1, sentence2, label\n",
    "\n",
    "# 加载\n",
    "dataset = Dataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e85d5d72-f53d-4761-a6f6-f11bdca2f3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般', '', 1)\n",
      "\n",
      "('15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜', '夹杂着人物之间的责任，关心和感情纠葛。一层又一层，惊险刺激。女主角谢怀珉的医术，的确相当专业。看来在现代，学的中医派上用场，成为她的铺垫。不过，书的最后，简直太意外了。都不希望是真的。书中的慧空说，她可以母仪天下。我就猜想，如果她爱的人士个君王，那就很有可能了。希望真是如此呢。', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(dataset[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a66e21-53b3-456a-ad48-7475eecf9aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9552, '今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当', '动程序、使用说明放在了E盘中。电池有延保卡。', 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试拼接的数据\n",
    "\n",
    "sentence1, sentence2, label = dataset[3]\n",
    "\n",
    "len(dataset), sentence1, sentence2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0290920b-732c-4c07-908e-29060ffc94ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# 加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "print(token)\n",
    "\n",
    "\n",
    "# 加载预训练模型\n",
    "pretrained = BertModel.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    "    force_download=False\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eda51dd-5d3b-4910-b1f4-fe873c0edf62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[:2] for i in data]  # 句子一, 句子二\n",
    "    labels = [i[2] for i in data]\n",
    "\n",
    "    # print('sents: ', sents)\n",
    "    \n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=200,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True,\n",
    "                                   add_special_tokens=True)\n",
    "\n",
    "    # input_ids: 编码之后的数字\n",
    "    # attention_mask: 是补零的位置是0,其他位置是1\n",
    "    # token_type_ids: 第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e679ef-c4fa-46f3-9414-e71871116081",
   "metadata": {},
   "source": [
    "## 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b5b75d-772b-4c38-ac72-40b15d23365e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194\n",
      "[CLS] 以 前 去 过 大 连 ， 今 年 选 择 住 在 博 览 ， 总 体 感 觉 还 是 挺 好 的 ， 就 是 住 在 这 里 的 日 本 人 挺 多 的 ， 最 讨 厌 小 日 本 了 。 我 个 人 习 惯 住 酒 店 最 在 意 的 是 卫 生 ， 尤 其 卫 生 间 的 卫 生 最 重 要 ， 这 里 的 卫 生 条 件 还 是 让 人 满 意 的 ， 服 务 也 不 像 说 的 那 么 不 好 ， 服 务 人 员 态 度 都 很 热 情 友 好 ， [SEP] 停 水 等 待 它 慢 慢 流 走 ， 如 果 一 味 痛 快 洗 澡 就 意 味 着 脚 脖 子 被 泡 沫 淹 掉 。 赶 紧 改 善 吧 ～ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=8,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    print(len(loader))\n",
    "    print(token.decode(input_ids[0]))\n",
    "    input_ids.shape, attention_mask.shape, token_type_ids.shape, labels\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446cfa9b-6444-4dd4-b774-c15979305ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8045ba75-ce34-4af7-8b27-a207c2a39b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "            pass\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "        out = out.softmax(dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f622b7d-cf74-4a6e-b94d-e3554267c417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "# model.load_state_dict(torch.load('chinese_infer_mission_2023_4_10.pt'))\n",
    "# chinese_infer_mission_2023_4_20_v1\n",
    "model.load_state_dict(torch.load('chinese_infer_mission_2023_4_20_v4.pt'))\n",
    "\n",
    "# 加载模型\n",
    "\n",
    "# 模型转移到GPU上\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa292177-36c6-446e-a65a-bccec623f8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 句子测试封装\n",
    "\n",
    "\n",
    "def str_felling_detect(model, str_sents):\n",
    "    \n",
    "    out = token.encode_plus(str_sents,\n",
    "                            truncation=True,\n",
    "                            padding='max_length',\n",
    "                            max_length=45,\n",
    "                            return_tensors='pt',\n",
    "                            return_length=True,\n",
    "                            add_special_tokens=True)\n",
    "    # print(out)\n",
    "    input_ids = out['input_ids'].to(device)\n",
    "    attention_mask = out['attention_mask'].to(device)\n",
    "    token_type_ids = out['token_type_ids'].to(device)\n",
    "    \n",
    "    out_test = model(input_ids=input_ids,\n",
    "                     attention_mask=attention_mask,                 \n",
    "                     token_type_ids=token_type_ids\n",
    "                )\n",
    "    \n",
    "    t1 = out_test[0][0].item()\n",
    "    t2 = out_test[0][1].item()\n",
    "    \n",
    "    # print('相关性: %.4f \\t 不相关性: %.4f' % (t1, t2))\n",
    "    y_head = out_test.argmax(dim=1).cpu().item()\n",
    "    \n",
    "    # y_head: 1: 句子不相关. 0: 相关\n",
    "    return t1, t2, y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbdcd88f-668b-47d4-9ba2-280f183f7b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6539220809936523, 0.34607794880867004, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# str_sents = '他说明天要去钓鱼, 在中山公园. 您的快递到了'\n",
    "str_sents = '您好, 您的快递到了'\n",
    "\n",
    "\n",
    "t1, t2, y_head = str_felling_detect(model, str_sents)\n",
    "\n",
    "t1, t2, y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f6180f0-bd79-4c64-aa48-7b73a6c7638f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/train/cache-2c893bcab2dc48fd.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9552\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Dataset('train')\n",
    "\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "363ea8ed-546a-4af7-933f-9d87e8cf1d32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:预测:1 实际label: 0,  0.0002 0.9998 \n",
      "input_str_1: 15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜\n",
      "input_str_2: 欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0000 1.0000 \n",
      "input_str_1: 1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来.\n",
      "input_str_2:  3.机盖的钢琴漆，手不能摸，一摸一个印. 4.硬盘分区不好办.\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0009 0.9991 \n",
      "input_str_1: 呵呵，虽然表皮看上去不错很精致，但是我还是能看得出来是盗的\n",
      "input_str_2: 。但是里面的内容真的不错，我妈爱看，我自己也学着找一些穴位。\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0004 0.9996 \n",
      "input_str_1: 这本书实在是太烂了,以前听浙大的老师说这本书怎么怎么不对,哪些地方都是\n",
      "input_str_2: 误导的还不相信,终于买了一本看一下,发现真是~~~无语,这种书都写得出来\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0012 0.9988 \n",
      "input_str_1: 我看过朋友的还可以，但是我订的书迟迟未到已有半个月，都没有收\n",
      "input_str_2: 到打电话也没有用，以后你们订书一定要考虑好！当当实在是太慢了\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0019 0.9981 \n",
      "input_str_1: 送的内胆包有点不好，还有外接电源中间连接处无法全部插入。\n",
      "input_str_2: 续航时间也没有标称的那么长，希望京东能注意宣传的真实性。\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0000 1.0000 \n",
      "input_str_1: 这是我第1次给全五星哦^_^超级快!这是最快收到书的一次了.我是中午的时候订的,结果第2天上午就收到了,算了一下,1天的时间都还没到呢!在此,感激下当当的服务.\n",
      "input_str_2: ..我的确是很急需这本书呢.关于书的本身,也很不错.内容还是很丰富的,值得推荐,对于训练和培养逻辑思维套式有一定的帮助,推荐一下~还有祝朋友们都面试成功,哈哈哈~\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0005 0.9995 \n",
      "input_str_1: 内存太小，偶配了2根“金条”，目前兼容。但不知道是内存不兼容还是什么，有时会听到硬盘“咔咔”运行的声\n",
      "input_str_2: 音。预置系统下Office是2007试用版，偶自装2003，则每次启动弹出正版增值计划窗口，很麻烦！\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0002 0.9998 \n",
      "input_str_1: 我去官网下了最新的XP驱动，结果声卡装好了，有声音了，接着装显卡，装完显卡，电脑就没声音了，一共\n",
      "input_str_2: 装了四次，第三次才发现显卡驱动和声卡驱动冲突， 然后在官网下了前一个显卡驱动，终于搞定了，没有冲突\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0006 0.9994 \n",
      "input_str_1: 是一个朋友送给我的，如今我又在把它推荐给其他朋友。是它为我打开了一扇门，通向图画书的宫殿的门。就连我这个所谓的教育工作者也要感叹自己的无知。感谢作者告诉了我们世界上有这么多的好的图画书。\n",
      "input_str_2: 我给晓晓买的图画书都是这样按图索骥买来的，本本都让她爱不释手！当然了，现在还发现了它另外一个妙用：省钱。哈哈，试想，把那里面的书都买下来要多少钱啊？嘿嘿，我不做傻瓜。就这样讲故事，也不错。\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0007 0.9993 \n",
      "input_str_1: 跟住招待所没什么太大区别。\n",
      "input_str_2:  绝对不会再住第2次的酒店！\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0000 1.0000 \n",
      "input_str_1: 差得要命,很大股霉味,勉强\n",
      "input_str_2: 住了一晚,第二天大早赶紧溜\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0003 0.9997 \n",
      "input_str_1: 1.上了半天网，都上不去，问酒店，答曰坏了。气愤！ 2.晚上入住适逢变天，天气变冷，开了半天空调，竟然还是冷风，问酒店，答曰晚上关空调。晕！床上只有一床单被，到处找被子，无。问酒店要，半小时催了三次电话才来。还是冷，要求在加一床被子，服务员去后再也没回，打到总台，又等了半天，回复没被子了。不把顾客当人啊。 3.一晚没睡成，凌晨5点多，被不知道哪里来的低沉的轰鸣声吵\n",
      "input_str_2: 醒。在房间找了半天没查出哪里来得声音，叫来服务员，（半小时催了三四次才来，人都要疯掉）。六点多服务员才来，也解决不了。就这么一夜没睡。买气受来了。 4.到总台要求当班经理来解释，换了几个人来，说了几遍，酒店不断去落实事情真相，最后才来一个自称客服经理的，说却有其事，表示道歉，做了一小时的冷板凳，一口热水没喝，口都讲干了。这就是他们的诚意。 总结：硬件老旧，服务太差。\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0000 1.0000 \n",
      "input_str_1: 拿回家的那天，我女儿第一时间要我给她讲完6本故事！累死我了。因为有第一套“神奇校车”，小家伙都很喜欢。但是，我个人认为，这一套没有第一套好看，人物画得和第一套有点差别，有时候故事衔接\n",
      "input_str_2: 不上，感觉不是正版或不是一个时间翻译出来的版本一样。总之没有第一套好看，对于5岁的孩子来说，内容有点深了，不够第一套吸引人。但是总体来说还是很好的，也许放到大一点再读是不是不一样呢？\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0002 0.9998 \n",
      "input_str_1: 没有比这更差的酒店了。房间灯光暗淡，空调无法调节，前台服务僵化。用早餐时，服务员居然叫我回房间把拖\n",
      "input_str_2: 鞋换成皮鞋再下来，到底谁是上帝，这家酒店的老总应该马上辞职。建议大家不要住这家酒店，有被骗的感觉！\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0083 0.9917 \n",
      "input_str_1: 我住的是特色标间，所谓特色，是有些类似家的感觉。寝具不是单调的白色，是条纹和大格子的，感觉很温馨。圈儿椅的靠垫是卡通的加菲猫头，明黄的颜色有点扎眼，和周围配在一起感觉有点不协调。 客房硬件评价3.5分。加湿器、烫衣板、电熨\n",
      "input_str_2: 斗、吹风机、小冰箱等俱全。缺点：液晶电视固定在墙上，位置不好。一是遮挡了一个电源插座；二是与桌子搭配不当。桌子（与冰箱柜一体）在电视下方，如果坐在桌子旁上网或工作学习，正好挡住了电视屏幕。错开坐，正好是冰箱柜，腿脚伸不开。\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0011 0.9989 \n",
      "input_str_1: 我是法语初学者,学了78个课时的初级班.因为我是老年人,法语又难,我只有再从头自学巩固一遍,然后准备报中级班.我们学的教材是孙辉老师的.买本练习答案对自学和检查做习题的正确性是非常必要的.而且孙\n",
      "input_str_2: 辉老师的教材在网上还有视频,对我们学习法语有很大的帮助.孙辉老师的教材编写得很好,我向学法语的朋友推荐.我想学好了上下册,再结合(走遍法国)的阅读和学习,会达到我们预期的愿望.这仅只是个人看法,\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0018 0.9982 \n",
      "input_str_1: 这家YMCA非常好，还主动帮我们升级房间，住起又棒又舒适，地点也\n",
      "input_str_2: 很方便，服务又好，号称香港物美价优第一流的YMCA，值得大家推荐。\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0015 0.9985 \n",
      "input_str_1: 时时都在阅读着这篇令人太伤感的小说，我是那样的爱狗狗，也一直希望自己能养一只狗狗，也是天天观察着自己身边一些些可爱的狗，然而爸爸妈妈却因为家里的条件一直没有聊了我的心愿，当我看到了作者简\n",
      "input_str_2: 介、文章简介，实在忍不住了，强烈要求父母给我买下它。狗狗这种伙伴，是给人开心、快乐和幸福的。而当我翻完此书的最后一页时，不禁有了非一般的伤感。同时，我敬佩作者与狗狗，并买下了金毛寻回犬……\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0002 0.9998 \n",
      "input_str_1: 我不是个偏激的人，所以我喜欢都各种各样的书。韩寒 和郭敬明的书我都回会去看，郭敬明写的东西比较的煽情，有时后觉得他写的东西让人很不能接受.......不过他的确是才华出众...可是韩寒写的东西却很有\n",
      "input_str_2: 深度、内涵，他的书我会用心的去读很多边都不会觉得枯燥......《他的国》我正在看，觉得在文字上，他依然保持着他的风格....不过还是很喜欢。加油！我一定会一直喜欢你的作品的，所以韩寒你要加油写书咯！\n",
      "\n",
      "error:预测:1 实际label: 0,  0.0003 0.9997 \n",
      "input_str_1: 酒店环境很好，商务中心很热情．免费接送机服务很好．房间设施尚可，但浴室条件稍差，没有吹\n",
      "input_str_2: 风机，热水也要早７：３０时以后才有．而且总说线路繁忙不能刷卡，房费要付现金，不太理解．\n",
      "\n",
      "acc: 0.5882\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sum_total = 0\n",
    "sum_acc_count = 0\n",
    "\n",
    "for i, str_sents in enumerate(test_dataset):\n",
    "    \n",
    "    input_str = str_sents[0] + str_sents[1]\n",
    "    label = str_sents[2]\n",
    "    \n",
    "    # print(i, input_str, label)\n",
    "    \n",
    "    t1, t2, y_head = str_felling_detect(model, input_str)\n",
    "    \n",
    "    if y_head == label:\n",
    "        sum_acc_count += 1\n",
    "    else:\n",
    "        print('error:预测:%s 实际label: %s,  %.4f %.4f ' % (y_head, label, t1, t2))\n",
    "        print(\"input_str_1: %s\" % str_sents[0])\n",
    "        print(\"input_str_2: %s\" % str_sents[1])\n",
    "        print(\"\")\n",
    "        pass\n",
    "\n",
    "    sum_total += 1\n",
    "    if i >= 50:\n",
    "        break\n",
    "\n",
    "\n",
    "# 1: 不相关 0:相关\n",
    "print('acc: %.4f' % (sum_acc_count / sum_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ba30f-56f4-432f-bf72-2f6556f1df01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "\n",
    "# 保存\n",
    "\n",
    "model_save_path = 'chinese_infer_mission_2023_4_10.pt'\n",
    "# torch.save(model.state_dict(),  model_save_path)  # 推荐的文件后缀名是pt或pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc6105-a217-4444-99b1-7d7fdb5165a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
