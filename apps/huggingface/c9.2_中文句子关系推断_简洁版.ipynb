{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d02d7b-061a-4a97-ab11-0091930f86b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d71f014a-fd28-4f0c-b70e-a138b6f83ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/train/cache-248dcc552a1bfcb6.arrow\n"
     ]
    }
   ],
   "source": [
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, split):\n",
    "        # dataset = load_dataset(path='lansinuote/ChnSentiCorp', split=split)\n",
    "        dataset = load_from_disk('./data/ChnSentiCorp')['%s' % split]\n",
    "        \n",
    "        def f(data):\n",
    "            return len(data['text']) > 40\n",
    "\n",
    "        self.dataset = dataset.filter(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset['text'][i]\n",
    "        \n",
    "        # print('原始数据: ', text)\n",
    "        # label = self.dataset['label'][i]\n",
    "        #切分一句话为前半句和后半句\n",
    "        sentence1 = text[:20]\n",
    "        sentence2 = text[20: ]\n",
    "        label = 0\n",
    "\n",
    "        # 有一半的概率把后半句替换为一句无关的话\n",
    "        if random.randint(0, 1) == 0:\n",
    "            j = random.randint(0, len(self.dataset) - 1)\n",
    "            # sentence2 = self.dataset[j]['text'][20:40]\n",
    "            sentence2 = self.dataset['text'][j][20:]\n",
    "            label = 1\n",
    "\n",
    "        return sentence1, sentence2, label\n",
    "\n",
    "# 加载\n",
    "dataset = Dataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e85d5d72-f53d-4761-a6f6-f11bdca2f3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('选择珠江花园的原因就是方便，有电动扶梯直', '接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 0)\n",
      "\n",
      "('15.4寸笔记本的键盘确实爽，基本跟台式', '机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错', 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(dataset[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c9a66e21-53b3-456a-ad48-7475eecf9aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8001,\n",
       " '今天才知道这书还有第6卷,真有点郁闷:为',\n",
       " '什么同一套书有两种版本呢?当当网是不是该跟出版社商量商量,单独出个第6卷,让我们的孩子不会有所遗憾。',\n",
       " 0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试拼接的数据\n",
    "\n",
    "sentence1, sentence2, label = dataset[3]\n",
    "\n",
    "len(dataset), sentence1, sentence2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0290920b-732c-4c07-908e-29060ffc94ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# 加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "print(token)\n",
    "\n",
    "\n",
    "# 加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5eda51dd-5d3b-4910-b1f4-fe873c0edf62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[:2] for i in data]  # 句子一, 句子二\n",
    "    labels = [i[2] for i in data]\n",
    "\n",
    "    # print('sents: ', sents)\n",
    "    \n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=300,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True,\n",
    "                                   add_special_tokens=True)\n",
    "\n",
    "    # input_ids: 编码之后的数字\n",
    "    # attention_mask: 是补零的位置是0,其他位置是1\n",
    "    # token_type_ids: 第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e679ef-c4fa-46f3-9414-e71871116081",
   "metadata": {},
   "source": [
    "## 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "98b5b75d-772b-4c38-ac72-40b15d23365e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[CLS] 我 是 在 图 书 馆 发 现 这 套 书 的 ， 看 了 第 1 集 后 就 [SEP] 以 前 货 到 付 款 都 没 这 么 久 ， 真 是 的 ， 收 了 钱 就 不 紧 张 了 ， 烦 烦 烦 ！ ！ ！ ！ ！ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=8,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    print(len(loader))\n",
    "    print(token.decode(input_ids[0]))\n",
    "    input_ids.shape, attention_mask.shape, token_type_ids.shape, labels\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "446cfa9b-6444-4dd4-b774-c15979305ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8045ba75-ce34-4af7-8b27-a207c2a39b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "            pass\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "        out = out.softmax(dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3f622b7d-cf74-4a6e-b94d-e3554267c417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load('chinese_infer_mission_2023_4_10.pt'))\n",
    "# 加载模型\n",
    "\n",
    "# 模型转移到GPU上\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aa292177-36c6-446e-a65a-bccec623f8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 句子测试封装\n",
    "\n",
    "\n",
    "def str_felling_detect(model, str_sents):\n",
    "    \n",
    "    out = token.encode_plus(str_sents,\n",
    "                            truncation=True,\n",
    "                            padding='max_length',\n",
    "                            max_length=45,\n",
    "                            return_tensors='pt',\n",
    "                            return_length=True,\n",
    "                            add_special_tokens=True)\n",
    "    # print(out)\n",
    "    input_ids = out['input_ids'].to(device)\n",
    "    attention_mask = out['attention_mask'].to(device)\n",
    "    token_type_ids = out['token_type_ids'].to(device)\n",
    "    \n",
    "    out_test = model(input_ids=input_ids,\n",
    "                     attention_mask=attention_mask,                 \n",
    "                     token_type_ids=token_type_ids\n",
    "                )\n",
    "    \n",
    "    t1 = out_test[0][0].item()\n",
    "    t2 = out_test[0][1].item()\n",
    "    \n",
    "    print('相关性: %.4f \\t 不相关性: %.4f' % (t1, t2))\n",
    "    pred = out_test.argmax(dim=1).cpu().item()\n",
    "    \n",
    "    # print(pred)\n",
    "    if pred == 1:\n",
    "        print('%s 句子不相关' % pred)\n",
    "    elif pred == 0:\n",
    "        print('%s 相关' % pred)\n",
    "    else:\n",
    "        print('%s 未知' % pred)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dbdcd88f-668b-47d4-9ba2-280f183f7b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相关性: 0.7952 \t 不相关性: 0.2048\n",
      "0 相关\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# str_sents = '他说明天要去钓鱼, 在中山公园. 您的快递到了'\n",
    "str_sents = '您好, 您的快递到了'\n",
    "\n",
    "\n",
    "predRes = str_felling_detect(model, str_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "363ea8ed-546a-4af7-933f-9d87e8cf1d32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mylady/code/python/DL-pytorch/apps/huggingface/data/ChnSentiCorp/test/cache-c06d6d85e943fbe5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 怀着十分激动的心情放映，可是看着看着发现的企业近一些。房间总是能听得很响的水声。希望酒店能降低价格。目前的设施同安徽省内其它酒店相比只值250元不到啊 1\n",
      "相关性: 0.3293 \t 不相关性: 0.6707\n",
      "1 句子不相关\n",
      "\n",
      "1 还稍微重了点，可能是硬盘大的原故，还要再Afee）是在第一次启动时决定是否安装。不像很多品牌的笔记本，一上来一股脑儿给你装上。而且实际使用后发现它的很多自带的实用软件非常出色，非常人性化！回家仔细使用后发现键盘手感果然如大家所说非常出色。另外第一次使用小红点就爱上它了--感觉比触摸板好多了，难怪它会成为ThinkPad基因之一机器虽然有点重，但是感觉很扎实，比某些“娱乐本”扎实很多。 1\n",
      "相关性: 0.2750 \t 不相关性: 0.7250\n",
      "1 句子不相关\n",
      "\n",
      "2 不错，作者的观点很颠覆目前中国父母的教育方式，其实古人们对于教育已经有了很系统的体系了，可是现在的父母以及祖父母们更多的娇惯纵容孩子，放眼看去自私的孩子是大多数，父母觉得自己的孩子在外面只要不吃亏就是好事，完全把古人几千年总结的教育古训抛在的九霄云外。所以推荐准妈妈们可以在等待宝宝降临的时候，好好学习一下，怎么把孩子教育成一个有爱心、有责任心、宽容、大度的人。 0\n",
      "相关性: 0.2644 \t 不相关性: 0.7356\n",
      "1 句子不相关\n",
      "\n",
      "3 有了第一本书的铺垫，读第二本的时候开始进入状态。基本上第二本就围绕主角们的能力训练展开，故事的主要发生场地设置在美洲的亚马逊丛林。心里一直疑惑这和西藏有什么关系，不过大概看完全书才能知道内里的线索。其中描述了很多热带雨林中特有的神秘动植物以及一些生存技巧和常识，受益匪浅。能够想像出要写这样一部书，融合这样许多的知识，作者需要花费多少心血来搜集和整理并成文。 0\n",
      "相关性: 0.4919 \t 不相关性: 0.5081\n",
      "1 句子不相关\n",
      "\n",
      "4 前台接待太差，酒店有A B楼之分，本人check－in后，前台未告诉B楼在何处，并且B楼无明显指示；房间太小，根本不像4星级设施，下次不会再选择入住此店啦。 0\n",
      "相关性: 0.3153 \t 不相关性: 0.6847\n",
      "1 句子不相关\n",
      "\n",
      "5 1. 白色的，很漂亮，做工还可以； 2. 网上的软件资源非常丰富，这是我买它的最主要原因； 3. 电池不错，昨天从下午两点到晚上十点还有25分钟的剩余时间（关闭摄像头，无线和蓝牙）主要拷贝东西，看起来正常使用八小时左右没问题； 4. 散热不错，CPU核心不过40~55度，很多小本要上到80度了； 5. 变压器很小巧，很多小本的电源都用的是大本的电源，本倒是很轻，可旅行重量还是比较重。 0\n",
      "相关性: 0.1009 \t 不相关性: 0.8991\n",
      "1 句子不相关\n",
      "\n",
      "6 在当当上买了很多书，都懒于评论。但这套书真的很好，3册都非常精彩。我家小一的女儿，认字多，非常喜爱，每天睡前必读。她还告诉我，学校的语文课本中也有相同的文章。我还借给我的同事的女儿，我同事一直头疼她女儿不爱看书，但这套书，她女儿非常喜欢。两周就看完了。建议买。很少写评论，但忍不住为这套书写下。也给别的读者参考下。 0\n",
      "相关性: 0.2747 \t 不相关性: 0.7253\n",
      "1 句子不相关\n",
      "\n",
      "7 19天硬盘就罢工了~~~算上运来的一周都 没有摄像头、读卡器，买的时候要考虑清楚。 1\n",
      "相关性: 0.2406 \t 不相关性: 0.7594\n",
      "1 句子不相关\n",
      "\n",
      "8 书一到，即被朋友借走，我是早已看过了的，觉有点软 外壳感觉不是那么结实 好像很薄一样 1\n",
      "相关性: 0.1742 \t 不相关性: 0.8258\n",
      "1 句子不相关\n",
      "\n",
      "9 书的印刷看起来也不好,画面也不漂亮,内容做个U盘启动 屏幕太小，男同胞可能看不习惯 1\n",
      "相关性: 0.1965 \t 不相关性: 0.8035\n",
      "1 句子不相关\n",
      "\n",
      "10 没有许多网友评价热度高的问题，第一次要手刺，完全与众不同的漫画方式。还有管他真真假假的“童年故事”。这本书里出现最多的“感化”一词，配上黑背带着杀气的表情，爆笑那是一定要的，一口气看完那绝对是必然的，对黑背的佩服和敬仰用滔滔江水来形容已经不够了。惹到谁，都不要惹到黑背。可是来不及了，我已经被感化了。。。学到不少。以后朋友们，请小心我！~ 1\n",
      "相关性: 0.2667 \t 不相关性: 0.7333\n",
      "1 句子不相关\n",
      "\n",
      "11 没有蓝牙、摄像头，连麦克风都省了 装XP有点小麻烦，MSDN原版没有SATA驱动无法安装，装雨林木风GHOST纯净版OK。 自带无线网卡驱动不能用，需到官网下载。 0\n",
      "相关性: 0.4575 \t 不相关性: 0.5425\n",
      "1 句子不相关\n",
      "\n",
      "12 在网上偶尔看到别人的推荐，于是买一本看看基本没有错别字，但检察院写成了检查院，故事也过于理想化，特别是岱西与公司、王伟等人的冲突，某些情节不符合逻辑。 1\n",
      "相关性: 0.1507 \t 不相关性: 0.8493\n",
      "1 句子不相关\n",
      "\n",
      "13 新机拿到手就有硬件问题，而且等了6天才到于思考作用，因为，天下没有一定之规，美利坚的好，不能遮盖华夏的发展阶段。留学生滞美不归者前几日给我们作了一次内部演讲，总之是说美国人权如何好，一开始很赞同，后来想过后发现一个问题，即“以高福利、全民普选、私人财产不容侵犯”为基点的宣讲，优则优矣，却不具备普世之视角，所谓一阴一阳谓之道未被参破。多说不必，阅者知之。 1\n",
      "相关性: 0.0935 \t 不相关性: 0.9065\n",
      "1 句子不相关\n",
      "\n",
      "14 我在晚上6点30左右入住的,当时是一位男服务员为我登记,我问他可不可以作信用卡预授权，他说不可以但建议我可以用现金作为押金到退房时可以用信用卡消费．房间很大但很旧，电视机只能收到４个台的节目，在第三天我退房时要求用信用卡消费，当时一位女服务员说我之前是用现金作为押金是不可以再用信用卡消费的，还反问我说如果我要消费的话在入住时为什么不做预授权，当时我就把之前那位男服务员说的话跟他说一次后又反口说是因为现在机器坏了不能用，我跟她说我在３０分钟之前见到有一位客人才用信用卡消费过，为什么会这么快就坏了，她见我态度那么强硬就帮我用卡消费了，事实上证明了刷卡机并没有坏，当时因为我们赶着到机场坐飞机就没有再跟她理论了，但一直觉得心中有气． 0\n",
      "相关性: 0.0205 \t 不相关性: 0.9795\n",
      "1 句子不相关\n",
      "\n",
      "15 这次入住少林宾馆还是比较满意的,宾馆的位有收获。它虽然强调“全”字，但是全得又很有逻辑和重点，可以看出作者有丰富的实战经验，对操作中会遇到什么样的问题了如指掌。有些基础的人，感觉就是在快速浏览中仍可发现一个一个的亮点。这本书既可以作为单独的学习教材，使学习者迅速掌握EXCEL的许多窍门，又可以作为工具书备查。书的质量确实一般，也有错字，但是不至于影响学习的情绪就是了 1\n",
      "相关性: 0.2082 \t 不相关性: 0.7918\n",
      "1 句子不相关\n",
      "\n",
      "16 这一套书我基本买齐了，也看了好多本了。是利用闲暇时间巩固英语，学习知识的好东东。当然，再好的东西也要适合才真的能用上，如果你觉得你目前的任务是冲刺学习，那这书不一定适合你。但是，如果，你想有一个长效的学习习惯，你就可以考虑这套英文读物。另外，MP3也很不错，可在上下班的路上听，就像听广播小说，在娱乐中提高了听力。 0\n",
      "相关性: 0.3870 \t 不相关性: 0.6130\n",
      "1 句子不相关\n",
      "\n",
      "17 酒店设施尚可,房间内镜子太多.携程的套餐包括送水果,还有巧克力,比较温馨. 服务一般,入住时前台试图诱导我买早餐,两大一小每天早餐440元,两天共计880元,另外冰箱里的东西可以随便吃,被我拒绝了!第二天就不再送水果巧克力了,连免费的两瓶水也不给了...酒店离九龙又一城步行约15分钟,旁边有个公园,但是离地铁远了点,需要做小巴,有时6元每人,有时3元每人,开得飞快很吓人,小孩同样费用,就是这点不满意,还是住得离地铁近些比较方便. 0\n",
      "相关性: 0.1756 \t 不相关性: 0.8244\n",
      "1 句子不相关\n",
      "\n",
      "18 这个价格这种房间环境很不错，感觉很干净^_^ 兴旺楼给我个人感觉很雅...下次来，肯定还定这里。。。 0\n",
      "相关性: 0.0954 \t 不相关性: 0.9046\n",
      "1 句子不相关\n",
      "\n",
      "19 选择的事例太离奇了，夸大了心理咨询的现实意义，让人失去了信任感！如果说这样写的效果能在一开始抓住读者的眼球，但是看到案例主人公心理问题的原因解释时就逐渐失去了兴趣，反正有点拣了芝麻丢了西瓜的感觉。 0\n",
      "相关性: 0.1426 \t 不相关性: 0.8574\n",
      "1 句子不相关\n",
      "\n",
      "20 简单，大方，在同类尺寸的款型的笔记本中不显厚重，轻薄感！很安静，几乎没有声音，音质不错屏幕不错，显的细腻可观。 感谢马连道提货点的工作人员（前台客服），服务态度超好，值得贵公司其他员工学习。 0\n",
      "相关性: 0.8682 \t 不相关性: 0.1318\n",
      "0 相关\n",
      "\n",
      "21 硬盘到手就发现一个坏块，因为是完美屏，没回京东换新，花了两天在本地换新硬盘，发票都不需要；电池衔接很松，可有1mm间隙；出厂时A、B面贴的保护膜太敷衍，太多气泡，虽然反正要撕掉，但说明厂家态度不严谨。 0\n",
      "相关性: 0.5257 \t 不相关性: 0.4743\n",
      "0 相关\n",
      "\n",
      "22 没发现什么优点，回来了开机什么都没有，有自己装的XP也没费什么劲，麦有些问题。对方听不清。调了好久才好了，整体还不错 0\n",
      "相关性: 0.0815 \t 不相关性: 0.9185\n",
      "1 句子不相关\n",
      "\n",
      "23 外观漂亮，系统不是很难弄，刚开始看了很多诉了你更多的理论,让家长更急迫地去审视中国的教育环境,在敬佩的同时,也深感自身育儿知识的缺乏.前者在无形中引起你的共鸣,后者则是给家长注射了一剂疫苗,在增强家长的批判意识的同时,也提醒各位时刻不要忘了学习,这样,才能跟上小朋友成长的步伐.比较适合学龄前小朋友的家长看,提前给家长朋友打预防针. 1\n",
      "相关性: 0.2198 \t 不相关性: 0.7802\n",
      "1 句子不相关\n",
      "\n",
      "24 9号下午下的单子,11中午拿到货,真的很赞.机子开开后发现送个内胆包,一个擦拭布,试用了2个多小时,发现声音很小,下在速度很很不错，开网页能同时6-7不卡. 0\n",
      "相关性: 0.0649 \t 不相关性: 0.9351\n",
      "1 句子不相关\n",
      "\n",
      "25 赠送的系统慢就改装XP算了，内装的备份是用GHOST的就好了，应该再配一根USB电脑连接线就更理想了。 0\n",
      "相关性: 0.2296 \t 不相关性: 0.7704\n",
      "1 句子不相关\n",
      "\n",
      "26 原本在网上订了两个套房，入住后，携程还给我打电话问是否只入住了一间，要扣我信用卡里的钱。真不知道酒店与携程是如何衔接的？ 宾馆反馈 2007年12月7日 ： 我们非常感谢您的留言，非常抱歉由于一些小误会给您带来的困惑。根据我们的担保订房要求，对于确认担保的房间未入住的会收取当天房费。当然，我们也会进一步加强与携程的沟通与协调，让您今后的入住感觉更加舒适与愉快。 0\n",
      "相关性: 0.0952 \t 不相关性: 0.9048\n",
      "1 句子不相关\n",
      "\n",
      "27 该酒店实际是兰州铁路局的内部招待所，位于设计不合理。 2：奶牛本装xp很麻烦，不过买一个带硬盘驱动的系统安装盘，先将硬盘分区，在安装系统。 1\n",
      "相关性: 0.0692 \t 不相关性: 0.9308\n",
      "1 句子不相关\n",
      "\n",
      "28 当时是同事极力推荐这本书。我看到网上的介有我原来的戴尔热。 各种触摸键和指示灯很美观，日系电器的典型设计特点。 性价比也不错 1\n",
      "相关性: 0.1058 \t 不相关性: 0.8942\n",
      "1 句子不相关\n",
      "\n",
      "29 从携程订房无数，唯独这家实在不感恭维。从对于那些手头较紧的人是个不错的选择。 关于驱动还是比较容易找的，盘里有XP驱动的。光盘里的software这个文件里面有的。大家可以试试。 1\n",
      "相关性: 0.0582 \t 不相关性: 0.9418\n",
      "1 句子不相关\n",
      "\n",
      "30 钱也付了，书干等不来，好不容易找到客服电那就真不错了。有人说胡兰成是无知的残忍，就像小孩不懂事，随便毁坏一样东西，还在那边邀功请赏，希望得到大家的认同，别人最好拍手鼓掌那自己就更问心无愧了。为什么文和人不能划等号呢，搞不懂！ 1\n",
      "相关性: 0.2088 \t 不相关性: 0.7912\n",
      "1 句子不相关\n",
      "\n",
      "31 容易产生指纹。不习惯分区。由于出货量大了象可爱，我很喜欢这只小狐狸~看《妈妈》这一章的时候，父母刚好都出差了，家里就我一人，所以看着看着就哭了（我已经一年多没哭过了）总体上非常好，不过因为我拿到的是崭新的书，所以刚打开时，油墨的味道很刺鼻，让人很难受。而且我看完整本书只花了10分钟，毕竟只是漫画书嘛，想追求丰富文字内容的朋友可能会失望……建议比较有“闲钱”的朋友购买~~ 1\n",
      "相关性: 0.0833 \t 不相关性: 0.9167\n",
      "1 句子不相关\n",
      "\n",
      "32 六心电池装在后面突出一大块影响美观，速度置很高，初步体验感觉很舒服，速度很快！键盘触摸板也很好，摸着就感觉舒服！ 1\n",
      "相关性: 0.5450 \t 不相关性: 0.4550\n",
      "0 相关\n",
      "\n",
      "33 这套书是买给儿子的，小家伙两岁半不到，正了我刚刚参加完高考的学生，相信会对他们有所启发。 1\n",
      "相关性: 0.3240 \t 不相关性: 0.6760\n",
      "1 句子不相关\n",
      "\n",
      "34 本人于清明假期一家三口高高兴兴通过携程订总感觉有句无篇。张迷尚可以一看。鉴定完毕。 1\n",
      "相关性: 0.3837 \t 不相关性: 0.6163\n",
      "1 句子不相关\n",
      "\n",
      "35 这样的男人，温柔的过了头，找不到一丝男人的气概。只会说：这是好的。。。这也是好的。。。无法欣赏这样的文笔优美在何处。爱情也是奇怪，张爱玲爱上了这样的一个人，隐隐地，大概是一辈子的影子。 0\n",
      "相关性: 0.5437 \t 不相关性: 0.4563\n",
      "0 相关\n",
      "\n",
      "36 第一次装xp到一半蓝屏，不过进bios，在advanced一栏里把硬盘模式从achi改为ide即可。驱动网上都有，下载g430驱动就行了 0\n",
      "相关性: 0.4807 \t 不相关性: 0.5193\n",
      "1 句子不相关\n",
      "\n",
      "37 光驱确实不怎么好，装系统的时候就怕它挂掉适 不足： 装系统比较麻烦~ 总结： 不错，对得起这个价钱 1\n",
      "相关性: 0.6666 \t 不相关性: 0.3334\n",
      "0 相关\n",
      "\n",
      "38 在我刚开始从事设计的时候，阿尔瓦 阿尔托的那么震撼，那么泪流满面。作者的文笔很差，很初级，叙述让人不能产生情感的共鸣，建议他多学习那些名作品，同样写文革，请看看余华是怎么写的，同样写爱情，请看看琼瑶阿姨怎么描述的还有我觉得，这篇史上最纯洁的爱情，太名不副实。艾米你以为单纯的描写男女主角不发生关系，就是纯洁了？？？？看来你对爱情的理解还太浅薄 1\n",
      "相关性: 0.2858 \t 不相关性: 0.7142\n",
      "1 句子不相关\n",
      "\n",
      "39 外包装写是内存2G，可是内清单标的是1G，不知道是怎么回事，希望有了解的朋友帮忙解决一下，谢谢。 0\n",
      "相关性: 0.5723 \t 不相关性: 0.4277\n",
      "0 相关\n",
      "\n",
      "40 这本书，让我觉得很不值得买。关于毕淑敏的用一段时间再给发表使用情况、心得体验给大家提供参考 1\n",
      "相关性: 0.1019 \t 不相关性: 0.8981\n",
      "1 句子不相关\n",
      "\n",
      "41 10月16日入住该酒店.感觉不错哦,离汽，求的是轻松。那天正好坐飞机去北京，在飞机上我不停喷笑，努力克制，好歹也要装出“杜拉拉”型的职场白骨精形象。等下飞机的时候，刚好看完，却开始心里泛堵。总觉得像咬了口大红薯，虽然味美，但是卡在食道下不去，憋得慌。坐出租去酒店的时候，看到初春的北京还是处处冷涩，突然有种顿悟的感觉，北京的青春和南方的如此不同，彪悍之余还带着一点忧伤。 1\n",
      "相关性: 0.0329 \t 不相关性: 0.9671\n",
      "1 句子不相关\n",
      "\n",
      "42 总体说来,应该是二星级宾馆.因为定的时间~~ 而且金碧辉煌的~~! 房间虽然不大不过我一个人住肯定够,很舒适 而且窗外是海景和轮船呢~~嘿嘿.. 酒店就在海鲜街上,走不远还有皇后大道~ 交通呢..其实在香港交通都会很方便拉~ 酒店门口就有有轨电车~可以到地铁站也可以直接到中环旺角什么的.. 出门不远有个锅贴大王东西又好吃又便宜~~推荐！ 下次有机会还会考虑住这儿.! 补充点评 2008年7月24日 ： 有空调!可以自己调温度！ 1\n",
      "相关性: 0.0703 \t 不相关性: 0.9297\n",
      "1 句子不相关\n",
      "\n",
      "43 充满了惨不忍睹的比喻 例如：用掺着血的牙膏沫子形容一串粉红色珍珠的色泽 让人情何以堪？！ 0\n",
      "相关性: 0.2834 \t 不相关性: 0.7166\n",
      "1 句子不相关\n",
      "\n",
      "44 这本书也许你不会一气读完，也许它不够多精房给你第一的感慨是灯光灰暗,我想看报只能去卫生间看,因卫生间照明亮度才能达到.我是晚上9点多进酒店,11点多想洗澡,却开不出热水,打电话给总台才知是市里供热管道出了问题,要我耐心等待.直到半夜1点多还是没供上,我又找了酒店值班柳经理,本人一天长途从上海开车12个小时到威海,就是能洗个澡好好休息 1\n",
      "相关性: 0.1043 \t 不相关性: 0.8957\n",
      "1 句子不相关\n",
      "\n",
      "45 入住西楼海景套房（据说离海更近一点），应净程度还算可以接受。电视遥控器也不好用，窗户完全不可以打开，外面贴着不知道什么广告。到晚上11点回来，已经不知道前台服务员去了哪里？无奈找到其他服务员，对前台告诉早上6：00叫早，前台也已经确认，结果是幸亏自己的手机有闹钟功能，不然一定赶不上飞机。早上结账时，告诉入住2位客人，需要另外交2元的城市建设费（好像是这个费用，甘肃省要求的）。在甘肃的其他酒店入住时，其他的酒店都自己承担的，钱虽然不多，不过可以反映酒店的服务水平。唯一的好处就是这个酒店可以提供免费接机服务，不过来接的车子也应该不是酒店的车子，一路上一直询问可否自己租他的车子。不过师傅倒是态度挺憨厚的。这样小的城市，这样的服务水平也就不足为奇了！！ 1\n",
      "相关性: 0.1307 \t 不相关性: 0.8693\n",
      "1 句子不相关\n",
      "\n",
      "46 房间稍微小了点 其他没有问题 很不错 上次和一日本客人一起住的 对方很中意 因为竟然有 NHK 0\n",
      "相关性: 0.0997 \t 不相关性: 0.9003\n",
      "1 句子不相关\n",
      "\n",
      "47 《离婚》也读完了。“离婚”翻译成更明白的话，应该叫幻灭。所有的对生活的希望都伴随着该离婚的人的不离婚而破灭了。 0\n",
      "相关性: 0.2509 \t 不相关性: 0.7491\n",
      "1 句子不相关\n",
      "\n",
      "48 做工一般，和预期有一定差距；本有点厚，再上,洗手间放了报纸.餐厅早餐自助餐服务,电梯服务都很周到.其实宾馆服务硬件不一定要很豪华,但要真心对待客人.希望坚持. 1\n",
      "相关性: 0.2316 \t 不相关性: 0.7684\n",
      "1 句子不相关\n",
      "\n",
      "49 床发出吱嘎吱嘎的声音，房间隔音太差，赠送就没有shuttle bus了。大堂很小，也没有什么设施。不过，房间很好，也有海景。 1\n",
      "相关性: 0.1335 \t 不相关性: 0.8665\n",
      "1 句子不相关\n",
      "\n",
      "50 产品很好也很满意，做工很精细。质量和性能也很超值，就是微星的售后服务点只在省会城市，很不方便。 0\n",
      "相关性: 0.5349 \t 不相关性: 0.4651\n",
      "0 相关\n",
      "\n",
      "acc: 0.5882\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sum_total = 0\n",
    "sum_acc_count = 0\n",
    "\n",
    "for i, str_sents in enumerate(Dataset('test')):\n",
    "    \n",
    "    input_str = str_sents[0] + str_sents[1]\n",
    "    label = str_sents[2]\n",
    "    \n",
    "    print(i, input_str, label)\n",
    "    \n",
    "    predRes = str_felling_detect(model, input_str)\n",
    "    \n",
    "    if predRes == label:\n",
    "        sum_acc_count += 1\n",
    "    \n",
    "    \n",
    "    sum_total += 1\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    if i >= 50:\n",
    "        break\n",
    "\n",
    "print('acc: %.4f' % (sum_acc_count / sum_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ed2ba30f-56f4-432f-bf72-2f6556f1df01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "\n",
    "# 保存\n",
    "\n",
    "model_save_path = 'chinese_infer_mission_2023_4_10.pt'\n",
    "# torch.save(model.state_dict(),  model_save_path)  # 推荐的文件后缀名是pt或pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc6105-a217-4444-99b1-7d7fdb5165a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
